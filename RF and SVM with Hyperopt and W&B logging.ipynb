{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f188b1-1256-462a-a1d4-1de2ba4aac80",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark modelling - optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed9ee60-5a17-4547-bcab-959751b81a99",
   "metadata": {},
   "source": [
    "## 1. Libraries and Spark setup\n",
    "\n",
    "\n",
    "This section imports necessary libraries and sets up the Spark environment:\n",
    "\n",
    "Libraries:\n",
    "* `o`, `sys`, `json`, `datetime`, `numpy`, `pandas`, `tqdm`, `matplotlib.pyplot`: General purpose libraries for file system access, system functionalities, JSON handling, date/time manipulation, numerical computation, data manipulation, progress bars, and plotting.\n",
    "\n",
    "pyspark libraries:\n",
    "* `SparkContext`, `SparkConf`: Core Spark functionalities for setting up the Spark context and configuration.\n",
    "* `SparkSession`: Entry point for interacting with Spark SQL.\n",
    "* `functions as F`: Provides various Spark SQL functions for data manipulation.\n",
    "* `types`: Defines data types for Spark DataFrames.\n",
    "* `Window`: Used for window functions in Spark SQL.\n",
    "* `ml.feature`: Provides feature engineering and transformation tools like `Word2Vec`, `Imputer`, `OneHotEncoder`, `StringIndexer`, `VectorAssembler`.\n",
    "* `ml.classification`: Provides classification algorithms like `LogisticRegression` and `RandomForestClassifier`.\n",
    "* `ml.evaluation`: Provides evaluation metrics like `BinaryClassificationEvaluator` and `BinaryClassificationMetrics`.\n",
    "* `ml.tuning`: Provides tools for hyperparameter tuning like `CrossValidator` and `ParamGridBuilder`.\n",
    "\n",
    "Spark Configuration:\n",
    "* `SparkConf`: Sets configuration parameters for the Spark application.\n",
    "* `spark.master`: Specifies the cluster manager; local[*] indicates using all available cores on the local machine.\n",
    "* `spark.driver.memory`, `spark.driver.maxResultSize`: Allocates memory for the driver process.\n",
    "* `SparkContext`, `SparkSession`: Creates the Spark context and session based on the configuration.\n",
    "\n",
    "Accessing Data:\n",
    "* `access_data`: Function to load JSON data from a local file.\n",
    "* `access_s3_data`: Loads AWS credentials from a local JSON file.\n",
    "\n",
    "Spark configuration is further set to access data from Yandex Cloud Storage (S3-compatible) using the loaded credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a78ba91-a8ad-43c3-806e-5fc23c39a35b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Create filenames \n",
    "\n",
    "import datetime\n",
    "import uuid\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d90ad87-b0ec-418e-bfd5-b50aa0d9281c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Using cached wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Collecting setproctitle\n",
      "  Using cached setproctitle-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Using cached sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from wandb) (4.11.0)\n",
      "Collecting appdirs>=1.4.3\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.11)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Installing collected packages: appdirs, urllib3, setproctitle, docker-pycreds, sentry-sdk, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.8\n",
      "    Uninstalling urllib3-1.26.8:\n",
      "      Successfully uninstalled urllib3-1.26.8\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.8.22 requires urllib3<2, but you have urllib3 2.2.1 which is incompatible.\n",
      "botocore 1.34.84 requires urllib3<1.27,>=1.25.4; python_version < \"3.10\", but you have urllib3 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed appdirs-1.4.4 docker-pycreds-0.4.0 sentry-sdk-1.45.0 setproctitle-1.3.3 urllib3-2.2.1 wandb-0.16.6\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23ca08e-c132-426e-be2b-acaee090a674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# W&B logging \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff1426-b038-4c06-a75d-d03a7f7f4383",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Insert the name of YOUR notebook here below </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803337b4-f484-46e1-9a9d-da8099c7b0c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set name of notebook\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"full_pipeline_with_logging_hyperopt_jake_1month-Copy1.ipynb\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897724d7-5ef5-4ff1-9b8e-a33ae8457ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pyspark general \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Pyspark pre-processing \n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Pyspark vectorization\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Pyspark models \n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "# Pyspark classifiers \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Pyspark cross-validation\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Pyspark reporting \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Pyspark other\n",
    "from pyspark.ml.functions import vector_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1a5c85-715f-4d1c-b23b-c30165e31c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: py4j in /opt/conda/lib/python3.9/site-packages (from hyperopt) (0.10.9.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from hyperopt) (4.62.3)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.9/site-packages (from hyperopt) (2.0.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from hyperopt) (1.8.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.9/site-packages (from hyperopt) (2.6.3)\n",
      "Collecting future\n",
      "  Using cached future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from hyperopt) (1.21.5)\n",
      "Installing collected packages: future, hyperopt\n",
      "Successfully installed future-1.0.0 hyperopt-0.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab30d97-b956-49a8-866c-64c943e7bb3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperopt related \n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll.base import scope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e146c-d9b3-429c-809d-8626be9a19db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color: red;\">Get your w&b API for the next section</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d7f747f-d28b-4b5a-a226-d0e2a344ea28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mst083972\u001b[0m (\u001b[33mgsom-diploma-jap\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights and biases login \n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f214c4d-2142-4557-814f-4fb22a0370d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set('spark.master', 'local[*]')\n",
    "conf.set('spark.driver.memory', '32G')\n",
    "conf.set('spark.driver.maxResultSize', '8G')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd2c2d3-3816-423f-b760-a70937e6b478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def access_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        access_data = json.load(file)\n",
    "    return access_data\n",
    "\n",
    "access_s3_data = access_data('.access_jhub_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05eb3b93-0d2b-467f-9ef8-20dcadc89256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set('fs.s3a.access.key', access_s3_data['aws_access_key_id'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.secret.key', access_s3_data['aws_secret_access_key'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.impl','org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.multipart.size', '104857600')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.block.size', '33554432')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.threads.max', '256')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 'http://storage.yandexcloud.net')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.aws.credentials.provider', \n",
    "                                     'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8ba75-8dcf-418c-9019-dae245606403",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Dataset\n",
    "\n",
    "\n",
    "This section defines variables for data processing and file paths, then performs several data processing stages:\n",
    "Variables:\n",
    "\n",
    "* `VER`: Version identifier.\n",
    "* `PROC_DS`, `PROC_LAGS`, `PROC_VECS`: Flags to control data processing stages.\n",
    "* `FRAC_0`: Fraction of negative examples to sample when processing lags.\n",
    "* `BUCKET`: S3 bucket name.\n",
    "* `files_path`, `files_mask`: Local paths and masks for raw data files.\n",
    "* `file_path_ds`, `file_path_lags`, `file_path_trn`, `file_path_tst`: S3 paths for different stages of processed data.\n",
    "\n",
    "\n",
    "• `VER = 'v2'`: This defines the version of your data processing pipeline. You might increment this version number when you make significant changes to the processing steps.\n",
    "• `BUCKET` = 'pvc-84ea79a0-dc20-4a2d-86ab-f83c1f8d4a7b': This specifies the name of the bucket where your data is stored (likely on a cloud storage platform like AWS S3 or Yandex Object Storage).\n",
    "\n",
    "**Flags:**\n",
    "\n",
    "• `PROC_DS` (False): This flag controls whether to process the raw dataset. If True, the code will read the raw CSV files, extract relevant columns, filter by date, and create the data_raw.parquet file.\n",
    "\n",
    "* Change to True: When you have new raw data or need to reprocess the existing raw data due to changes in the extraction logic.\n",
    "* Keep as False: When you already have a processed data_raw.parquet file and don't need to re-process it.\n",
    "\n",
    "• `PROC_LAGS` (False): This flag controls whether to process and create lag features. If True, the code will calculate lag features based on event history for different time windows and store them in the data_lags.parquet file.\n",
    "\n",
    "* Change to True: When you need to recalculate lag features, such as when you've changed the time window definitions or added new events.\n",
    "* Keep as False: When you already have the desired lag features in data_lags.parquet and don't need to recompute them.\n",
    "\n",
    "• `FRAC_0` (.001): This variable sets the sampling fraction for events with payment_event_flag = 0 when processing lags. This is used to reduce the size of the dataset for faster processing while maintaining a representative sample.\n",
    "\n",
    "* Adjust Value: You might change this value depending on the size of your dataset and the desired balance between processing time and data representativeness.\n",
    "\n",
    "• `PROC_VECS` (True): This flag controls whether to vectorize the data using TF-IDF. If True, the code will transform the lag features into numerical vectors using TF-IDF and store them in data_vec_train.parquet and data_vec_test.parquet files.\n",
    "\n",
    "* Change to False: If you want to experiment with other vectorization methods or use the data in its raw form.\n",
    "* Keep as True: When TF-IDF vectorization is the desired approach for your modeling tasks.\n",
    "\n",
    "**File Paths:**\n",
    "\n",
    "• `files_path`, `files_mask`: These define the location and pattern of the raw data files.\n",
    "• `file_path_ds`, `file_path_lags`, `file_path_trn`, `file_path_tst`: These specify the storage locations for the processed datasets at different stages of the pipeline.\n",
    "\n",
    "By understanding these flags and variables, you can control which parts of the data processing pipeline are executed, allowing for efficient experimentation and iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d37a69-949f-44e2-bf89-ec7a726edc7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "v5 = data week \n",
    "2024, 1, 1, 0, 0, 0 \n",
    "     2024, 4, 16, 23, 59, 59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468306f1-b367-4460-b1ec-75c1401bea43",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Insert YOUR version here below, I.e. V9. If creating NEW data, set all to TRUE</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6671005-60e1-471a-9510-60ae3dad10b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VER = 'vJ1rf' # <-- insert YOUR version here \n",
    "PROC_DS = True # <-- set to true \n",
    "PROC_LAGS = True # <-- set to true \n",
    "FRAC_0 = .001 # used only if `PROC_LAGS = True`\n",
    "PROC_VECS = True\n",
    "BUCKET = 'pvc-84ea79a0-dc20-4a2d-86ab-f83c1f8d4a7b'\n",
    "PRJ_PATH = '/home/jovyan/__RAYPFP'\n",
    "\n",
    "files_path = 'data/events'\n",
    "files_mask = f'{files_path}/data_202*-*-*.csv'\n",
    "\n",
    "file_path_ds = f's3a://{BUCKET}/work/{VER}/data_raw.parquet'\n",
    "file_path_lags = f's3a://{BUCKET}/work/{VER}/data_lags.parquet'\n",
    "file_path_trn = f's3a://{BUCKET}/work/{VER}/data_vec_train.parquet'\n",
    "file_path_tst = f's3a://{BUCKET}/work/{VER}/data_vec_test.parquet'\n",
    "file_path_lags1 = f's3a://{BUCKET}/work/{VER}/data_lags1.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e5c4d7-0cbd-49c8-962b-26df684b9491",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#file_path_lags1 = f's3a://{BUCKET}/work/{VER}/data_lags1.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7f1e5a2-7795-49d9-b139-3a63385380de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_parquet(path):\n",
    "    cmd = path.replace(\n",
    "        f's3a://{BUCKET}',\n",
    "        f'rm -rf {PRJ_PATH}'\n",
    "    )\n",
    "    !{cmd}\n",
    "    return f'command to run: {cmd}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffaa2985-0b24-4b13-a90f-6584f77e709a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create filenames that will be used later when saving predictions \n",
    "\n",
    "current_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "current_user = os.environ['JUPYTERHUB_SERVICE_PREFIX']\n",
    "current_user = current_user.split(\"/\")[2]  \n",
    "unique_identifier = str(uuid.uuid4())[:8]  # Generate a unique identifier (first 8 characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3723b66f-323b-43ca-8ef1-95381160e23a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Filename Information:**\n",
      "- Current Date and Time: 20240425_062613\n",
      "- Current User: st083972\n",
      "- Unique Identifier: 9de7d825\n"
     ]
    }
   ],
   "source": [
    "print(\"**Filename Information:**\")\n",
    "print(f\"- Current Date and Time: {current_datetime}\")\n",
    "print(f\"- Current User: {current_user}\")\n",
    "print(f\"- Unique Identifier: {unique_identifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1718da-1149-4422-9160-9a96d3ea1be7",
   "metadata": {},
   "source": [
    "### 2.1. Load or preprocess data - `raw` stage\n",
    "\n",
    "2.1. Load or Preprocess Data - raw Stage\n",
    "\n",
    "This section checks the `PROC_DS` flag.\n",
    "\n",
    "If True, it reads raw CSV data from S3, parses timestamps, filters and flags payment events within a specific timeframe, and selects relevant columns.\n",
    "\n",
    "The processed data is then saved as a parquet file in S3 and the DataFrame is unloaded from memory.\n",
    "\n",
    "Finally, it reads the processed data from the parquet file and displays a few rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b1c50-3c9f-426b-9b07-321026d78c96",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Ensure dates used are as follows</span>\n",
    "\n",
    "flag_min_datetime = datetime.datetime(2024, 3, 21, 0, 0, 0)\n",
    "\n",
    "flag_max_datetime = datetime.datetime(2024, 4, 18, 23, 59, 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec139b21-815a-4680-a53e-c43a5e24d355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-21 00:00:00 2024-04-18 23:59:59\n",
      "CPU times: user 269 ms, sys: 80.3 ms, total: 350 ms\n",
      "Wall time: 20min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if PROC_DS:\n",
    "    sdf = spark.read.option('escape','\"').csv(f's3a://{BUCKET}/{files_mask}', header=True)\n",
    "    sdf = sdf.withColumn('event_datetime', F.to_timestamp(\"event_datetime\"))\n",
    "    flag_min_datetime = datetime.datetime(2024, 3, 21, 0, 0, 0)\n",
    "    flag_max_datetime = datetime.datetime(2024, 4, 18, 23, 59, 59)\n",
    "    print(flag_min_datetime, flag_max_datetime)\n",
    "    sdf = sdf.withColumn(\n",
    "        'payment_event_flag', \n",
    "        (\n",
    "            (F.col('event_name').like('%Мои штрафы/Оплата/Завершили оплату%') | \n",
    "            F.col('event_name').like('%Мои штрафы/Оплата/Платёж принят%')) &\n",
    "            F.col('event_datetime').between(flag_min_datetime, flag_max_datetime)\n",
    "        ).cast(\"int\")\n",
    "    )\n",
    "    sdf = sdf.select(\n",
    "        'profile_id',\n",
    "        'event_datetime',\n",
    "        'payment_event_flag',\n",
    "        'event_name'\n",
    "    )\n",
    "    sdf.repartition(1).write.parquet(file_path_ds)\n",
    "    sdf.unpersist()\n",
    "sdf = spark.read.parquet(file_path_ds)\n",
    "sdf.limit(5).toPandas()\n",
    "\n",
    "\n",
    "\n",
    "if not PROC_DS:\n",
    "    # Code to execute if PROC_DS is False\n",
    "    flag_min_datetime = datetime.datetime(2024, 3, 21, 0, 0, 0)\n",
    "    flag_max_datetime = datetime.datetime(2024, 4, 18, 23, 59, 59)\n",
    "    print(\"flag min datetime: \", flag_min_datetime, '\\n', \n",
    "          \"flag max datetime: \", flag_max_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f106cea-7908-4444-966a-5b0a04bdeb4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|payment_event_flag|    count|\n",
      "+------------------+---------+\n",
      "|                 0|755277302|\n",
      "|                 1|    45742|\n",
      "+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.groupBy('payment_event_flag').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a90e8d11-f747-4fca-8f00-043fc4726c56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20240425_065142-yynmh3ty</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/yynmh3ty' target=\"_blank\">ancient-disco-21</a></strong> to <a href='https://wandb.ai/gsom-diploma-jap/ray-diploma' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gsom-diploma-jap/ray-diploma' target=\"_blank\">https://wandb.ai/gsom-diploma-jap/ray-diploma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/yynmh3ty' target=\"_blank\">https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/yynmh3ty</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/yynmh3ty?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe0e81fd040>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init Weights and Biases to begin storing data \n",
    "\n",
    "wandb.init(project=\"ray-diploma\", config={\n",
    "    \"version\": VER,\n",
    "    \"proc_ds\": PROC_DS,\n",
    "    \"proc_lags\": PROC_LAGS,\n",
    "    \"proc_vecs\": PROC_VECS,\n",
    "    \"frac_0\": FRAC_0,\n",
    "    \"min_datetime\": flag_min_datetime,\n",
    "    \"max_datetime\": flag_max_datetime,\n",
    "    \"current_user\": current_user, \n",
    "    \"uuid\": unique_identifier,\n",
    "    # ... other common parameters ...\n",
    "}, \n",
    "           mode=\"online\", \n",
    "           dir=\"/home/jovyan/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64c8ec84-705b-4e15-b180-28ce1a8e586a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Settings {'_args': None, '_aws_lambda': False, '_async_upload_concurrency_limit': None, '_cli_only_mode': None, '_code_path_local': None, '_colab': False, '_cuda': None, '_disable_meta': False, '_disable_service': False, '_disable_setproctitle': False, '_disable_stats': False, '_disable_update_check': None, '_disable_viewer': None, '_disable_machine_info': False, '_except_exit': None, '_executable': None, '_extra_http_headers': None, '_file_stream_retry_max': 125, '_file_stream_retry_wait_min_seconds': 2.0, '_file_stream_retry_wait_max_seconds': 60.0, '_file_stream_timeout_seconds': 180.0, '_file_transfer_retry_max': 20, '_file_transfer_retry_wait_min_seconds': 2.0, '_file_transfer_retry_wait_max_seconds': 60.0, '_file_transfer_timeout_seconds': 0.0, '_flow_control_custom': False, '_flow_control_disabled': False, '_graphql_retry_max': 20, '_graphql_retry_wait_min_seconds': 2.0, '_graphql_retry_wait_max_seconds': 60.0, '_graphql_timeout_seconds': 30.0, '_internal_check_process': 8.0, '_internal_queue_timeout': 2.0, '_ipython': False, '_jupyter': True, '_jupyter_name': None, '_jupyter_path': None, '_jupyter_root': None, '_kaggle': False, '_live_policy_rate_limit': None, '_live_policy_wait_time': None, '_log_level': 10, '_network_buffer': None, '_noop': False, '_notebook': True, '_offline': False, '_sync': False, '_os': None, '_platform': 'linux', '_proxies': None, '_python': None, '_runqueue_item_id': None, '_require_core': False, '_save_requirements': True, '_service_transport': None, '_service_wait': 30.0, '_shared': False, '_start_datetime': None, '_start_time': None, '_stats_pid': None, '_stats_sample_rate_seconds': 2.0, '_stats_samples_to_average': 15, '_stats_join_assets': True, '_stats_neuron_monitor_config_path': None, '_stats_open_metrics_endpoints': None, '_stats_open_metrics_filters': ('.*',), '_stats_disk_paths': ('/',), '_stats_buffer_size': 0, '_tmp_code_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None/tmp/code', '_tracelog': None, '_unsaved_keys': None, '_windows': False, 'allow_val_change': None, 'anonymous': None, 'api_key': '***REDACTED***', 'azure_account_url_to_access_key': None, 'base_url': 'https://api.wandb.ai', 'code_dir': None, 'colab_url': None, 'config_paths': None, 'console': 'wrap', 'deployment': 'cloud', 'disable_code': False, 'disable_git': False, 'disable_hints': None, 'disable_job_creation': False, 'disabled': False, 'docker': None, 'email': None, 'entity': None, 'files_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None/files', 'force': None, 'fork_from': None, 'git_commit': None, 'git_remote': 'origin', 'git_remote_url': None, 'git_root': None, 'heartbeat_seconds': 30, 'host': None, 'ignore_globs': (), 'init_timeout': 90.0, 'is_local': False, 'job_name': None, 'job_source': None, 'label_disable': None, 'launch': None, 'launch_config_path': None, 'log_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None/logs', 'log_internal': '/home/jovyan/__RAYPFP/wandb/run-None-None/logs/debug-internal.log', 'log_symlink_internal': '/home/jovyan/__RAYPFP/wandb/debug-internal.log', 'log_symlink_user': '/home/jovyan/__RAYPFP/wandb/debug.log', 'log_user': '/home/jovyan/__RAYPFP/wandb/run-None-None/logs/debug.log', 'login_timeout': None, 'mode': 'online', 'notebook_name': None, 'problem': 'fatal', 'program': None, 'program_abspath': None, 'program_relpath': None, 'project': None, 'project_url': '', 'quiet': None, 'reinit': None, 'relogin': None, 'resume': None, 'resume_fname': '/home/jovyan/__RAYPFP/wandb/wandb-resume.json', 'resumed': False, 'root_dir': '/home/jovyan/__RAYPFP', 'run_group': None, 'run_id': None, 'run_job_type': None, 'run_mode': 'run', 'run_name': None, 'run_notes': None, 'run_tags': None, 'run_url': '', 'sagemaker_disable': None, 'save_code': None, 'settings_system': '/home/jovyan/.config/wandb/settings', 'settings_workspace': '/home/jovyan/__RAYPFP/wandb/settings', 'show_colors': None, 'show_emoji': None, 'show_errors': True, 'show_info': True, 'show_warnings': True, 'silent': False, 'start_method': None, 'strict': None, 'summary_errors': None, 'summary_timeout': 60, 'summary_warnings': 5, 'sweep_id': None, 'sweep_param_path': None, 'sweep_url': '', 'symlink': None, 'sync_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None', 'sync_file': '/home/jovyan/__RAYPFP/wandb/run-None-None/run-None.wandb', 'sync_symlink_latest': '/home/jovyan/__RAYPFP/wandb/latest-run', 'system_sample': 15, 'system_sample_seconds': 2, 'table_raise_on_max_row_limit_exceeded': False, 'timespec': None, 'tmp_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None/tmp', 'username': None, 'wandb_dir': '/home/jovyan/__RAYPFP/wandb/'}>\n"
     ]
    }
   ],
   "source": [
    "settings = wandb.Settings()\n",
    "print(settings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02f3f918-4948-4dff-9f42-ac6a2768af30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**After Initial Data Loading**\n",
      "   payment_event_flag      count\n",
      "0                   0  755277302\n",
      "1                   1      45742\n"
     ]
    }
   ],
   "source": [
    "# Weights and Biases logging here:\n",
    "\n",
    "counts = sdf.groupBy('payment_event_flag').count().toPandas()\n",
    "wandb.log({\"data_count\": counts})\n",
    "print(\"**After Initial Data Loading**\")\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c54c5-820c-4822-825f-4557981ad2b6",
   "metadata": {},
   "source": [
    "### 2.2. Load or preprocess data - `lags` stage\n",
    "\n",
    "2.2. Load or Preprocess Data - lags Stage\n",
    "\n",
    "The `dataset_lags` function defines window specifications for different time intervals (e.g., 10 minutes to 1 hour, 1 day to 3 days).\n",
    "\n",
    "It then uses these windows to calculate the list of event names within each time interval for each profile, creating lag features.\n",
    "\n",
    "If `PROC_LAGS` is True, the function samples the data based on the `payment_event_flag` and the specified fraction for negative examples.\n",
    "\n",
    "The processed data with lag features is saved as a parquet file and unloaded from memory.\n",
    "\n",
    "Finally, it reads the data with lags and displays the count of positive and negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546adde-a046-46d8-8fe9-1457ccd281e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fe103c1-8be5-4d1e-9cff-cf52060d5c97",
   "metadata": {},
   "source": [
    "## Lags new implementation \n",
    "\n",
    "## <span style=\"color: red;\">Do NOT change</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d69585d-eb48-4a8a-a1b3-e87618a5e488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataset_lags(sdf, shift=0):\n",
    "    hour = 60 * 60\n",
    "    day = 24 * 60 * 60\n",
    "\n",
    "    w_10min_to_1week = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-7 * day + shift, -10 * 60 + shift))\n",
    "    w_1week_to_2weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-14 * day + shift, -7 * day + shift))\n",
    "    w_2weeks_to_3weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-21 * day + shift, -14 * day + shift))\n",
    "    w_3weeks_to_4weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-28 * day + shift, -21 * day + shift))\n",
    "    w_4weeks_to_5weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-35 * day + shift, -28 * day + shift))\n",
    "    w_5weeks_to_6weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-42 * day + shift, -35 * day + shift))\n",
    "    w_6weeks_to_7weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-49 * day + shift, -42 * day + shift))\n",
    "    w_7weeks_to_8weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-56 * day + shift, -49 * day + shift))\n",
    "    w_8weeks_to_9weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-63 * day + shift, -56 * day + shift))\n",
    "    w_9weeks_to_10weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-70 * day + shift, -63 * day + shift))\n",
    "    w_10weeks_to_11weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-77 * day + shift, -70 * day + shift))\n",
    "    w_11weeks_to_12weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-84 * day + shift, -77 * day + shift))\n",
    "    w_12weeks_to_13weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-91 * day + shift, -84 * day + shift))\n",
    "    w_13weeks_to_14weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-98 * day + shift, -91 * day + shift))   \n",
    "    w_14weeks_to_15weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-105 * day + shift, -98 * day + shift))\n",
    "    w_15weeks_to_16weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-112 * day + shift, -105 * day + shift))\n",
    "    w_16weeks_to_17weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-119 * day + shift, -112 * day + shift))\n",
    "    w_17weeks_to_18weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-126 * day + shift, -119 * day + shift))\n",
    "    w_18weeks_to_19weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-133 * day + shift, -126 * day + shift))\n",
    "    w_19weeks_to_20weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-140 * day + shift, -133 * day + shift))\n",
    "    w_20weeks_to_21weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-147 * day + shift, -140 * day + shift))\n",
    "    w_21weeks_to_22weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-154 * day + shift, -147 * day + shift))\n",
    "    w_22weeks_to_23weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-161 * day + shift, -154 * day + shift))\n",
    "    w_23weeks_to_24weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-168 * day + shift, -161 * day + shift))\n",
    "    w_24weeks_to_25weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-175 * day + shift, -168 * day + shift))\n",
    "    w_25weeks_to_26weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-182 * day + shift, -175 * day + shift)) \n",
    "    w_26weeks_to_27weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-189 * day + shift, -182 * day + shift)) \n",
    "    \n",
    "    return (\n",
    "        sdf\n",
    "            #.withColumn('lag_10min_to_1week', F.collect_list('event_name').over(w_10min_to_1week))\n",
    "            .withColumn('lag_1week_to_2weeks', F.collect_list('event_name').over(w_1week_to_2weeks))\n",
    "            .withColumn('lag_2weeks_to_3weeks', F.collect_list('event_name').over(w_2weeks_to_3weeks))\n",
    "            .withColumn('lag_3weeks_to_4weeks', F.collect_list('event_name').over(w_3weeks_to_4weeks))\n",
    "            .withColumn('lag_4weeks_to_5weeks', F.collect_list('event_name').over(w_4weeks_to_5weeks))\n",
    "            .withColumn('lag_5weeks_to_6weeks', F.collect_list('event_name').over(w_5weeks_to_6weeks))\n",
    "            .withColumn('lag_6weeks_to_7weeks', F.collect_list('event_name').over(w_6weeks_to_7weeks))\n",
    "            .withColumn('lag_7weeks_to_8weeks', F.collect_list('event_name').over(w_7weeks_to_8weeks))\n",
    "            .withColumn('lag_8weeks_to_9weeks', F.collect_list('event_name').over(w_8weeks_to_9weeks))\n",
    "            .withColumn('lag_9weeks_to_10weeks', F.collect_list('event_name').over(w_9weeks_to_10weeks))\n",
    "            .withColumn('lag_10weeks_to_11weeks', F.collect_list('event_name').over(w_10weeks_to_11weeks))\n",
    "            .withColumn('lag_11weeks_to_12weeks', F.collect_list('event_name').over(w_11weeks_to_12weeks))\n",
    "            .withColumn('lag_12weeks_to_13weeks', F.collect_list('event_name').over(w_12weeks_to_13weeks))\n",
    "            .withColumn('lag_13weeks_to_14weeks', F.collect_list('event_name').over(w_13weeks_to_14weeks))\n",
    "            .withColumn('lag_14weeks_to_15weeks', F.collect_list('event_name').over(w_14weeks_to_15weeks))\n",
    "            .withColumn('lag_15weeks_to_16weeks', F.collect_list('event_name').over(w_15weeks_to_16weeks))\n",
    "            .withColumn('lag_16weeks_to_17weeks', F.collect_list('event_name').over(w_16weeks_to_17weeks))\n",
    "            .withColumn('lag_17weeks_to_18weeks', F.collect_list('event_name').over(w_17weeks_to_18weeks))\n",
    "            .withColumn('lag_18weeks_to_19weeks', F.collect_list('event_name').over(w_18weeks_to_19weeks))\n",
    "            .withColumn('lag_19weeks_to_20weeks', F.collect_list('event_name').over(w_19weeks_to_20weeks))\n",
    "            .withColumn('lag_20weeks_to_21weeks', F.collect_list('event_name').over(w_20weeks_to_21weeks))\n",
    "            .withColumn('lag_21weeks_to_22weeks', F.collect_list('event_name').over(w_21weeks_to_22weeks))\n",
    "            .withColumn('lag_22weeks_to_23weeks', F.collect_list('event_name').over(w_22weeks_to_23weeks))\n",
    "            .withColumn('lag_23weeks_to_24weeks', F.collect_list('event_name').over(w_23weeks_to_24weeks))\n",
    "            .withColumn('lag_24weeks_to_25weeks', F.collect_list('event_name').over(w_24weeks_to_25weeks))\n",
    "            .withColumn('lag_25weeks_to_26weeks', F.collect_list('event_name').over(w_25weeks_to_26weeks))\n",
    "            .withColumn('lag_26weeks_to_27weeks', F.collect_list('event_name').over(w_26weeks_to_27weeks))\n",
    "            .select(\n",
    "                'profile_id',\n",
    "                'event_datetime',\n",
    "                'payment_event_flag',\n",
    "                'event_name',\n",
    "                #'lag_10min_to_1week',\n",
    "                'lag_1week_to_2weeks',\n",
    "                'lag_2weeks_to_3weeks',\n",
    "                'lag_3weeks_to_4weeks',\n",
    "                'lag_4weeks_to_5weeks',\n",
    "                'lag_5weeks_to_6weeks',\n",
    "                'lag_6weeks_to_7weeks',\n",
    "                'lag_7weeks_to_8weeks',\n",
    "                'lag_8weeks_to_9weeks',\n",
    "                'lag_9weeks_to_10weeks',\n",
    "                'lag_10weeks_to_11weeks',\n",
    "                'lag_11weeks_to_12weeks',\n",
    "                'lag_12weeks_to_13weeks',\n",
    "                'lag_13weeks_to_14weeks',\n",
    "                'lag_14weeks_to_15weeks',\n",
    "                'lag_15weeks_to_16weeks',\n",
    "                'lag_16weeks_to_17weeks',\n",
    "                'lag_17weeks_to_18weeks',\n",
    "                'lag_18weeks_to_19weeks',\n",
    "                'lag_19weeks_to_20weeks',\n",
    "                'lag_20weeks_to_21weeks',\n",
    "                'lag_21weeks_to_22weeks',\n",
    "                'lag_22weeks_to_23weeks',\n",
    "                'lag_23weeks_to_24weeks',\n",
    "                'lag_24weeks_to_25weeks',\n",
    "                'lag_25weeks_to_26weeks',\n",
    "                'lag_26weeks_to_27weeks'\n",
    "            )\n",
    "        .orderBy(F.col('event_datetime'), ascending=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d70e8-d262-456b-8c7a-c7845b7adfe7",
   "metadata": {},
   "source": [
    "## -- New implementation of time lag windows -- \n",
    "\n",
    "Added the following code; \n",
    "\n",
    "*     clean_parquet(file_path_lags)\n",
    "*     dates  = (flag_min_datetime, flag_max_datetime)\n",
    "\n",
    "1. `clean_parquet(file_path_lags)`:\n",
    "\n",
    "This line of code is used to clean or remove any existing parquet files at the specified file_path_lags location before writing new data.\n",
    "\n",
    "By calling `clean_parquet(file_path_lags)` before writing the new data with time lag windows, it ensures that any previous data stored at the same location is removed, preventing any conflicts or data inconsistencies.\n",
    "\n",
    "**we have commented it out for this because we actually want to reuse the existing data when training and grid searching our models** \n",
    "\n",
    "2. `dates = (flag_min_datetime, flag_max_datetime):`\n",
    "\n",
    "This line creates a tuple named dates that contains two datetime values: flag_min_datetime and flag_max_datetime.\n",
    "\n",
    "The purpose of dates = (flag_min_datetime, flag_max_datetime) is to create a tuple that represents a date range for filtering the data. The values of flag_min_datetime and flag_max_datetime are defined earlier in the notebook – I.e. \n",
    "\n",
    "\"\"\"flag_min_datetime = datetime.datetime(2023, 8, 1, 0, 0, 0)\n",
    "flag_max_datetime = datetime.datetime(2023, 8, 31, 23, 59, 59)\n",
    "print(flag_min_datetime, flag_max_datetime)\"\"\" \n",
    "\n",
    "After creating the dates tuple, the code uses it to filter the sdf DataFrame based on the event_datetime column. \n",
    "\n",
    "The asterisk (*) before dates is used to unpack the tuple and pass the individual datetime values as arguments to the between function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1096c4e5-1885-4cd3-adde-38d6e407a93c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Old version before VG update to filter for dates\n",
    "\n",
    "\n",
    "\n",
    "if PROC_LAGS:\n",
    "    sdf = sdf.sampleBy(\n",
    "        'payment_event_flag', \n",
    "        fractions={0: FRAC_0, 1: 1}, \n",
    "        seed=2023\n",
    "    )\n",
    "    sdf = dataset_lags(sdf)\n",
    "    dates  = (flag_min_datetime, flag_max_datetime)\n",
    "    sdf = sdf.filter(sdf.event_datetime.between(*dates))\n",
    "    sdf = sdf.filter(\n",
    "        #(F.size('lag_10min_to_1week')      > 0) |\n",
    "        (F.size('lag_1week_to_2weeks')     > 0) |\n",
    "        (F.size('lag_2weeks_to_3weeks')    > 0) |\n",
    "        (F.size('lag_3weeks_to_4weeks')    > 0) |\n",
    "        (F.size('lag_4weeks_to_5weeks')    > 0) |\n",
    "        (F.size('lag_5weeks_to_6weeks')    > 0) |\n",
    "        (F.size('lag_6weeks_to_7weeks')    > 0) |\n",
    "        (F.size('lag_7weeks_to_8weeks')    > 0) |\n",
    "        (F.size('lag_8weeks_to_9weeks')    > 0) |\n",
    "        (F.size('lag_9weeks_to_10weeks')   > 0) |\n",
    "        (F.size('lag_10weeks_to_11weeks')  > 0) |\n",
    "        (F.size('lag_11weeks_to_12weeks')  > 0) |\n",
    "        (F.size('lag_12weeks_to_13weeks')  > 0) |\n",
    "        (F.size('lag_13weeks_to_14weeks')  > 0) |\n",
    "        (F.size('lag_14weeks_to_15weeks')  > 0) |\n",
    "        (F.size('lag_15weeks_to_16weeks')  > 0) |\n",
    "        (F.size('lag_16weeks_to_17weeks')  > 0) |\n",
    "        (F.size('lag_17weeks_to_18weeks')  > 0) |\n",
    "        (F.size('lag_18weeks_to_19weeks')  > 0) |\n",
    "        (F.size('lag_19weeks_to_20weeks')  > 0) |\n",
    "        (F.size('lag_20weeks_to_21weeks')  > 0) |\n",
    "        (F.size('lag_21weeks_to_22weeks')  > 0) |\n",
    "        (F.size('lag_22weeks_to_23weeks')  > 0) |\n",
    "        (F.size('lag_23weeks_to_24weeks')  > 0) |\n",
    "        (F.size('lag_24weeks_to_25weeks')  > 0) |\n",
    "        (F.size('lag_25weeks_to_26weeks')  > 0) |\n",
    "        (F.size('lag_26weeks_to_27weeks')  > 0) \n",
    "    )\n",
    "    clean_parquet(file_path_lags)\n",
    "    sdf.repartition(8).write.parquet(file_path_lags)\n",
    "    sdf.unpersist()\n",
    "sdf = spark.read.parquet(file_path_lags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad738be7-ee7f-44d1-b7f4-8d7d32a99835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weights and Biases logging here:\n",
    "counts = sdf.groupBy('payment_event_flag').count().toPandas()\n",
    "wandb.log({\"data_count_with_lags\": counts})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f346f86-6a48-468d-967f-88c67de8d420",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Double check your dataset has been reduced in the following cell</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "039dc7dd-985f-42c9-b2a7-455d3c586d49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Dataset Size After Lag Feature Creation** /n    payment_event_flag  count\n",
      "0                   1  23083\n",
      "1                   0  19471\n"
     ]
    }
   ],
   "source": [
    "print(\"**Dataset Size After Lag Feature Creation**\", '/n', counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92ef1637-35b4-43bc-85e3-badc3fdf5eab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- profile_id: string (nullable = true)\n",
      " |-- event_datetime: timestamp (nullable = true)\n",
      " |-- payment_event_flag: integer (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- lag_1week_to_2weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_2weeks_to_3weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_3weeks_to_4weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_4weeks_to_5weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_5weeks_to_6weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_6weeks_to_7weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_7weeks_to_8weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_8weeks_to_9weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_9weeks_to_10weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_10weeks_to_11weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_11weeks_to_12weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_12weeks_to_13weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_13weeks_to_14weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_14weeks_to_15weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_15weeks_to_16weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_16weeks_to_17weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_17weeks_to_18weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_18weeks_to_19weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_19weeks_to_20weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_20weeks_to_21weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_21weeks_to_22weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_22weeks_to_23weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_23weeks_to_24weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_24weeks_to_25weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_25weeks_to_26weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_26weeks_to_27weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3b490-5702-4c20-883e-613a7d5fa3c0",
   "metadata": {},
   "source": [
    "### Train test split process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8de83425-0fbd-46bc-978a-cfad7ce0c311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define train test split function \n",
    "\n",
    "def stratified_split(sdf, frac, label, seed=2023):\n",
    "    zeros = sdf.filter(sdf[label] == 0)\n",
    "    ones = sdf.filter(sdf[label] == 1)\n",
    "    train_, test_ = zeros.randomSplit([1 - frac, frac], seed=seed)\n",
    "    train, test = ones.randomSplit([1 - frac, frac], seed=seed)\n",
    "    train = train.union(train_)\n",
    "    test = test.union(test_)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa94f255-3f3a-453b-aad9-8772ce8f5a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conduct train test split \n",
    "\n",
    "sdf_train, sdf_test = stratified_split(\n",
    "    sdf, \n",
    "    frac=.2, # Size of the test dataset\n",
    "    label='payment_event_flag',\n",
    "    seed=2023\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4c0e6-8257-4ef1-8110-8128ea5dbaf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color: red;\">Check your data has been split and classes are approx equal</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "501e35b0-bd8f-4318-a973-155b4fd72a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>18435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>15552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag  count\n",
       "0                   1  18435\n",
       "1                   0  15552"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_train.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "776b6d5a-10bc-4e1c-9460-65e0e204ef82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag  count\n",
       "0                   1   4648\n",
       "1                   0   3919"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_test.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf9adc0b-6be7-4e8c-a092-19f282425ded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weights and Biases logging here:\n",
    "\n",
    "train_counts = sdf_train.groupBy('payment_event_flag').count().toPandas()\n",
    "wandb.log({\"train_data_count\": train_counts})\n",
    "\n",
    "test_counts = sdf_test.groupBy('payment_event_flag').count().toPandas()\n",
    "wandb.log({\"test_data_count\": test_counts})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf01e30-f5ef-40dc-99c7-99e99dc27974",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color: red;\">Ensure date ranges used are the ones you set earlier</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ed66acb-2642-463d-870b-d3d0aef7e33a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Date: 2024-03-21 00:00:17\n",
      "Maximum Date: 2024-04-18 23:59:23\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "# Find the minimum and maximum dates\n",
    "min_date = sdf.agg(min(\"event_datetime\")).collect()[0][0]\n",
    "max_date = sdf.agg(max(\"event_datetime\")).collect()[0][0]\n",
    "\n",
    "print(f\"Minimum Date: {min_date}\")\n",
    "print(f\"Maximum Date: {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6541e050-fb85-43bc-9e12-bb25cfbb7a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Minimum Date: 2024-03-21 00:00:17\n",
      "Training Set Maximum Date: 2024-04-18 23:59:23\n"
     ]
    }
   ],
   "source": [
    "# Training set\n",
    "train_min_date = sdf_train.agg(min(\"event_datetime\")).collect()[0][0]\n",
    "train_max_date = sdf_train.agg(max(\"event_datetime\")).collect()[0][0]\n",
    "\n",
    "print(f\"Training Set Minimum Date: {train_min_date}\")\n",
    "print(f\"Training Set Maximum Date: {train_max_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80751428-6358-472c-a4f0-65e9f14ceeae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Minimum Date: 2024-03-21 00:01:28\n",
      "Test Set Maximum Date: 2024-04-18 23:57:47\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "test_min_date = sdf_test.agg(min(\"event_datetime\")).collect()[0][0]\n",
    "test_max_date = sdf_test.agg(max(\"event_datetime\")).collect()[0][0]\n",
    "\n",
    "print(f\"Test Set Minimum Date: {test_min_date}\")\n",
    "print(f\"Test Set Maximum Date: {test_max_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9856ef-8571-467e-871f-70cbf66e6e6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3. Load or preprocess data - `vectorize` stage\n",
    "\n",
    "2.3. Load or Preprocess Data - vectorize Stage\n",
    "\n",
    "This section defines a list of lag features to be used.\n",
    "\n",
    "The datasets_tfidf function performs `TF-IDF` vectorization on the lag features for both training and test datasets.\n",
    "\n",
    "It uses `HashingTF` to convert lists of event names into numerical feature vectors and `IDF` to rescale the features based on their document frequency.\n",
    "\n",
    "The function also creates a dictionary mapping feature indices to the corresponding event names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62b8a249-c54f-493f-9679-baaa4e3dff6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lags = [\n",
    "#    'lag_10min_to_1week',\n",
    "    'lag_1week_to_2weeks',\n",
    "    'lag_2weeks_to_3weeks',\n",
    "    'lag_3weeks_to_4weeks',\n",
    "    'lag_4weeks_to_5weeks',\n",
    "    'lag_5weeks_to_6weeks',\n",
    "    'lag_6weeks_to_7weeks',\n",
    "    'lag_7weeks_to_8weeks',\n",
    "    'lag_8weeks_to_9weeks',\n",
    "    'lag_9weeks_to_10weeks',\n",
    "    'lag_10weeks_to_11weeks',\n",
    "    'lag_11weeks_to_12weeks',\n",
    "    'lag_12weeks_to_13weeks',\n",
    "    'lag_13weeks_to_14weeks',\n",
    "    'lag_14weeks_to_15weeks',\n",
    "    'lag_15weeks_to_16weeks',\n",
    "    'lag_16weeks_to_17weeks',\n",
    "    'lag_17weeks_to_18weeks',\n",
    "    'lag_18weeks_to_19weeks',\n",
    "    'lag_19weeks_to_20weeks',\n",
    "    'lag_20weeks_to_21weeks',\n",
    "    'lag_21weeks_to_22weeks',\n",
    "    'lag_22weeks_to_23weeks',\n",
    "    'lag_23weeks_to_24weeks',\n",
    "    'lag_24weeks_to_25weeks',\n",
    "    'lag_25weeks_to_26weeks',\n",
    "    'lag_26weeks_to_27weeks'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02591fe2-1f8b-4331-868a-96560fdb17b9",
   "metadata": {},
   "source": [
    "## TF-IDF implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddc7853f-0b02-4982-84ca-a58d91b8e6c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datasets_tfidf(sdf_train, sdf_test, lags, min_freq=3, num_features=10):\n",
    "    features_dict = {}\n",
    "    count = 0\n",
    "    for lag in tqdm(lags):\n",
    "        hashingTF = HashingTF(\n",
    "            inputCol=lag, \n",
    "            outputCol=lag + '_tf', \n",
    "            numFeatures=num_features\n",
    "        )\n",
    "        featurizedData = hashingTF.transform(sdf_train)\n",
    "        idf = IDF(\n",
    "            inputCol=lag + '_tf', \n",
    "            outputCol=lag + '_tfidf',\n",
    "            minDocFreq=min_freq  \n",
    "        )\n",
    "        idfModel = idf.fit(featurizedData)\n",
    "        sdf_train = idfModel.transform(featurizedData)\n",
    "        sdf_test = idfModel.transform(\n",
    "            hashingTF.transform(sdf_test)\n",
    "        )\n",
    "        events = [\n",
    "            x\n",
    "            for xs in sdf_train.select(lag).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            for x in xs\n",
    "        ]\n",
    "        hash_dict = {}\n",
    "        for e in events:\n",
    "            hash_dict[lag + '_' + e] = hashingTF.indexOf(e)\n",
    "        for feat_num in range(num_features):\n",
    "            tmp_list = []\n",
    "            for k, v in hash_dict.items():\n",
    "                if v == feat_num: tmp_list.append(k)\n",
    "            features_dict[count * num_features + feat_num] = tmp_list\n",
    "        count += 1\n",
    "    return sdf_train, sdf_test, features_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b0500-bcea-44f6-9b39-873654df40de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Breakdown of PROC_VECS code: \n",
    "\n",
    "\n",
    "**Conditional Execution:**\n",
    "\n",
    "• if PROC_VECS:: The code within this block is executed only if the PROC_VECS flag is set to True. This flag controls whether TF-IDF vectorization is performed on the data.\n",
    "\n",
    "**TF-IDF Vectorization:**\n",
    "\n",
    "`sdf_train, sdf_test, vectorizers = datasets_tfidf(...)`: This line calls the datasets_tfidf function, which performs TF-IDF vectorization on the lag features present in the sdf_train and sdf_test DataFrames.\n",
    "* The lags argument provides the list of lag feature column names to be vectorized.\n",
    "* The vec_size=10 argument specifies the desired dimensionality (number of features) of the resulting TF-IDF vectors.\n",
    "\n",
    "**The function returns three values:**\n",
    "\n",
    "* `sdf_train`: The training DataFrame with the added TF-IDF vector columns.\n",
    "* `sdf_test`: The test DataFrame with the added TF-IDF vector columns.\n",
    "* `vectorizers`: A list of fitted TF-IDF vectorizer models (one for each lag feature).\n",
    "\n",
    "**Cleaning and Saving Parquet Files:**\n",
    "\n",
    "`clean_parquet(file_path_trn)`: This line calls a function (not shown) to clean up any existing Parquet files at the specified path (file_path_trn) before saving the new data.\n",
    "\n",
    "`sdf_train.repartition(8).write.parquet(file_path_trn)`: The training DataFrame (sdf_train) is repartitioned into 8 partitions for optimized writing.\n",
    "\n",
    "* The write.parquet method saves the DataFrame as a Parquet file at the specified path (file_path_trn).\n",
    "* The same process is repeated for the test DataFrame (sdf_test) using file_path_tst.\n",
    "\n",
    "**Unpersisting DataFrames:**\n",
    "\n",
    "* `sdf_train.unpersist(), sdf_test.unpersist()`: These lines remove the DataFrames from Spark's memory. Since the data has been saved to disk, it can be reloaded later if needed, freeing up memory for subsequent processing.\n",
    "\n",
    "**Reloading DataFrames (if necessary):**\n",
    "\n",
    "* `sdf_train = spark.read.parquet(file_path_trn)`: This line reloads the training data from the saved Parquet file if it's not already in memory.\n",
    "\n",
    "The same is done for the test data using file_path_tst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f0b3cb5-737c-4aed-a668-827b9ca58f66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385f4503ec1746eba3baca84de605813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NEW version with clean_parquet (TF-IDF)\n",
    "\n",
    "if PROC_VECS:\n",
    "    sdf_train, sdf_test, features_dict = datasets_tfidf(\n",
    "        sdf_train,\n",
    "        sdf_test,\n",
    "        lags,\n",
    "        #vec_size=10,\n",
    "        min_freq=3,\n",
    "        num_features=100\n",
    "    )\n",
    "    clean_parquet(file_path_trn)\n",
    "    sdf_train.repartition(8).write.parquet(file_path_trn)\n",
    "    clean_parquet(file_path_tst)\n",
    "    sdf_test.repartition(8).write.parquet(file_path_tst)\n",
    "    sdf_train.unpersist()\n",
    "    sdf_test.unpersist()\n",
    "sdf_train = spark.read.parquet(file_path_trn)\n",
    "sdf_test = spark.read.parquet(file_path_tst)\n",
    "\n",
    "\n",
    "if not PROC_VECS:\n",
    "    sdf_train, sdf_test, features_dict = datasets_tfidf(\n",
    "        sdf_train, \n",
    "        sdf_test, \n",
    "        lags, \n",
    "        min_freq=3,\n",
    "        num_features=100\n",
    "    )\n",
    "    print(len(features_dict.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf769561-8968-4530-83f0-4243e0c588d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Training Set After TF-IDF**\n",
      "+------------------+-----+\n",
      "|payment_event_flag|count|\n",
      "+------------------+-----+\n",
      "|                 1|18435|\n",
      "|                 0|15552|\n",
      "+------------------+-----+\n",
      "\n",
      "**Testing Set After TF-IDF**\n",
      "+------------------+-----+\n",
      "|payment_event_flag|count|\n",
      "+------------------+-----+\n",
      "|                 1| 4648|\n",
      "|                 0| 3919|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check data size after reloading\n",
    "print(\"**Training Set After TF-IDF**\")\n",
    "sdf_train.groupBy('payment_event_flag').count().show()\n",
    "print(\"**Testing Set After TF-IDF**\")\n",
    "sdf_test.groupBy('payment_event_flag').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a666c70-fcb7-4f5c-83fa-5c5c29d9d007",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- profile_id: string (nullable = true)\n",
      " |-- event_datetime: timestamp (nullable = true)\n",
      " |-- payment_event_flag: integer (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- lag_1week_to_2weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_2weeks_to_3weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_3weeks_to_4weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_4weeks_to_5weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_5weeks_to_6weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_6weeks_to_7weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_7weeks_to_8weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_8weeks_to_9weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_9weeks_to_10weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_10weeks_to_11weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_11weeks_to_12weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_12weeks_to_13weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_13weeks_to_14weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_14weeks_to_15weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_15weeks_to_16weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_16weeks_to_17weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_17weeks_to_18weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_18weeks_to_19weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_19weeks_to_20weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_20weeks_to_21weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_21weeks_to_22weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_22weeks_to_23weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_23weeks_to_24weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_24weeks_to_25weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_25weeks_to_26weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_26weeks_to_27weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_1week_to_2weeks_tf: vector (nullable = true)\n",
      " |-- lag_1week_to_2weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_2weeks_to_3weeks_tf: vector (nullable = true)\n",
      " |-- lag_2weeks_to_3weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_3weeks_to_4weeks_tf: vector (nullable = true)\n",
      " |-- lag_3weeks_to_4weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_4weeks_to_5weeks_tf: vector (nullable = true)\n",
      " |-- lag_4weeks_to_5weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_5weeks_to_6weeks_tf: vector (nullable = true)\n",
      " |-- lag_5weeks_to_6weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_6weeks_to_7weeks_tf: vector (nullable = true)\n",
      " |-- lag_6weeks_to_7weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_7weeks_to_8weeks_tf: vector (nullable = true)\n",
      " |-- lag_7weeks_to_8weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_8weeks_to_9weeks_tf: vector (nullable = true)\n",
      " |-- lag_8weeks_to_9weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_9weeks_to_10weeks_tf: vector (nullable = true)\n",
      " |-- lag_9weeks_to_10weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_10weeks_to_11weeks_tf: vector (nullable = true)\n",
      " |-- lag_10weeks_to_11weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_11weeks_to_12weeks_tf: vector (nullable = true)\n",
      " |-- lag_11weeks_to_12weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_12weeks_to_13weeks_tf: vector (nullable = true)\n",
      " |-- lag_12weeks_to_13weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_13weeks_to_14weeks_tf: vector (nullable = true)\n",
      " |-- lag_13weeks_to_14weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_14weeks_to_15weeks_tf: vector (nullable = true)\n",
      " |-- lag_14weeks_to_15weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_15weeks_to_16weeks_tf: vector (nullable = true)\n",
      " |-- lag_15weeks_to_16weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_16weeks_to_17weeks_tf: vector (nullable = true)\n",
      " |-- lag_16weeks_to_17weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_17weeks_to_18weeks_tf: vector (nullable = true)\n",
      " |-- lag_17weeks_to_18weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_18weeks_to_19weeks_tf: vector (nullable = true)\n",
      " |-- lag_18weeks_to_19weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_19weeks_to_20weeks_tf: vector (nullable = true)\n",
      " |-- lag_19weeks_to_20weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_20weeks_to_21weeks_tf: vector (nullable = true)\n",
      " |-- lag_20weeks_to_21weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_21weeks_to_22weeks_tf: vector (nullable = true)\n",
      " |-- lag_21weeks_to_22weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_22weeks_to_23weeks_tf: vector (nullable = true)\n",
      " |-- lag_22weeks_to_23weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_23weeks_to_24weeks_tf: vector (nullable = true)\n",
      " |-- lag_23weeks_to_24weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_24weeks_to_25weeks_tf: vector (nullable = true)\n",
      " |-- lag_24weeks_to_25weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_25weeks_to_26weeks_tf: vector (nullable = true)\n",
      " |-- lag_25weeks_to_26weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_26weeks_to_27weeks_tf: vector (nullable = true)\n",
      " |-- lag_26weeks_to_27weeks_tfidf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab3b06b-98ae-4212-89c1-e42c7b696e89",
   "metadata": {},
   "source": [
    "## -- end TF-IDF --\n",
    "\n",
    "## Word2Vec implementaion \n",
    "\n",
    "#### <span style=\"color: red;\">We are not currently using word2vec</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52bbf517-2d00-449e-943a-966cacaea33d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from pyspark.ml.feature import Word2Vec\\n\\ndef datasets_vecorized(sdf_train, sdf_test, lags, vec_size=10):\\n    vectorizers = []\\n    for lag in tqdm(lags):\\n        word2Vec = Word2Vec(\\n            vectorSize=vec_size,\\n            minCount=0,\\n            inputCol=lag,\\n            outputCol=lag + '_vec'\\n        )\\n        vectorizer = word2Vec.fit(sdf_train)\\n        sdf_train = vectorizer.transform(sdf_train)\\n        sdf_test = vectorizer.transform(sdf_test)\\n        vectorizers.append(vectorizer)\\n    return sdf_train, sdf_test, vectorizers\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "def datasets_vecorized(sdf_train, sdf_test, lags, vec_size=10):\n",
    "    vectorizers = []\n",
    "    for lag in tqdm(lags):\n",
    "        word2Vec = Word2Vec(\n",
    "            vectorSize=vec_size,\n",
    "            minCount=0,\n",
    "            inputCol=lag,\n",
    "            outputCol=lag + '_vec'\n",
    "        )\n",
    "        vectorizer = word2Vec.fit(sdf_train)\n",
    "        sdf_train = vectorizer.transform(sdf_train)\n",
    "        sdf_test = vectorizer.transform(sdf_test)\n",
    "        vectorizers.append(vectorizer)\n",
    "    return sdf_train, sdf_test, vectorizers\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25830c0a-d5d4-49a9-b692-a7021e661caf",
   "metadata": {},
   "source": [
    "## -- End word2vec -- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6b9e0-0e7b-4459-ab37-e970f26fb3d6",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd83edd-91ab-4364-b7a3-64abcd9def7f",
   "metadata": {},
   "source": [
    "### 3.1. Features assembling\n",
    "\n",
    "3.1. Features Assembling\n",
    "\n",
    "The features_assembled function prepares the data for model training:\n",
    "\n",
    "* It selects the `TF-IDF` features and the target variable (`payment_event_flag`).\n",
    "* It uses VectorAssembler to combine the `TF-IDF` features into a single vector column named `features`.\n",
    "* It returns a DataFrame with the target variable and the assembled feature vector.\n",
    "\n",
    "The upsampled function can be used to address class imbalance:\n",
    "\n",
    "* It separates the data into positive and negative examples.\n",
    "* It duplicates the positive examples to achieve a balanced class distribution based on the `UPSAMPLE` setting.\n",
    "\n",
    "The code then:\n",
    "* Defines a list of lag features to be used.\n",
    "* Assembles features for both training and test sets.\n",
    "* Optionally performs **upsampling** on the training set (and potentially the test set) if `UPSAMPLE` is enabled.\n",
    "* Displays the class distribution after upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f662437f-0848-4e87-83f6-28edba536c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def features_assembled(sdf, feats):\n",
    "    cols_to_model = [x + '_tfidf' for x in feats]\n",
    "    cols_to_model.extend(['payment_event_flag'])\n",
    "    print('columns to model:', cols_to_model)\n",
    "    vecAssembler = VectorAssembler(\n",
    "        inputCols=[c for c in cols_to_model if c != 'payment_event_flag'], \n",
    "        outputCol='features'\n",
    "    )\n",
    "    features = sdf.select(cols_to_model)\n",
    "    features_vec = vecAssembler.transform(features)\n",
    "    features_data = features_vec.select('payment_event_flag', 'features')\n",
    "    return features_data\n",
    "\n",
    "def upsampled(sdf, label, upsample='max'):\n",
    "    zeros = sdf.filter(sdf[label] == 0)\n",
    "    ones = sdf.filter(sdf[label] == 1)\n",
    "    res = zeros.union(ones)\n",
    "    if upsample == 'max':\n",
    "        up_count = int(zeros.count() / ones.count())\n",
    "        for _ in range(up_count - 1):\n",
    "            res = res.union(ones)\n",
    "    else:\n",
    "        for _ in range(upsample - 1):\n",
    "            res = res.union(ones)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b29bfde6-70c1-4272-8bc6-0e317a673776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"MAX\" Strategy: Setting UPSAMPLE = 'max' instructs the upsampled function to duplicate the minority class examples\n",
    "# They are upsampled until their count becomes equal to the count of the majority class. \n",
    "# In other words, it aims for a 1:1 class ratio.\n",
    "\n",
    "UPSAMPLE = 'max' # Can be either 'none' or 'max'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aefefcd5-8048-4120-93fa-9687290ada7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns to model: ['lag_1week_to_2weeks_tfidf', 'lag_2weeks_to_3weeks_tfidf', 'lag_3weeks_to_4weeks_tfidf', 'lag_4weeks_to_5weeks_tfidf', 'lag_5weeks_to_6weeks_tfidf', 'lag_6weeks_to_7weeks_tfidf', 'lag_7weeks_to_8weeks_tfidf', 'lag_8weeks_to_9weeks_tfidf', 'lag_9weeks_to_10weeks_tfidf', 'lag_10weeks_to_11weeks_tfidf', 'lag_11weeks_to_12weeks_tfidf', 'lag_12weeks_to_13weeks_tfidf', 'lag_13weeks_to_14weeks_tfidf', 'lag_14weeks_to_15weeks_tfidf', 'lag_15weeks_to_16weeks_tfidf', 'lag_16weeks_to_17weeks_tfidf', 'lag_17weeks_to_18weeks_tfidf', 'lag_18weeks_to_19weeks_tfidf', 'lag_19weeks_to_20weeks_tfidf', 'lag_20weeks_to_21weeks_tfidf', 'lag_21weeks_to_22weeks_tfidf', 'lag_22weeks_to_23weeks_tfidf', 'lag_23weeks_to_24weeks_tfidf', 'lag_24weeks_to_25weeks_tfidf', 'lag_25weeks_to_26weeks_tfidf', 'lag_26weeks_to_27weeks_tfidf', 'payment_event_flag']\n",
      "columns to model: ['lag_1week_to_2weeks_tfidf', 'lag_2weeks_to_3weeks_tfidf', 'lag_3weeks_to_4weeks_tfidf', 'lag_4weeks_to_5weeks_tfidf', 'lag_5weeks_to_6weeks_tfidf', 'lag_6weeks_to_7weeks_tfidf', 'lag_7weeks_to_8weeks_tfidf', 'lag_8weeks_to_9weeks_tfidf', 'lag_9weeks_to_10weeks_tfidf', 'lag_10weeks_to_11weeks_tfidf', 'lag_11weeks_to_12weeks_tfidf', 'lag_12weeks_to_13weeks_tfidf', 'lag_13weeks_to_14weeks_tfidf', 'lag_14weeks_to_15weeks_tfidf', 'lag_15weeks_to_16weeks_tfidf', 'lag_16weeks_to_17weeks_tfidf', 'lag_17weeks_to_18weeks_tfidf', 'lag_18weeks_to_19weeks_tfidf', 'lag_19weeks_to_20weeks_tfidf', 'lag_20weeks_to_21weeks_tfidf', 'lag_21weeks_to_22weeks_tfidf', 'lag_22weeks_to_23weeks_tfidf', 'lag_23weeks_to_24weeks_tfidf', 'lag_24weeks_to_25weeks_tfidf', 'lag_25weeks_to_26weeks_tfidf', 'lag_26weeks_to_27weeks_tfidf', 'payment_event_flag']\n"
     ]
    }
   ],
   "source": [
    "# Setting feats, make sure to comment out (#) any lags we will not use for train/pred \n",
    "\n",
    "feats = [\n",
    "#    'lag_10min_to_1week',\n",
    "    'lag_1week_to_2weeks',\n",
    "    'lag_2weeks_to_3weeks',\n",
    "    'lag_3weeks_to_4weeks',\n",
    "    'lag_4weeks_to_5weeks',\n",
    "    'lag_5weeks_to_6weeks',\n",
    "    'lag_6weeks_to_7weeks',\n",
    "    'lag_7weeks_to_8weeks',\n",
    "    'lag_8weeks_to_9weeks',\n",
    "    'lag_9weeks_to_10weeks',\n",
    "    'lag_10weeks_to_11weeks',\n",
    "    'lag_11weeks_to_12weeks',\n",
    "    'lag_12weeks_to_13weeks',\n",
    "    'lag_13weeks_to_14weeks',\n",
    "    'lag_14weeks_to_15weeks',\n",
    "    'lag_15weeks_to_16weeks',\n",
    "    'lag_16weeks_to_17weeks',\n",
    "    'lag_17weeks_to_18weeks',\n",
    "    'lag_18weeks_to_19weeks',\n",
    "    'lag_19weeks_to_20weeks',\n",
    "    'lag_20weeks_to_21weeks',\n",
    "    'lag_21weeks_to_22weeks',\n",
    "    'lag_22weeks_to_23weeks',\n",
    "    'lag_23weeks_to_24weeks',\n",
    "    'lag_24weeks_to_25weeks',\n",
    "    'lag_25weeks_to_26weeks',\n",
    "    'lag_26weeks_to_27weeks'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "features_train = features_assembled(sdf_train, feats=feats)\n",
    "features_test = features_assembled(sdf_test, feats=feats)\n",
    "\n",
    "if UPSAMPLE:\n",
    "    features_train = upsampled(\n",
    "        features_train, \n",
    "        label='payment_event_flag', \n",
    "        upsample=UPSAMPLE\n",
    "    )\n",
    "    # Use to upsample test set\n",
    "    #features_test = upsampled(\n",
    "    #    features_test, \n",
    "    #    label='payment_event_flag', \n",
    "    #    upsample=UPSAMPLE\n",
    "    #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "388d38ed-6ced-4935-9762-3d0941a29e58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag  count\n",
       "0                   0  15552\n",
       "1                   1  18435"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3f2dfc9-ed4f-4139-9074-c986b12cdc7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag  count\n",
       "0                   1   4648\n",
       "1                   0   3919"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2864416b-80b2-4117-9938-044d98cd51a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag                                           features\n",
       "0                   0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "1                   0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2                   0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d82501c0-c4f1-43f8-a2fe-2b2b643f273d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(2600, {38: 2.4797, 145: 4.0024, 248: 5.0819, 1945: 4.1552, 2045: 4.0587})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.limit(3).toPandas()['features'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4814c-5ddf-46e7-bea6-03c45f94cac7",
   "metadata": {},
   "source": [
    "### 3.2. Training and evaluating\n",
    "\n",
    "#### <span style=\"color: red;\">This will train and evaluate WITHOUT hyperopt</span>\n",
    "\n",
    "\n",
    "A `RandomForestClassifier` is initialized with the following settings:\n",
    "\n",
    "* `labelCol`: Specifies the target variable column (\"`payment_event_flag`\").\n",
    "* `featuresCol`: Specifies the feature vector column (\"`features`\").\n",
    "* `numTrees`: Sets the number of trees in the random forest to 100.\n",
    "* `maxDepth`: Sets the maximum depth of each tree to 16.\n",
    "\n",
    "* The model is trained using the fit method on the training data.\n",
    "* The trained model is used to make predictions on the test data.\n",
    "\n",
    "The `BinaryClassificationMetrics` class is used to calculate evaluation metrics:\n",
    "\n",
    "* `areaUnderROC`: Area under the ROC curve, which measures the model's ability to distinguish between classes.\n",
    "* `areaUnderPR`: Area under the Precision-Recall curve, which is more informative for imbalanced datasets.\n",
    "\n",
    "The code also uses `classification_report` from `scikit-learn` to get a detailed report including precision, recall, F1-score, and support for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470aac0-f989-4394-845d-e44141d29cec",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Choose your model type from the options below</span>\n",
    "\n",
    "`# Options include`\n",
    "`## \"LinearSVCModel\"`\n",
    "`## \"GBTClassificationModel\"`\n",
    "`## \"LogisticRegressionModel\" `\n",
    "`## \"RandomForestClassificationModel\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b82127ac-bfc4-4e16-9833-a713fd405d28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Prediction File Path: s3a://pvc-84ea79a0-dc20-4a2d-86ab-f83c1f8d4a7b/work/vJ1rf/preds_RandomForestClassificationModel_20240425_062613_st083972_9de7d825.csv\n"
     ]
    }
   ],
   "source": [
    "# Define model filepath to link predictions results with the actual CSV \n",
    "\n",
    "current_model = \"RandomForestClassificationModel\" # <-- place your model name here\n",
    "# Options include \n",
    "## \"LinearSVCModel\"\n",
    "## \"GBTClassificationModel\"\n",
    "## \"LogisticRegressionModel\" \n",
    "## \"RandomForestClassificationModel\"\n",
    "\n",
    "file_path_pred = f's3a://{BUCKET}/work/{VER}/preds_{current_model}_{current_datetime}_{current_user}_{unique_identifier}.csv'\n",
    "print(f\"- Prediction File Path: {file_path_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df63ca-802d-4f1b-bfb6-9fc813dd36b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color: red;\">Update code for your own model</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d70016c5-a214-4b56-9a21-76708bdb760c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RF implementation with W&B logging \n",
    "rf = RandomForestClassifier(labelCol=\"payment_event_flag\", featuresCol=\"features\", numTrees=100, maxDepth=16)\n",
    "\n",
    "wandb.config.update({\n",
    "    \"upsample\": UPSAMPLE,\n",
    "    \"model_type\": type(rf).__name__,\n",
    "    \"num_trees\": rf.getNumTrees,\n",
    "    \"max_depth\": rf.getMaxDepth,\n",
    "    \"lags\": lags,\n",
    "    \"num_features\": len(features_dict.items()),\n",
    "    \"output_file\": file_path_pred\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8aea5479-7980-4c0e-b192-91c94baeb3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 ms, sys: 0 ns, total: 15.8 ms\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = rf.fit(features_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31002e97-3c87-4d83-9559-81396bb4a860",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.5737474466817074\n",
      "Area under PR-curve: 0.583466858336571\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(features_test)\n",
    "payment_event_flag_preds = predictions.select('prediction', 'payment_event_flag')\n",
    "metrics = BinaryClassificationMetrics(\n",
    "    payment_event_flag_preds.rdd.map(\n",
    "        lambda lines: [float(x) for x in lines]\n",
    "    )\n",
    ")\n",
    "print('ROC AUC:', metrics.areaUnderROC)\n",
    "print('Area under PR-curve:', metrics.areaUnderPR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2bb88cb0-f5de-4c18-9f69-704a50145318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weights and Biases logging here\n",
    "wandb.log({\"roc_auc\": metrics.areaUnderROC})\n",
    "wandb.log({\"pr_auc\": metrics.areaUnderPR})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c8b6414-d70b-40a6-9786-e6d11ac93be4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.22      0.33      3919\n",
      "           1       0.58      0.93      0.72      4648\n",
      "\n",
      "    accuracy                           0.60      8567\n",
      "   macro avg       0.66      0.57      0.53      8567\n",
      "weighted avg       0.65      0.60      0.54      8567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate classification report\n",
    "y_true = predictions.select('payment_event_flag').rdd.map(lambda x: x['payment_event_flag']).collect()\n",
    "y_pred = predictions.select('prediction').rdd.map(lambda x: x['prediction']).collect()\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7236e67c-19ff-4c17-9fe0-a03786a851df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weights and Biases logging here\n",
    "wandb.log({\"classification_report\": wandb.Html(report)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8575d210-a901-45ba-b5f9-b14a65043dd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dict_to_html_table(data):\n",
    "    \"\"\"Converts a dictionary into an HTML table.\"\"\"\n",
    "    html = \"\"\n",
    "    for key, value in data.items():\n",
    "        html += f\"\"\n",
    "    html += \"{key}{value}\"\n",
    "    return html\n",
    "\n",
    "# Example Usage (after calculating classification report for RF):\n",
    "\n",
    "rf_report = classification_report(y_true, y_pred, output_dict=True)  # Get RF report as a dictionary\n",
    "rf_report_html = dict_to_html_table(rf_report)\n",
    "\n",
    "wandb.log({\"rf_classification_report_html\": wandb.Html(rf_report_html)})  # Log as HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5384298c-0fe7-497e-b68b-ad1d079ccbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert report to HTML format\n",
    "rf_report_html = classification_report(y_true, y_pred, output_dict=False)\n",
    "\n",
    "# Log the report as HTML to W&B \n",
    "wandb.log({\"rf_classification_report_html\": wandb.Html(rf_report_html)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62188f-db1d-4b75-ab71-8b3e085ff8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de49a4f9-08b3-45b0-b763-c642c754d5c6",
   "metadata": {},
   "source": [
    "#### Get feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06174ff0-7e57-4238-a6c3-06a14019134a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "feature number: 38 | feature importance: 0.1388936524507944\n",
      "features: ['lag_1week_to_2weeks_Мои штрафы/Оплата/Завершили оплату', 'lag_1week_to_2weeks_Страхование/Главная/Лендинг/Ошибка загрузки лендинга', 'lag_1week_to_2weeks_Пуш Локальный/Скидочный штраф/Показан']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "feature number: 78 | feature importance: 0.10010848244047943\n",
      "features: ['lag_1week_to_2weeks_Мои штрафы/Оплата/Платёж принят', 'lag_1week_to_2weeks_Проверка/Список штрафов/У штрафа не все данные', 'lag_1week_to_2weeks_Поиск полиса по докам/Запрос/Ошибка']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "feature number: 178 | feature importance: 0.08749186396713882\n",
      "features: ['lag_2weeks_to_3weeks_Мои штрафы/Оплата/Платёж принят', 'lag_2weeks_to_3weeks_Проверка/Список штрафов/У штрафа не все данные']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "feature number: 138 | feature importance: 0.08180633640623222\n",
      "features: ['lag_2weeks_to_3weeks_Мои штрафы/Оплата/Завершили оплату', 'lag_2weeks_to_3weeks_Страхование/Главная/Лендинг/Ошибка загрузки лендинга']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "feature number: 238 | feature importance: 0.04468966061557644\n",
      "features: ['lag_3weeks_to_4weeks_Мои штрафы/Оплата/Завершили оплату', 'lag_3weeks_to_4weeks_Страхование/Главная/Лендинг/Ошибка загрузки лендинга']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "feature number: 278 | feature importance: 0.038330143150451856\n",
      "features: ['lag_3weeks_to_4weeks_Мои штрафы/Оплата/Платёж принят', 'lag_3weeks_to_4weeks_Проверка/Список штрафов/У штрафа не все данные']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "feature number: 545 | feature importance: 0.011513126199470912\n",
      "features: ['lag_6weeks_to_7weeks_Проверен документ', 'lag_6weeks_to_7weeks_Справочник/КоАП/Открыли детали']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "feature number: 169 | feature importance: 0.011259654394138568\n",
      "features: ['lag_2weeks_to_3weeks_Штраф']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "feature number: 1545 | feature importance: 0.010835117601737405\n",
      "features: ['lag_16weeks_to_17weeks_Проверен документ']\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "\n",
    "TH = .01 \n",
    "\n",
    "features_imps = {}\n",
    "for i, v in enumerate(model.featureImportances.toArray()):\n",
    "    if v >= TH: features_imps[i] = v\n",
    "features_imps = dict(sorted(features_imps.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "for k, v in features_imps.items():\n",
    "    print('-' * 100)\n",
    "    print('feature number:', k, '| feature importance:', v)\n",
    "    print('features:', features_dict[k])\n",
    "\n",
    "# Weights and biases logging here\n",
    "wandb.log({\"feature_importances\": wandb.Table(dataframe=pd.DataFrame.from_dict(features_imps, orient='index'))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e367d-cb81-4262-94e8-1716895390ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"def log_feature_importance(model, features_dict, model_name=\"model\", top_n=10):\n",
    "    #Extracts and logs feature importances from a model.\n",
    "    importances = model.featureImportances.toArray()\n",
    "    feature_importances = list(zip(features_dict.keys(), importances))\n",
    "    feature_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print top features\n",
    "    print(f\"Top Features for {model_name} Model:\")\n",
    "    for feat_num, importance in feature_importances[:top_n]:\n",
    "        print(f\"Feature {feat_num}: {features_dict[feat_num]} - Importance: {importance:.4f}\")\n",
    "\n",
    "    # Create DataFrame and log to W&B\n",
    "    importance_df = pd.DataFrame(feature_importances, columns=[\"Feature Number\", \"Importance\"])\n",
    "    wandb.log({f\"{model_name}_feature_importance\": wandb.Table(dataframe=importance_df)})\n",
    "\n",
    "# Example Usage (after training a Random Forest model):\n",
    "\n",
    "log_feature_importance(rf, features_dict, model_name=\"rf\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8db03d76-2a29-4589-9c40-69e5ab4c8506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features for rf Model:\n",
      "Feature 38: ['lag_1week_to_2weeks_Мои штрафы/Оплата/Завершили оплату', 'lag_1week_to_2weeks_Страхование/Главная/Лендинг/Ошибка загрузки лендинга', 'lag_1week_to_2weeks_Пуш Локальный/Скидочный штраф/Показан'] - Importance: 0.1389\n",
      "Feature 78: ['lag_1week_to_2weeks_Мои штрафы/Оплата/Платёж принят', 'lag_1week_to_2weeks_Проверка/Список штрафов/У штрафа не все данные', 'lag_1week_to_2weeks_Поиск полиса по докам/Запрос/Ошибка'] - Importance: 0.1001\n",
      "Feature 178: ['lag_2weeks_to_3weeks_Мои штрафы/Оплата/Платёж принят', 'lag_2weeks_to_3weeks_Проверка/Список штрафов/У штрафа не все данные'] - Importance: 0.0875\n",
      "Feature 138: ['lag_2weeks_to_3weeks_Мои штрафы/Оплата/Завершили оплату', 'lag_2weeks_to_3weeks_Страхование/Главная/Лендинг/Ошибка загрузки лендинга'] - Importance: 0.0818\n",
      "Feature 238: ['lag_3weeks_to_4weeks_Мои штрафы/Оплата/Завершили оплату', 'lag_3weeks_to_4weeks_Страхование/Главная/Лендинг/Ошибка загрузки лендинга'] - Importance: 0.0447\n",
      "Feature 278: ['lag_3weeks_to_4weeks_Мои штрафы/Оплата/Платёж принят', 'lag_3weeks_to_4weeks_Проверка/Список штрафов/У штрафа не все данные'] - Importance: 0.0383\n",
      "Feature 545: ['lag_6weeks_to_7weeks_Проверен документ', 'lag_6weeks_to_7weeks_Справочник/КоАП/Открыли детали'] - Importance: 0.0115\n",
      "Feature 169: ['lag_2weeks_to_3weeks_Штраф'] - Importance: 0.0113\n",
      "Feature 1545: ['lag_16weeks_to_17weeks_Проверен документ'] - Importance: 0.0108\n",
      "Feature 1845: ['lag_19weeks_to_20weeks_Проверен документ'] - Importance: 0.0089\n"
     ]
    }
   ],
   "source": [
    "def log_feature_importance(model, features_dict, model_name=\"rf\", top_n=10):\n",
    "    \"\"\"Extracts and logs feature importances from a Random Forest model.\"\"\"\n",
    "    importances = model.featureImportances.toArray()\n",
    "    feature_importances = list(zip(features_dict.keys(), importances))\n",
    "    feature_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print top features\n",
    "    print(f\"Top Features for {model_name} Model:\")\n",
    "    for feat_num, importance in feature_importances[:top_n]:\n",
    "        print(f\"Feature {feat_num}: {features_dict[feat_num]} - Importance: {importance:.4f}\")\n",
    "\n",
    "    # Create DataFrame and log to W&B\n",
    "    importance_df = pd.DataFrame(feature_importances, columns=[\"Feature Number\", \"Importance\"])\n",
    "    wandb.log({f\"{model_name}_feature_importance\": wandb.Table(dataframe=importance_df)})\n",
    "\n",
    "# Example Usage (after training a Random Forest model):\n",
    "\n",
    "log_feature_importance(model, features_dict)  # model_name defaults to \"rf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "00c5b62d-6dec-4a39-810b-2663567e04ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_feature_importance(model, features_dict, model_name=\"rf\", top_n=10):\n",
    "    \"\"\"Extracts and logs feature importances with lag details from a Random Forest model.\"\"\"\n",
    "    importances = model.featureImportances.toArray()\n",
    "    feature_importances = list(zip(features_dict.keys(), importances))\n",
    "    feature_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print top features with lag details\n",
    "    print(f\"Top Features for {model_name} Model:\")\n",
    "    for feat_num, importance in feature_importances[:top_n]:\n",
    "        lag_details = features_dict[feat_num]\n",
    "        for lag_feature in lag_details:\n",
    "            print(f\"  - Feature {feat_num}: {lag_feature} - Importance: {importance:.4f}\")\n",
    "            \n",
    "    # Create a list to store feature information\n",
    "    feature_info = []\n",
    "\n",
    "    # Iterate through features and extract information\n",
    "    for feat_num, importance in feature_importances:\n",
    "        lag_details = features_dict[feat_num]\n",
    "        for lag_feature in lag_details:\n",
    "            feature_info.append({\n",
    "                \"Feature Number\": feat_num,\n",
    "                \"Lag Feature\": lag_feature,\n",
    "                \"Importance\": importance\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    importance_df = pd.DataFrame(feature_info)\n",
    "\n",
    "    # Log the DataFrame as a W&B Table\n",
    "    wandb.log({f\"{model_name}_feature_importance\": wandb.Table(dataframe=importance_df)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e213fe40-578f-44cc-9437-d88a2faedabb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features for rf Model:\n",
      "  - Feature 38: lag_1week_to_2weeks_Мои штрафы/Оплата/Завершили оплату - Importance: 0.1389\n",
      "  - Feature 38: lag_1week_to_2weeks_Страхование/Главная/Лендинг/Ошибка загрузки лендинга - Importance: 0.1389\n",
      "  - Feature 38: lag_1week_to_2weeks_Пуш Локальный/Скидочный штраф/Показан - Importance: 0.1389\n",
      "  - Feature 78: lag_1week_to_2weeks_Мои штрафы/Оплата/Платёж принят - Importance: 0.1001\n",
      "  - Feature 78: lag_1week_to_2weeks_Проверка/Список штрафов/У штрафа не все данные - Importance: 0.1001\n",
      "  - Feature 78: lag_1week_to_2weeks_Поиск полиса по докам/Запрос/Ошибка - Importance: 0.1001\n",
      "  - Feature 178: lag_2weeks_to_3weeks_Мои штрафы/Оплата/Платёж принят - Importance: 0.0875\n",
      "  - Feature 178: lag_2weeks_to_3weeks_Проверка/Список штрафов/У штрафа не все данные - Importance: 0.0875\n",
      "  - Feature 138: lag_2weeks_to_3weeks_Мои штрафы/Оплата/Завершили оплату - Importance: 0.0818\n",
      "  - Feature 138: lag_2weeks_to_3weeks_Страхование/Главная/Лендинг/Ошибка загрузки лендинга - Importance: 0.0818\n",
      "  - Feature 238: lag_3weeks_to_4weeks_Мои штрафы/Оплата/Завершили оплату - Importance: 0.0447\n",
      "  - Feature 238: lag_3weeks_to_4weeks_Страхование/Главная/Лендинг/Ошибка загрузки лендинга - Importance: 0.0447\n",
      "  - Feature 278: lag_3weeks_to_4weeks_Мои штрафы/Оплата/Платёж принят - Importance: 0.0383\n",
      "  - Feature 278: lag_3weeks_to_4weeks_Проверка/Список штрафов/У штрафа не все данные - Importance: 0.0383\n",
      "  - Feature 545: lag_6weeks_to_7weeks_Проверен документ - Importance: 0.0115\n",
      "  - Feature 545: lag_6weeks_to_7weeks_Справочник/КоАП/Открыли детали - Importance: 0.0115\n",
      "  - Feature 169: lag_2weeks_to_3weeks_Штраф - Importance: 0.0113\n",
      "  - Feature 1545: lag_16weeks_to_17weeks_Проверен документ - Importance: 0.0108\n",
      "  - Feature 1845: lag_19weeks_to_20weeks_Проверен документ - Importance: 0.0089\n"
     ]
    }
   ],
   "source": [
    "log_feature_importance(model, features_dict)  # model_name defaults to \"rf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca086ee-e563-4410-97db-f0e6b71c9b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3b9bb-b7ff-47cf-9b59-1fccdf7fdc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "99841a14-cc34-46a1-9c23-dee8f0ab15ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_grouped_feature_importance(model, features_dict, model_name=\"rf\", top_n=10):\n",
    "    \"\"\"Extracts and logs feature importances with grouped lag details.\"\"\"\n",
    "    importances = model.featureImportances.toArray()\n",
    "    feature_importances = list(zip(features_dict.keys(), importances))\n",
    "    feature_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    grouped_feature_info = []\n",
    "\n",
    "    for feat_num, importance in feature_importances:\n",
    "        lag_details = features_dict[feat_num]\n",
    "        combined_lags = \", \".join(lag_details)  # Combine lag features into a single string\n",
    "        grouped_feature_info.append({\n",
    "            \"Feature Number\": feat_num,\n",
    "            \"Lag Features\": combined_lags,\n",
    "            \"Importance\": importance\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    grouped_importance_df = pd.DataFrame(grouped_feature_info)\n",
    "\n",
    "    # Log to W&B\n",
    "    wandb.log({f\"{model_name}_grouped_feature_importance\": wandb.Table(dataframe=grouped_importance_df)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22c48bd5-8faa-4cd5-92bf-644115e77b38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_grouped_feature_importance(model, features_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b92150c-533d-443b-a8b9-9d75bc3f397b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32711c8-1ffa-4229-9743-faf0a5adacb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89c2f7f1-a364-49d1-8a08-390bca1f0287",
   "metadata": {},
   "source": [
    "#### Run Hyperopt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ccdea5a4-a72e-4f8b-8ba3-3414360b2b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expanded search space for Random Forest\n",
    "rf_search_space = {\n",
    "    'numTrees': hp.quniform('numTrees', 10, 200, 10),\n",
    "    'maxDepth': hp.quniform('maxDepth', 5, 30, 1),\n",
    "    'minInstancesPerNode': hp.quniform('minInstancesPerNode', 1, 10, 1),  # Minimum instances per leaf\n",
    "    'maxBins': hp.quniform('maxBins', 10, 50, 5),  # Number of bins for discretizing continuous features\n",
    "    'subsamplingRate': hp.uniform('subsamplingRate', 0.5, 1.0),  # Subsampling rate for each tree\n",
    "    'featureSubsetStrategy': hp.choice('featureSubsetStrategy', ['auto', 'all', 'sqrt', 'log2', 'onethird']), \n",
    "    'impurity': hp.choice('impurity', ['gini', 'entropy'])  # Impurity measure (Gini or entropy)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "981ee52f-827f-4a98-bdee-a36472140ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the objective function for Hyperopt\n",
    "def rf_objective(params):\n",
    "    rf = RandomForestClassifier(labelCol=\"payment_event_flag\", featuresCol=\"features\", **params)\n",
    "    rf_model = rf.fit(features_train)\n",
    "    predictions = rf_model.transform(features_test)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"payment_event_flag\")\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    # Log metrics and params to W&B\n",
    "    wandb.log({\"rf_hyperopt_auc\": auc, \"rf_hyperopt_params\": params})\n",
    "    return {'loss': -auc, 'status': STATUS_OK}  # Hyperopt minimizes loss\n",
    "\n",
    "# Update W&B run config\n",
    "wandb.config.update({\n",
    "    \"model_type\": \"RandomForestHyperopt\",\n",
    "    \"hyperopt_search_space\": rf_search_space\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "73d2df9e-1c8d-4a1f-9af6-e67b504973db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [1:09:36<00:00, 139.21s/trial, best loss: -0.7258779220699368]\n"
     ]
    }
   ],
   "source": [
    "# Run Hyperopt optimization\n",
    "rf_trials = Trials()\n",
    "best_rf_params = fmin(\n",
    "    fn=rf_objective,\n",
    "    space=rf_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=30,\n",
    "    trials=rf_trials\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e00bfae7-96d9-43ae-b42d-c0c8dc21dd15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log best hyperparameters\n",
    "wandb.log({\"best_rf_hyperparameters\": best_rf_params}) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "74c1f09c-7d6b-41f5-a5fc-8d5f970ac30c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"def convert_hyperopt_params(params):\n",
    "    #Converts Hyperopt parameter types for better logging.\n",
    "    strategy_map = {0: \"auto\", 1: \"all\", 2: \"sqrt\", 3: \"log2\", 4: \"onethird\"}  # Mapping for featureSubsetStrategy\n",
    "    impurity_map = {0: \"gini\", 1: \"entropy\"}  # Mapping for impurity \n",
    "    converted_params = {}\n",
    "    for key, value in params.items():\n",
    "        if isinstance(value, dict):\n",
    "            converted_params[key] = value['quniform'] \n",
    "        elif key == 'featureSubsetStrategy':\n",
    "            strategy_index = int(value) \n",
    "            converted_params[key] = strategy_map.get(strategy_index, \"auto\")\n",
    "        elif key == 'impurity':\n",
    "            impurity_index = int(value) \n",
    "            converted_params[key] = impurity_map.get(impurity_index)\n",
    "        else:\n",
    "            converted_params[key] = value\n",
    "    return converted_params\"\"\"\n",
    "\n",
    "\n",
    "def convert_hyperopt_params(params):\n",
    "    #Converts Hyperopt parameter types for better logging.\n",
    "    strategy_map = {0: \"auto\", 1: \"all\", 2: \"sqrt\", 3: \"log2\", 4: \"onethird\"}  # Mapping for featureSubsetStrategy\n",
    "    impurity_map = {0: \"gini\", 1: \"entropy\"}  # Mapping for impurity\n",
    "    converted_params = {}\n",
    "    for key, value in params.items():\n",
    "        if isinstance(value, dict):\n",
    "            converted_params[key] = value['quniform']\n",
    "        elif key == 'featureSubsetStrategy':\n",
    "            if isinstance(value, str):\n",
    "                converted_params[key] = value  # Already a string, no conversion needed\n",
    "            else:\n",
    "                strategy_index = int(value)\n",
    "                converted_params[key] = strategy_map.get(strategy_index, \"auto\")\n",
    "        elif key == 'impurity':\n",
    "            impurity_index = int(value)\n",
    "            converted_params[key] = impurity_map.get(impurity_index)\n",
    "        else:\n",
    "            converted_params[key] = value\n",
    "    return converted_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2d13cb63-94a2-4bf7-a319-cd1adb5a8690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert Hyperopt parameter types\n",
    "best_rf_params = convert_hyperopt_params(best_rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f8427a6f-6eb5-4437-9392-2d30b75ce25f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the best RF model\n",
    "best_rf = RandomForestClassifier(labelCol=\"payment_event_flag\", featuresCol=\"features\", **best_rf_params)\n",
    "best_rf_model = best_rf.fit(features_train) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3aa7ece4-e345-4eae-b3a5-ecc067d53314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ... (After training the best RF model: best_rf_model = best_rf.fit(features_train)) ...\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "predictions = best_rf_model.transform(features_test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"payment_event_flag\")\n",
    "best_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "# Calculate PR AUC\n",
    "pr_evaluator = BinaryClassificationEvaluator(labelCol=\"payment_event_flag\", metricName=\"areaUnderPR\")\n",
    "best_pr_auc = pr_evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "419507ab-fee3-4fe7-8aba-28c2c172988d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log metrics and report to W&B\n",
    "wandb.log({\"best_rf_roc_auc\": best_auc})\n",
    "wandb.log({\"best_rf_pr_auc\": best_pr_auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eb420ca8-036f-486d-87c3-11f865dc5e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_rf_roc_auc:  0.725861507488782\n",
      "best_rf_pr_auc:  0.7674507664282019\n"
     ]
    }
   ],
   "source": [
    "# Log metrics and report to W&B\n",
    "print(\"best_rf_roc_auc: \", best_auc)\n",
    "print(\"best_rf_pr_auc: \", best_pr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc6a7d9c-248d-4b5b-8fdd-3a53b31a238a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "y_true = predictions.select('payment_event_flag').rdd.map(lambda x: x['payment_event_flag']).collect()\n",
    "y_pred = predictions.select('prediction').rdd.map(lambda x: x['prediction']).collect()\n",
    "report = classification_report(y_true, y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "52494d07-035f-480e-b26b-3f111cc79c62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.31      0.43      3919\n",
      "           1       0.60      0.88      0.72      4648\n",
      "\n",
      "    accuracy                           0.62      8567\n",
      "   macro avg       0.65      0.60      0.57      8567\n",
      "weighted avg       0.64      0.62      0.59      8567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "be3eec70-69d5-4292-996a-2fc954d8a24a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "wandb.log({\"best_rf_classification_report\": wandb.Html(report)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88461048-e0f3-48ef-aeab-c4b7db261931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd5a5d-0e9d-4009-849b-af9b1df75f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (Evaluate and log the best model's performance as before) ... \n",
    "\n",
    "# --- Functions for Logging Hyperopt Trials ---\n",
    "\n",
    "def convert_hyperopt_params(params):\n",
    "    \"\"\"Converts Hyperopt parameter types for better logging.\"\"\"\n",
    "    converted_params = {}\n",
    "    for key, value in params.items():\n",
    "        if isinstance(value, dict):\n",
    "            # Handle nested dictionaries (e.g., from hp.quniform)\n",
    "            converted_params[key] = value['quniform'] \n",
    "        else:\n",
    "            converted_params[key] = value\n",
    "    return converted_params\n",
    "\n",
    "def prepare_trials_df(trials):\n",
    "    \"\"\"Prepares a Pandas DataFrame from Hyperopt trials for logging to W&B.\"\"\"\n",
    "    trial_data = []\n",
    "    for i, trial in enumerate(trials.trials):\n",
    "        params = trial['misc']['vals']\n",
    "        loss = trial['result']['loss']\n",
    "        converted_params = convert_hyperopt_params(params)\n",
    "        trial_dict = {\"trial_id\": i, \"loss\": loss}\n",
    "        trial_dict.update(converted_params)  \n",
    "        trial_data.append(trial_dict)\n",
    "    trials_df = pd.DataFrame(trial_data)\n",
    "    return trials_df\n",
    "\n",
    "# Prepare and log the trials DataFrame\n",
    "trials_df = prepare_trials_df(rf_trials)\n",
    "wandb.log({\"hyperopt_trials\": wandb.Table(dataframe=trials_df)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6ac76-ea6b-4d05-a840-83c201ed31a0",
   "metadata": {},
   "source": [
    "#### Feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14438387-974d-422a-899c-2bae5ae9a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (After evaluating and logging the best RF model's performance) ... \n",
    "\n",
    "# Get feature importances from the best RF model\n",
    "importances = best_rf_model.featureImportances.toArray()\n",
    "\n",
    "# Zip importances with feature names (from features_dict)\n",
    "feature_importances = list(zip(features_dict.keys(), importances))\n",
    "\n",
    "# Sort features by importance (descending order)\n",
    "feature_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print top features and their importances\n",
    "print(\"Top Features for Best Random Forest Model:\")\n",
    "for feat_num, importance in feature_importances[:10]:\n",
    "    print(f\"Feature {feat_num}: {features_dict[feat_num]} - Importance: {importance:.4f}\")\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "feature_importance_df = pd.DataFrame(feature_importances, columns=[\"Feature Number\", \"Importance\"])\n",
    "\n",
    "# Log the DataFrame as a W&B Table\n",
    "wandb.log({\"best_rf_feature_importance\": wandb.Table(dataframe=feature_importance_df)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455584fe-016c-437e-936a-484dff2071b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d3bd1f2-9629-413d-bab3-be262b7506b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm = LinearSVC(labelCol=\"payment_event_flag\", featuresCol=\"features\", maxIter=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e98333a-db07-4abe-a4fc-6ddcac460171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log model type and parameters to W&B\n",
    "wandb.config.update({\n",
    "    \"model_type\": type(svm).__name__,\n",
    "    \"max_iter\": svm.getMaxIter(),\n",
    "    \"upsample\": UPSAMPLE,\n",
    "    \"lags\": lags,\n",
    "    \"num_features\": len(features_dict.items()),\n",
    "    \"output_file\": file_path_pred\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b08c574-27cf-4999-a215-e59b5e63a7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "svm_model = svm.fit(features_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a30df402-e822-40ae-a54c-7906ea45ed6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = svm_model.transform(features_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2584825e-dddf-41b3-8b5c-508c44b0ad0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"payment_event_flag\")\n",
    "svm_auc = evaluator.evaluate(predictions)\n",
    "\n",
    "# Calculate classification report\n",
    "y_true = predictions.select('payment_event_flag').rdd.map(lambda x: x['payment_event_flag']).collect()\n",
    "y_pred = predictions.select('prediction').rdd.map(lambda x: x['prediction']).collect()\n",
    "report = classification_report(y_true, y_pred)\n",
    "\n",
    "# Log metrics to W&B\n",
    "wandb.log({\"svm_roc_auc\": svm_auc})\n",
    "wandb.log({\"svm_classification_report\": wandb.Html(report)})\n",
    "\n",
    "print(f\"SVM ROC AUC: {svm_auc}\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8636095f-4954-4664-a322-872b2b98931e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM ROC AUC: 0.7163015593809696\n"
     ]
    }
   ],
   "source": [
    "print(f\"SVM ROC AUC: {svm_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b05a9-e8f6-4c2a-884a-6c05757891da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "267659a6-d606-4d08-8e63-dc4c96e5ff36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "y_true = predictions.select('payment_event_flag').rdd.map(lambda x: x['payment_event_flag']).collect()\n",
    "y_pred = predictions.select('prediction').rdd.map(lambda x: x['prediction']).collect()\n",
    "report = classification_report(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b471ac6-7de4-4f91-8313-a1c7cbc38fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log metrics to W&B\n",
    "wandb.log({\"svm_roc_auc\": svm_auc})\n",
    "wandb.log({\"svm_classification_report\": wandb.Html(report)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c807d79a-886a-41a5-bfc7-0051c2eb6fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM ROC AUC: 0.7163015593809696\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.81      0.68      3948\n",
      "           1       0.77      0.52      0.62      4669\n",
      "\n",
      "    accuracy                           0.65      8617\n",
      "   macro avg       0.68      0.67      0.65      8617\n",
      "weighted avg       0.69      0.65      0.65      8617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"SVM ROC AUC: {svm_auc}\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e455925a-073e-4787-b7b1-88eff3e4119e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svm_report = classification_report(y_true, y_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a0a3c5f3-231e-4c1d-b2e0-89407044bc31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svm_report_html = dict_to_html_table(svm_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e3d561c3-fbaa-410a-8fa2-04d6cb23c670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.log({\"svm_classification_report_html\": wandb.Html(svm_report_html)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c517dddf-b53d-4787-8e0a-d9c8110381ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM PR AUC: 0.7584486213386411\n"
     ]
    }
   ],
   "source": [
    "# Set the SVM model to output raw predictions (needed for probabilities)\n",
    "svm.setRawPredictionCol(\"rawPrediction\")\n",
    "\n",
    "# Train the model\n",
    "svm_model = svm.fit(features_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = svm_model.transform(features_test)\n",
    "\n",
    "# Get raw prediction and true labels\n",
    "prediction_raw = predictions.select(\"rawPrediction\", \"payment_event_flag\")\n",
    "\n",
    "# Convert to RDD and format for BinaryClassificationMetrics\n",
    "preds_rdd = prediction_raw.rdd.map(lambda lp: (float(lp[0][1]), float(lp[1])))\n",
    "\n",
    "# Create BinaryClassificationMetrics object\n",
    "metrics = BinaryClassificationMetrics(preds_rdd)\n",
    "\n",
    "# Calculate PR AUC\n",
    "pr_auc = metrics.areaUnderPR\n",
    "\n",
    "# Log PR AUC to W&B\n",
    "wandb.log({\"svm_pr_auc\": pr_auc})\n",
    "\n",
    "print(f\"SVM PR AUC: {pr_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "83154f25-0285-489e-babe-bcec7b604bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features for SVM Model:\n",
      "Feature 2015: ['lag_21weeks_to_22weeks_Проверка/История платежей/Детали'] - Weight: 3.4729\n",
      "Feature 773: ['lag_8weeks_to_9weeks_Мои штрафы/Проверка ВУ/Нажали на ВУ'] - Weight: 3.1714\n",
      "Feature 777: ['lag_8weeks_to_9weeks_Мои штрафы/Детали штрафа/Нажали кнопку \"Оплатить\"'] - Weight: 3.0667\n",
      "Feature 70: ['lag_1week_to_2weeks_Мои штрафы/Оплата/Оплата не прошла'] - Weight: -1.6171\n",
      "Feature 1483: ['lag_15weeks_to_16weeks_Мои штрафы/Пуш/Открыт'] - Weight: 1.1093\n",
      "Feature 538: ['lag_6weeks_to_7weeks_Мои штрафы/Оплата/Завершили оплату', 'lag_6weeks_to_7weeks_Пуш Локальный/Скидочный штраф/Показан', 'lag_6weeks_to_7weeks_Страхование/Главная/Лендинг/Ошибка загрузки лендинга'] - Weight: -1.0175\n",
      "Feature 2242: ['lag_23weeks_to_24weeks_Мои штрафы/Оплата/Начали оплату'] - Weight: 0.8045\n",
      "Feature 2246: ['lag_23weeks_to_24weeks_Мои штрафы/Оплата/Сберпей/Открыт'] - Weight: 0.7952\n",
      "Feature 1882: ['lag_19weeks_to_20weeks_Мои штрафы/Оплата/Ушел с ввода данных'] - Weight: 0.7904\n",
      "Feature 7: ['lag_1week_to_2weeks_Purchase', 'lag_1week_to_2weeks_Пуш Локальный/Скидочный штраф/Открыт', 'lag_1week_to_2weeks_Проверка/История платежей'] - Weight: 0.7634\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients (feature weights)\n",
    "svm_coef = svm_model.coefficients\n",
    "\n",
    "# Zip coefficients with feature names\n",
    "features_weights = list(zip(features_dict.keys(), svm_coef))\n",
    "\n",
    "# Sort features by absolute value of their weights (descending order)\n",
    "features_weights.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print top features and their weights\n",
    "print(\"Top Features for SVM Model:\")\n",
    "for feat_num, weight in features_weights[:10]:\n",
    "    print(f\"Feature {feat_num}: {features_dict[feat_num]} - Weight: {weight:.4f}\")\n",
    "\n",
    "# Create a table for logging to W&B\n",
    "\n",
    "features_importance_df = pd.DataFrame(features_weights, columns=[\"Feature Number\", \"Weight\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "72175fe3-de6d-47a9-9802-142056e261e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list to store feature information\n",
    "feature_info = []\n",
    "\n",
    "# Iterate through top features and extract information\n",
    "for feat_num, weight in features_weights[:30]:\n",
    "    event_names = features_dict[feat_num]\n",
    "    for event_name in event_names:\n",
    "        feature_info.append({\n",
    "            \"Feature Number\": feat_num,\n",
    "            \"Event Name\": event_name,\n",
    "            \"Weight\": weight\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "feature_importance_df = pd.DataFrame(feature_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1ed770fe-9108-4c5c-aec7-03e05408029b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log the DataFrame as a W&B Table\n",
    "wandb.log({\"svm_feature_importance\": wandb.Table(dataframe=feature_importance_df)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ceb26ab-e22d-464c-9d8b-0a8381c206a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Number</th>\n",
       "      <th>Event Name</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>lag_21weeks_to_22weeks_Проверка/История платеж...</td>\n",
       "      <td>3.472949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>773</td>\n",
       "      <td>lag_8weeks_to_9weeks_Мои штрафы/Проверка ВУ/На...</td>\n",
       "      <td>3.171449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>777</td>\n",
       "      <td>lag_8weeks_to_9weeks_Мои штрафы/Детали штрафа/...</td>\n",
       "      <td>3.066674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>lag_1week_to_2weeks_Мои штрафы/Оплата/Оплата н...</td>\n",
       "      <td>-1.617134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1483</td>\n",
       "      <td>lag_15weeks_to_16weeks_Мои штрафы/Пуш/Открыт</td>\n",
       "      <td>1.109276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>538</td>\n",
       "      <td>lag_6weeks_to_7weeks_Мои штрафы/Оплата/Заверши...</td>\n",
       "      <td>-1.017492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>538</td>\n",
       "      <td>lag_6weeks_to_7weeks_Пуш Локальный/Скидочный ш...</td>\n",
       "      <td>-1.017492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>538</td>\n",
       "      <td>lag_6weeks_to_7weeks_Страхование/Главная/Ленди...</td>\n",
       "      <td>-1.017492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2242</td>\n",
       "      <td>lag_23weeks_to_24weeks_Мои штрафы/Оплата/Начал...</td>\n",
       "      <td>0.804529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2246</td>\n",
       "      <td>lag_23weeks_to_24weeks_Мои штрафы/Оплата/Сберп...</td>\n",
       "      <td>0.795154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1882</td>\n",
       "      <td>lag_19weeks_to_20weeks_Мои штрафы/Оплата/Ушел ...</td>\n",
       "      <td>0.790397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>lag_1week_to_2weeks_Purchase</td>\n",
       "      <td>0.763351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>lag_1week_to_2weeks_Пуш Локальный/Скидочный шт...</td>\n",
       "      <td>0.763351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>lag_1week_to_2weeks_Проверка/История платежей</td>\n",
       "      <td>0.763351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>205</td>\n",
       "      <td>lag_3weeks_to_4weeks_Мои штрафы/Оплата/Завешил...</td>\n",
       "      <td>0.753006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>205</td>\n",
       "      <td>lag_3weeks_to_4weeks_Мои штрафы/Оплата/Завешил...</td>\n",
       "      <td>0.753006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1682</td>\n",
       "      <td>lag_17weeks_to_18weeks_Мои штрафы/Оплата/Ушел ...</td>\n",
       "      <td>0.725274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1096</td>\n",
       "      <td>lag_11weeks_to_12weeks_ОСАГО/Дата покупки/Форм...</td>\n",
       "      <td>-0.713892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>982</td>\n",
       "      <td>lag_10weeks_to_11weeks_Мои штрафы/Оплата/Ушел ...</td>\n",
       "      <td>0.691629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1842</td>\n",
       "      <td>lag_19weeks_to_20weeks_Мои штрафы/Оплата/Начал...</td>\n",
       "      <td>0.684535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1842</td>\n",
       "      <td>lag_19weeks_to_20weeks_Карта с местом нарушения</td>\n",
       "      <td>0.684535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>281</td>\n",
       "      <td>lag_3weeks_to_4weeks_Пуш о проверке доков чере...</td>\n",
       "      <td>0.632726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>281</td>\n",
       "      <td>lag_3weeks_to_4weeks_Список оплаченых</td>\n",
       "      <td>0.632726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1819</td>\n",
       "      <td>lag_19weeks_to_20weeks_Детали штрафа/Увидел ош...</td>\n",
       "      <td>-0.617216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2115</td>\n",
       "      <td>lag_22weeks_to_23weeks_Проверка/История платеж...</td>\n",
       "      <td>0.596502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2115</td>\n",
       "      <td>lag_22weeks_to_23weeks_Иконка авто/</td>\n",
       "      <td>0.596502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1007</td>\n",
       "      <td>lag_11weeks_to_12weeks_Purchase</td>\n",
       "      <td>0.566617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1184</td>\n",
       "      <td>lag_12weeks_to_13weeks_Мои штрафы/Задолженност...</td>\n",
       "      <td>0.546406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1184</td>\n",
       "      <td>lag_12weeks_to_13weeks_Справочник/ПДД/Открыли ...</td>\n",
       "      <td>0.546406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1242</td>\n",
       "      <td>lag_13weeks_to_14weeks_Мои штрафы/Оплата/Начал...</td>\n",
       "      <td>0.523158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1242</td>\n",
       "      <td>lag_13weeks_to_14weeks_Мои штрафы/Оплата по по...</td>\n",
       "      <td>0.523158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2211</td>\n",
       "      <td>lag_23weeks_to_24weeks_Мои штрафы/Оплата/Начал...</td>\n",
       "      <td>0.488098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2211</td>\n",
       "      <td>lag_23weeks_to_24weeks_Push/Получен пуш от api</td>\n",
       "      <td>0.488098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2211</td>\n",
       "      <td>lag_23weeks_to_24weeks_Мои штрафы/Редактирован...</td>\n",
       "      <td>0.488098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>418</td>\n",
       "      <td>lag_5weeks_to_6weeks_TouchId/Auth/Ошибка</td>\n",
       "      <td>0.465414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>418</td>\n",
       "      <td>lag_5weeks_to_6weeks_Страхование/Главная</td>\n",
       "      <td>0.465414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>988</td>\n",
       "      <td>lag_10weeks_to_11weeks_Мои штрафы/Оплата/Втора...</td>\n",
       "      <td>0.461156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>lag_1week_to_2weeks_Мои штрафы/Оплата/Завершил...</td>\n",
       "      <td>0.455985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>lag_1week_to_2weeks_Пуш Локальный/Скидочный шт...</td>\n",
       "      <td>0.455985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>78</td>\n",
       "      <td>lag_1week_to_2weeks_Мои штрафы/Оплата/Платёж п...</td>\n",
       "      <td>0.444351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>78</td>\n",
       "      <td>lag_1week_to_2weeks_Проверка/Список штрафов/У ...</td>\n",
       "      <td>0.444351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5</td>\n",
       "      <td>lag_1week_to_2weeks_Мои штрафы/Оплата/Завешили...</td>\n",
       "      <td>0.425247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5</td>\n",
       "      <td>lag_1week_to_2weeks_Справочник/Поиск/Открыл Ко...</td>\n",
       "      <td>0.425247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>651</td>\n",
       "      <td>lag_7weeks_to_8weeks_Добавлен документ</td>\n",
       "      <td>-0.424243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>651</td>\n",
       "      <td>lag_7weeks_to_8weeks_Сервисы/Показ</td>\n",
       "      <td>-0.424243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1611</td>\n",
       "      <td>lag_17weeks_to_18weeks_Мои штрафы/Редактирован...</td>\n",
       "      <td>0.408814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1611</td>\n",
       "      <td>lag_17weeks_to_18weeks_Мои штрафы/Оплата/Начал...</td>\n",
       "      <td>0.408814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1611</td>\n",
       "      <td>lag_17weeks_to_18weeks_Push/Получен пуш от api</td>\n",
       "      <td>0.408814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1791</td>\n",
       "      <td>lag_18weeks_to_19weeks_Мои штрафы/Документы/От...</td>\n",
       "      <td>-0.405247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature Number                                         Event Name  \\\n",
       "0             2015  lag_21weeks_to_22weeks_Проверка/История платеж...   \n",
       "1              773  lag_8weeks_to_9weeks_Мои штрафы/Проверка ВУ/На...   \n",
       "2              777  lag_8weeks_to_9weeks_Мои штрафы/Детали штрафа/...   \n",
       "3               70  lag_1week_to_2weeks_Мои штрафы/Оплата/Оплата н...   \n",
       "4             1483       lag_15weeks_to_16weeks_Мои штрафы/Пуш/Открыт   \n",
       "5              538  lag_6weeks_to_7weeks_Мои штрафы/Оплата/Заверши...   \n",
       "6              538  lag_6weeks_to_7weeks_Пуш Локальный/Скидочный ш...   \n",
       "7              538  lag_6weeks_to_7weeks_Страхование/Главная/Ленди...   \n",
       "8             2242  lag_23weeks_to_24weeks_Мои штрафы/Оплата/Начал...   \n",
       "9             2246  lag_23weeks_to_24weeks_Мои штрафы/Оплата/Сберп...   \n",
       "10            1882  lag_19weeks_to_20weeks_Мои штрафы/Оплата/Ушел ...   \n",
       "11               7                       lag_1week_to_2weeks_Purchase   \n",
       "12               7  lag_1week_to_2weeks_Пуш Локальный/Скидочный шт...   \n",
       "13               7      lag_1week_to_2weeks_Проверка/История платежей   \n",
       "14             205  lag_3weeks_to_4weeks_Мои штрафы/Оплата/Завешил...   \n",
       "15             205  lag_3weeks_to_4weeks_Мои штрафы/Оплата/Завешил...   \n",
       "16            1682  lag_17weeks_to_18weeks_Мои штрафы/Оплата/Ушел ...   \n",
       "17            1096  lag_11weeks_to_12weeks_ОСАГО/Дата покупки/Форм...   \n",
       "18             982  lag_10weeks_to_11weeks_Мои штрафы/Оплата/Ушел ...   \n",
       "19            1842  lag_19weeks_to_20weeks_Мои штрафы/Оплата/Начал...   \n",
       "20            1842    lag_19weeks_to_20weeks_Карта с местом нарушения   \n",
       "21             281  lag_3weeks_to_4weeks_Пуш о проверке доков чере...   \n",
       "22             281              lag_3weeks_to_4weeks_Список оплаченых   \n",
       "23            1819  lag_19weeks_to_20weeks_Детали штрафа/Увидел ош...   \n",
       "24            2115  lag_22weeks_to_23weeks_Проверка/История платеж...   \n",
       "25            2115                lag_22weeks_to_23weeks_Иконка авто/   \n",
       "26            1007                    lag_11weeks_to_12weeks_Purchase   \n",
       "27            1184  lag_12weeks_to_13weeks_Мои штрафы/Задолженност...   \n",
       "28            1184  lag_12weeks_to_13weeks_Справочник/ПДД/Открыли ...   \n",
       "29            1242  lag_13weeks_to_14weeks_Мои штрафы/Оплата/Начал...   \n",
       "30            1242  lag_13weeks_to_14weeks_Мои штрафы/Оплата по по...   \n",
       "31            2211  lag_23weeks_to_24weeks_Мои штрафы/Оплата/Начал...   \n",
       "32            2211     lag_23weeks_to_24weeks_Push/Получен пуш от api   \n",
       "33            2211  lag_23weeks_to_24weeks_Мои штрафы/Редактирован...   \n",
       "34             418           lag_5weeks_to_6weeks_TouchId/Auth/Ошибка   \n",
       "35             418           lag_5weeks_to_6weeks_Страхование/Главная   \n",
       "36             988  lag_10weeks_to_11weeks_Мои штрафы/Оплата/Втора...   \n",
       "37              38  lag_1week_to_2weeks_Мои штрафы/Оплата/Завершил...   \n",
       "38              38  lag_1week_to_2weeks_Пуш Локальный/Скидочный шт...   \n",
       "39              78  lag_1week_to_2weeks_Мои штрафы/Оплата/Платёж п...   \n",
       "40              78  lag_1week_to_2weeks_Проверка/Список штрафов/У ...   \n",
       "41               5  lag_1week_to_2weeks_Мои штрафы/Оплата/Завешили...   \n",
       "42               5  lag_1week_to_2weeks_Справочник/Поиск/Открыл Ко...   \n",
       "43             651             lag_7weeks_to_8weeks_Добавлен документ   \n",
       "44             651                 lag_7weeks_to_8weeks_Сервисы/Показ   \n",
       "45            1611  lag_17weeks_to_18weeks_Мои штрафы/Редактирован...   \n",
       "46            1611  lag_17weeks_to_18weeks_Мои штрафы/Оплата/Начал...   \n",
       "47            1611     lag_17weeks_to_18weeks_Push/Получен пуш от api   \n",
       "48            1791  lag_18weeks_to_19weeks_Мои штрафы/Документы/От...   \n",
       "\n",
       "      Weight  \n",
       "0   3.472949  \n",
       "1   3.171449  \n",
       "2   3.066674  \n",
       "3  -1.617134  \n",
       "4   1.109276  \n",
       "5  -1.017492  \n",
       "6  -1.017492  \n",
       "7  -1.017492  \n",
       "8   0.804529  \n",
       "9   0.795154  \n",
       "10  0.790397  \n",
       "11  0.763351  \n",
       "12  0.763351  \n",
       "13  0.763351  \n",
       "14  0.753006  \n",
       "15  0.753006  \n",
       "16  0.725274  \n",
       "17 -0.713892  \n",
       "18  0.691629  \n",
       "19  0.684535  \n",
       "20  0.684535  \n",
       "21  0.632726  \n",
       "22  0.632726  \n",
       "23 -0.617216  \n",
       "24  0.596502  \n",
       "25  0.596502  \n",
       "26  0.566617  \n",
       "27  0.546406  \n",
       "28  0.546406  \n",
       "29  0.523158  \n",
       "30  0.523158  \n",
       "31  0.488098  \n",
       "32  0.488098  \n",
       "33  0.488098  \n",
       "34  0.465414  \n",
       "35  0.465414  \n",
       "36  0.461156  \n",
       "37  0.455985  \n",
       "38  0.455985  \n",
       "39  0.444351  \n",
       "40  0.444351  \n",
       "41  0.425247  \n",
       "42  0.425247  \n",
       "43 -0.424243  \n",
       "44 -0.424243  \n",
       "45  0.408814  \n",
       "46  0.408814  \n",
       "47  0.408814  \n",
       "48 -0.405247  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2db7c8eb-75ab-43ba-a866-314926cac60d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate total absolute weight\n",
    "total_weight = sum(abs(weight) for _, weight in features_weights)\n",
    "\n",
    "# Create a list to store feature information\n",
    "feature_info = []\n",
    "\n",
    "# Extract information for top 20 features\n",
    "for feat_num, weight in features_weights[:20]:\n",
    "    event_names = features_dict[feat_num]\n",
    "    for event_name in event_names:\n",
    "        percentage_contribution = (abs(weight) / total_weight) * 100\n",
    "        feature_info.append({\n",
    "            \"Feature Number\": feat_num,\n",
    "            \"Event Name\": event_name,\n",
    "            \"Weight\": weight,\n",
    "            \"Percentage Contribution\": f\"{percentage_contribution:.2f}%\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "feature_weight_df = pd.DataFrame(feature_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0241530a-f7e6-47ef-a329-a4b565a701db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log the DataFrame as a W&B Table\n",
    "wandb.log({\"svm_feature_weight\": wandb.Table(dataframe=feature_weight_df)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9c171f56-53e9-49be-8b57-c055f8502bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Number</th>\n",
       "      <th>Event Name</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Percentage Contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>lag_21weeks_to_22weeks_Проверка/История платеж...</td>\n",
       "      <td>3.472949</td>\n",
       "      <td>2.04%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>773</td>\n",
       "      <td>lag_8weeks_to_9weeks_Мои штрафы/Проверка ВУ/На...</td>\n",
       "      <td>3.171449</td>\n",
       "      <td>1.86%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>777</td>\n",
       "      <td>lag_8weeks_to_9weeks_Мои штрафы/Детали штрафа/...</td>\n",
       "      <td>3.066674</td>\n",
       "      <td>1.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>lag_1week_to_2weeks_Мои штрафы/Оплата/Оплата н...</td>\n",
       "      <td>-1.617134</td>\n",
       "      <td>0.95%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1483</td>\n",
       "      <td>lag_15weeks_to_16weeks_Мои штрафы/Пуш/Открыт</td>\n",
       "      <td>1.109276</td>\n",
       "      <td>0.65%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>538</td>\n",
       "      <td>lag_6weeks_to_7weeks_Мои штрафы/Оплата/Заверши...</td>\n",
       "      <td>-1.017492</td>\n",
       "      <td>0.60%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>538</td>\n",
       "      <td>lag_6weeks_to_7weeks_Пуш Локальный/Скидочный ш...</td>\n",
       "      <td>-1.017492</td>\n",
       "      <td>0.60%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>538</td>\n",
       "      <td>lag_6weeks_to_7weeks_Страхование/Главная/Ленди...</td>\n",
       "      <td>-1.017492</td>\n",
       "      <td>0.60%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2242</td>\n",
       "      <td>lag_23weeks_to_24weeks_Мои штрафы/Оплата/Начал...</td>\n",
       "      <td>0.804529</td>\n",
       "      <td>0.47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2246</td>\n",
       "      <td>lag_23weeks_to_24weeks_Мои штрафы/Оплата/Сберп...</td>\n",
       "      <td>0.795154</td>\n",
       "      <td>0.47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1882</td>\n",
       "      <td>lag_19weeks_to_20weeks_Мои штрафы/Оплата/Ушел ...</td>\n",
       "      <td>0.790397</td>\n",
       "      <td>0.46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>lag_1week_to_2weeks_Purchase</td>\n",
       "      <td>0.763351</td>\n",
       "      <td>0.45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>lag_1week_to_2weeks_Пуш Локальный/Скидочный шт...</td>\n",
       "      <td>0.763351</td>\n",
       "      <td>0.45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>lag_1week_to_2weeks_Проверка/История платежей</td>\n",
       "      <td>0.763351</td>\n",
       "      <td>0.45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>205</td>\n",
       "      <td>lag_3weeks_to_4weeks_Мои штрафы/Оплата/Завешил...</td>\n",
       "      <td>0.753006</td>\n",
       "      <td>0.44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>205</td>\n",
       "      <td>lag_3weeks_to_4weeks_Мои штрафы/Оплата/Завешил...</td>\n",
       "      <td>0.753006</td>\n",
       "      <td>0.44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1682</td>\n",
       "      <td>lag_17weeks_to_18weeks_Мои штрафы/Оплата/Ушел ...</td>\n",
       "      <td>0.725274</td>\n",
       "      <td>0.43%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1096</td>\n",
       "      <td>lag_11weeks_to_12weeks_ОСАГО/Дата покупки/Форм...</td>\n",
       "      <td>-0.713892</td>\n",
       "      <td>0.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>982</td>\n",
       "      <td>lag_10weeks_to_11weeks_Мои штрафы/Оплата/Ушел ...</td>\n",
       "      <td>0.691629</td>\n",
       "      <td>0.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1842</td>\n",
       "      <td>lag_19weeks_to_20weeks_Мои штрафы/Оплата/Начал...</td>\n",
       "      <td>0.684535</td>\n",
       "      <td>0.40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1842</td>\n",
       "      <td>lag_19weeks_to_20weeks_Карта с местом нарушения</td>\n",
       "      <td>0.684535</td>\n",
       "      <td>0.40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>281</td>\n",
       "      <td>lag_3weeks_to_4weeks_Пуш о проверке доков чере...</td>\n",
       "      <td>0.632726</td>\n",
       "      <td>0.37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>281</td>\n",
       "      <td>lag_3weeks_to_4weeks_Список оплаченых</td>\n",
       "      <td>0.632726</td>\n",
       "      <td>0.37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1819</td>\n",
       "      <td>lag_19weeks_to_20weeks_Детали штрафа/Увидел ош...</td>\n",
       "      <td>-0.617216</td>\n",
       "      <td>0.36%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2115</td>\n",
       "      <td>lag_22weeks_to_23weeks_Проверка/История платеж...</td>\n",
       "      <td>0.596502</td>\n",
       "      <td>0.35%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2115</td>\n",
       "      <td>lag_22weeks_to_23weeks_Иконка авто/</td>\n",
       "      <td>0.596502</td>\n",
       "      <td>0.35%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1007</td>\n",
       "      <td>lag_11weeks_to_12weeks_Purchase</td>\n",
       "      <td>0.566617</td>\n",
       "      <td>0.33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1184</td>\n",
       "      <td>lag_12weeks_to_13weeks_Мои штрафы/Задолженност...</td>\n",
       "      <td>0.546406</td>\n",
       "      <td>0.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1184</td>\n",
       "      <td>lag_12weeks_to_13weeks_Справочник/ПДД/Открыли ...</td>\n",
       "      <td>0.546406</td>\n",
       "      <td>0.32%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature Number                                         Event Name  \\\n",
       "0             2015  lag_21weeks_to_22weeks_Проверка/История платеж...   \n",
       "1              773  lag_8weeks_to_9weeks_Мои штрафы/Проверка ВУ/На...   \n",
       "2              777  lag_8weeks_to_9weeks_Мои штрафы/Детали штрафа/...   \n",
       "3               70  lag_1week_to_2weeks_Мои штрафы/Оплата/Оплата н...   \n",
       "4             1483       lag_15weeks_to_16weeks_Мои штрафы/Пуш/Открыт   \n",
       "5              538  lag_6weeks_to_7weeks_Мои штрафы/Оплата/Заверши...   \n",
       "6              538  lag_6weeks_to_7weeks_Пуш Локальный/Скидочный ш...   \n",
       "7              538  lag_6weeks_to_7weeks_Страхование/Главная/Ленди...   \n",
       "8             2242  lag_23weeks_to_24weeks_Мои штрафы/Оплата/Начал...   \n",
       "9             2246  lag_23weeks_to_24weeks_Мои штрафы/Оплата/Сберп...   \n",
       "10            1882  lag_19weeks_to_20weeks_Мои штрафы/Оплата/Ушел ...   \n",
       "11               7                       lag_1week_to_2weeks_Purchase   \n",
       "12               7  lag_1week_to_2weeks_Пуш Локальный/Скидочный шт...   \n",
       "13               7      lag_1week_to_2weeks_Проверка/История платежей   \n",
       "14             205  lag_3weeks_to_4weeks_Мои штрафы/Оплата/Завешил...   \n",
       "15             205  lag_3weeks_to_4weeks_Мои штрафы/Оплата/Завешил...   \n",
       "16            1682  lag_17weeks_to_18weeks_Мои штрафы/Оплата/Ушел ...   \n",
       "17            1096  lag_11weeks_to_12weeks_ОСАГО/Дата покупки/Форм...   \n",
       "18             982  lag_10weeks_to_11weeks_Мои штрафы/Оплата/Ушел ...   \n",
       "19            1842  lag_19weeks_to_20weeks_Мои штрафы/Оплата/Начал...   \n",
       "20            1842    lag_19weeks_to_20weeks_Карта с местом нарушения   \n",
       "21             281  lag_3weeks_to_4weeks_Пуш о проверке доков чере...   \n",
       "22             281              lag_3weeks_to_4weeks_Список оплаченых   \n",
       "23            1819  lag_19weeks_to_20weeks_Детали штрафа/Увидел ош...   \n",
       "24            2115  lag_22weeks_to_23weeks_Проверка/История платеж...   \n",
       "25            2115                lag_22weeks_to_23weeks_Иконка авто/   \n",
       "26            1007                    lag_11weeks_to_12weeks_Purchase   \n",
       "27            1184  lag_12weeks_to_13weeks_Мои штрафы/Задолженност...   \n",
       "28            1184  lag_12weeks_to_13weeks_Справочник/ПДД/Открыли ...   \n",
       "\n",
       "      Weight Percentage Contribution  \n",
       "0   3.472949                   2.04%  \n",
       "1   3.171449                   1.86%  \n",
       "2   3.066674                   1.80%  \n",
       "3  -1.617134                   0.95%  \n",
       "4   1.109276                   0.65%  \n",
       "5  -1.017492                   0.60%  \n",
       "6  -1.017492                   0.60%  \n",
       "7  -1.017492                   0.60%  \n",
       "8   0.804529                   0.47%  \n",
       "9   0.795154                   0.47%  \n",
       "10  0.790397                   0.46%  \n",
       "11  0.763351                   0.45%  \n",
       "12  0.763351                   0.45%  \n",
       "13  0.763351                   0.45%  \n",
       "14  0.753006                   0.44%  \n",
       "15  0.753006                   0.44%  \n",
       "16  0.725274                   0.43%  \n",
       "17 -0.713892                   0.42%  \n",
       "18  0.691629                   0.41%  \n",
       "19  0.684535                   0.40%  \n",
       "20  0.684535                   0.40%  \n",
       "21  0.632726                   0.37%  \n",
       "22  0.632726                   0.37%  \n",
       "23 -0.617216                   0.36%  \n",
       "24  0.596502                   0.35%  \n",
       "25  0.596502                   0.35%  \n",
       "26  0.566617                   0.33%  \n",
       "27  0.546406                   0.32%  \n",
       "28  0.546406                   0.32%  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_weight_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3208ddc-9874-48d6-b750-6f1b7bba4a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de25bdbc-d333-4e0f-a490-92939bb6a34e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Logging notebook characteristics\n",
    "\n",
    "The below code snippet prints a formatted summary with clear headings and labels for each variable or parameter.\n",
    "\n",
    "It includes information about data processing flags, lag feature details, date range used for filtering payment events, upsampling strategy, model type, and the output file path.\n",
    "\n",
    "You can further customize this code by adding more relevant variables or model-specific parameters based on your pipeline configuration.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "* Reproducibility: Having a summary of the pipeline parameters improves the reproducibility of your results and makes it easier to track the specific settings used for a particular experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "30657a17-5ff9-4c28-a6f8-c237c07cae50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Pipeline Summary ##\n",
      "- PROC_DS: False\n",
      "- PROC_LAGS: False\n",
      "- PROC_VECS: True\n",
      "- Lag Features Used: ['lag_1week_to_2weeks', 'lag_2weeks_to_3weeks', 'lag_3weeks_to_4weeks', 'lag_4weeks_to_5weeks', 'lag_5weeks_to_6weeks', 'lag_6weeks_to_7weeks', 'lag_7weeks_to_8weeks', 'lag_8weeks_to_9weeks', 'lag_9weeks_to_10weeks', 'lag_10weeks_to_11weeks', 'lag_11weeks_to_12weeks', 'lag_12weeks_to_13weeks', 'lag_13weeks_to_14weeks', 'lag_14weeks_to_15weeks', 'lag_15weeks_to_16weeks', 'lag_16weeks_to_17weeks', 'lag_17weeks_to_18weeks', 'lag_18weeks_to_19weeks', 'lag_19weeks_to_20weeks', 'lag_20weeks_to_21weeks', 'lag_21weeks_to_22weeks', 'lag_22weeks_to_23weeks', 'lag_23weeks_to_24weeks', 'lag_24weeks_to_25weeks', 'lag_25weeks_to_26weeks', 'lag_26weeks_to_27weeks']\n",
      "- Number of Features after TF-IDF: 2600\n",
      "- flag_min_datetime: 2024-03-21 00:00:00\n",
      "- flag_max_datetime: 2024-04-18 23:59:59\n",
      "- UPSAMPLE: max\n",
      "- Model Type: LinearSVCModel\n",
      "- Predictions saved to:  s3a://pvc-84ea79a0-dc20-4a2d-86ab-f83c1f8d4a7b/work/vJ1/preds_LinearSVCModel_20240422_145726_st083972_21a474e5.csv\n",
      "## End of Summary ##\n"
     ]
    }
   ],
   "source": [
    "print(\"## Pipeline Summary ##\")\n",
    "\n",
    "# Data Processing Flags:\n",
    "print(f\"- PROC_DS: {PROC_DS}\")\n",
    "print(f\"- PROC_LAGS: {PROC_LAGS}\")\n",
    "print(f\"- PROC_VECS: {PROC_VECS}\")\n",
    "\n",
    "# Lag Feature Information:\n",
    "print(f\"- Lag Features Used: {lags}\")\n",
    "print(f\"- Number of Features after TF-IDF: {len(features_dict.items())}\")\n",
    "\n",
    "# Date Range:\n",
    "print(f\"- flag_min_datetime: {flag_min_datetime}\")\n",
    "print(f\"- flag_max_datetime: {flag_max_datetime}\")\n",
    "\n",
    "# Upsampling:\n",
    "print(f\"- UPSAMPLE: {UPSAMPLE}\")\n",
    "\n",
    "# Model:\n",
    "print(f\"- Model Type: {type(svm_model).__name__}\")\n",
    "# Add more model-specific parameters if needed \n",
    "\n",
    "# Output:\n",
    "print(f\"- Predictions saved to: \", file_path_pred)\n",
    "\n",
    "print(\"## End of Summary ##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "962283f8-4d5e-4a72-bc98-d3308afa3f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_dict = {\n",
    "    \"PROC_DS\": PROC_DS,\n",
    "    \"PROC_LAGS\": PROC_LAGS,\n",
    "    \"PROC_VECS\": PROC_VECS,\n",
    "    \"flag_min_datetime\": flag_min_datetime,\n",
    "    \"flag_max_datetime\": flag_max_datetime,\n",
    "    \"UPSAMPLE\": UPSAMPLE,\n",
    "    \"Number of Features after TF-IDF\": len(features_dict.items()),\n",
    "    \"Model Type\": type(svm_model).__name__,\n",
    "    \"maxIter\": svm_model.getMaxIter(),\n",
    "    \"regParam\": svm_model.getRegParam(),\n",
    "    \"tol\": svm_model.getTol(),\n",
    "    \"Predictions saved to\": file_path_pred, \n",
    "    \"user_id\": current_user, \n",
    "    \"unique_identifier\": unique_identifier\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "81ca2ad7-5d4f-44ee-adc5-f4aa35beb5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_dict[\"Number of Features after TF-IDF\"] = str(len(features_dict.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "02c7baee-9224-4fe1-877b-6b36dedead77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dict_to_html_table(data):\n",
    "    html = \"<table>\"\n",
    "    for key, value in data.items():\n",
    "        html += f\"<tr><th>{key}</th><td>{value}</td></tr>\"\n",
    "    html += \"</table>\"\n",
    "    return html\n",
    "\n",
    "summary_html = dict_to_html_table(summary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "14e1f4cf-030e-4621-8be4-25a7801a5467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.log({\"pipeline_summary_html\": wandb.Html(summary_html)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce348f-3116-4cdb-a9a1-53bdcb25c6e9",
   "metadata": {},
   "source": [
    "# <span style=\"color: red;\">!!!!! BELOW WILL RUN HYPEROPT AGAIN TF-IDF !!!!!\n",
    "\n",
    "It also does not include the date code before running the model\n",
    "\n",
    "# *** Add filtering here ***\n",
    "    if PROC_LAGS:\n",
    "        dates  = (flag_min_datetime, flag_max_datetime)\n",
    "        features_train_filtered = features_train.filter(features_train.event_datetime.between(*dates))\n",
    "    else:\n",
    "        features_train_filtered = features_train \n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f33944c-9fe4-48d0-88c6-47966858660c",
   "metadata": {},
   "source": [
    "## Random forest WITHOUT tf-idf elements\n",
    "\n",
    "#### Analysis of Hyperparameter Choices for SVM with Hyperopt\n",
    "\n",
    "`maxIter`: This parameter controls the maximum number of iterations the optimization algorithm will run before stopping.\n",
    "\n",
    "`Choice`: scope.int(hp.quniform('maxIter', 10, 200, 10)) suggests exploring values between 10 and 200 with a step size of 10.\n",
    "\n",
    "`Rationale`: A higher maxIter allows the algorithm to search more extensively for the optimal solution but increases training time. The chosen range provides a reasonable starting point to balance exploration and efficiency.\n",
    "\n",
    "**Alternative Choices:**\n",
    "\n",
    "`Smaller Range/Step Size`: If training time is a major concern, you might reduce the range or step size to explore fewer values.\n",
    "\n",
    "`Larger Range/Step Size`: If you suspect the optimal value might be outside the current range, you could expand it.\n",
    "\n",
    "`regParam`: This parameter controls the regularization strength. Higher values lead to stronger regularization, which can help prevent overfitting but might also underfit if too high.\n",
    "\n",
    "`Choice`: hp.loguniform('regParam', -6, 0) samples values from a log-uniform distribution between 1e-6 and 1.\n",
    "\n",
    "`Rationale`: The log-uniform distribution allows exploring a wide range of regularization strengths, from very small to relatively large values, which is often suitable for regularization parameters.\n",
    "\n",
    "**Alternative Choices:**\n",
    "\n",
    "`Uniform Distribution`: If you have prior knowledge about the reasonable range for regParam, you could use a uniform distribution within that range.\n",
    "\n",
    "`Specific Values`: In some cases, you might want to try specific values based on experience or domain knowledge.\n",
    "\n",
    "`tol`: This parameter sets the tolerance for the stopping criterion. The optimization algorithm stops when the improvement in the objective function falls below this tolerance level.\n",
    "\n",
    "`Choice`: hp.loguniform('tol', -6, -1) samples values from a log-uniform distribution between 1e-6 and 1e-1.\n",
    "\n",
    "`Rationale`: Similar to regParam, the log-uniform distribution allows exploring a wide range of tolerance values.\n",
    "\n",
    "**Alternative Choices:**\n",
    "\n",
    "`Uniform Distribution`: If you have a better understanding of the appropriate tolerance range, you could use a uniform distribution.\n",
    "\n",
    "`Fixed Value`: In some cases, you might set a fixed tolerance value based on the desired level of precision.\n",
    "\n",
    "**Overall Assessment:**\n",
    "\n",
    "The hyperparameter choices in the provided search space are reasonable for a starting point. \n",
    "\n",
    "The ranges and distributions allow for a broad exploration of different configurations.\n",
    "\n",
    "Fine-tuning the search space might be beneficial based on the results of initial experiments and your understanding of the problem and the SVM model.\n",
    "\n",
    "Exploring additional hyperparameters such as fitIntercept or different kernels for non-linear \n",
    "\n",
    "SVMs could be considered depending on the complexity of the data and the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ba42b63b-7f79-4b0b-9514-bf97a712ceb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the search space for Hyperopt\n",
    "svm_search_space = {\n",
    "    'maxIter': scope.int(hp.quniform('maxIter', 10, 200, 10)),\n",
    "    'regParam': hp.loguniform('regParam', -6, 0),\n",
    "    'tol': hp.loguniform('tol', -6, -1),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c20d6213-3465-439c-a094-05864f3ab4e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the objective function for Hyperopt\n",
    "def svm_objective(params):\n",
    "    svm = LinearSVC(labelCol=\"payment_event_flag\", featuresCol=\"features\", **params)\n",
    "    svm_model = svm.fit(features_train)\n",
    "    predictions = svm_model.transform(features_test)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"payment_event_flag\")\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    # Log metrics and params to W&B\n",
    "    wandb.log({\"svm_hyperopt_auc\": auc, \"svm_hyperopt_params\": params})\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "09684c94-b887-448e-87dd-8bb7e2c20bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update W&B run config\n",
    "wandb.config.update({\n",
    "    \"model_type\": \"LinearSVCHyperopt\",\n",
    "    \"hyperopt_search_space\": svm_search_space\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "67ac9c05-caea-4b33-b5f9-70a3cd0f9fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:34<00:00,  5.14s/trial, best loss: -0.7137594902071325]\n"
     ]
    }
   ],
   "source": [
    "# Run Hyperopt optimization\n",
    "svm_trials = Trials()\n",
    "best_svm_params = fmin(\n",
    "    fn=svm_objective,\n",
    "    space=svm_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=30,\n",
    "    trials=svm_trials\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "15a49af9-1dee-4bca-98d5-dbfc090bb07f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'maxIter': 80.0, 'regParam': 0.005145783339572193, 'tol': 0.00313329957772047}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svm_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f05b1aa6-09ca-4d17-a190-e60da5c217c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log best hyperparameters\n",
    "wandb.log({\"best_svm_hyperparameters\": best_svm_params})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "70c0e302-3b17-4015-926d-39d8d019a0d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the best SVM model\n",
    "best_svm = LinearSVC(labelCol=\"payment_event_flag\", featuresCol=\"features\", **best_svm_params)\n",
    "best_svm_model = best_svm.fit(features_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6a6f170a-b717-4bd6-823a-1c01ce5db329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the best SVM model\n",
    "best_svm = LinearSVC(labelCol=\"payment_event_flag\", featuresCol=\"features\", **best_svm_params)\n",
    "best_svm_model = best_svm.fit(features_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "362e53e0-693f-426b-8465-49fe45d4af01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "best_predictions = best_svm_model.transform(features_test)\n",
    "best_evaluator = BinaryClassificationEvaluator(labelCol=\"payment_event_flag\")\n",
    "best_svm_auc = best_evaluator.evaluate(best_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6fffb137-15d2-4323-bcc1-df9a776c76ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate classification report for the best model\n",
    "y_true = best_predictions.select('payment_event_flag').rdd.map(lambda x: x['payment_event_flag']).collect()\n",
    "y_pred = best_predictions.select('prediction').rdd.map(lambda x: x['prediction']).collect()\n",
    "best_report = classification_report(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b598e287-68ff-4555-bc92-85ea270bc5b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.50      0.56      3948\n",
      "           1       0.64      0.75      0.69      4669\n",
      "\n",
      "    accuracy                           0.64      8617\n",
      "   macro avg       0.63      0.63      0.62      8617\n",
      "weighted avg       0.64      0.64      0.63      8617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(best_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "42f03887-2ef4-466b-b62a-65a2fd4de334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert report to HTML format\n",
    "hyperopt_report_html = classification_report(y_true, y_pred, output_dict=False)\n",
    "\n",
    "# fix \n",
    "hyperopt_svm_report_html = dict_to_html_table(svm_report)\n",
    "\n",
    "# Log the report as HTML to W&B\n",
    "wandb.log({\"hopt_best_svm_classification_report\": wandb.Html(hyperopt_svm_report_html)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b01273d4-c000-4bf6-8db2-52e6afa6bd36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM ROC AUC: 0.7137535769674866\n",
      "Best SVM PR AUC: 0.7562001878676617\n"
     ]
    }
   ],
   "source": [
    "# Get prediction probabilities and true labels\n",
    "prediction_probs = best_predictions.select(\"rawPrediction\", \"payment_event_flag\")\n",
    "\n",
    "# Convert to RDD and format for BinaryClassificationMetrics\n",
    "preds_rdd = prediction_probs.rdd.map(lambda lp: (float(lp[0][1]), float(lp[1])))\n",
    "\n",
    "# Create BinaryClassificationMetrics object\n",
    "metrics = BinaryClassificationMetrics(preds_rdd)\n",
    "\n",
    "# Calculate AUC-ROC and AUC-PR\n",
    "roc_auc = metrics.areaUnderROC\n",
    "pr_auc = metrics.areaUnderPR\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Best SVM ROC AUC: {roc_auc}\")\n",
    "print(f\"Best SVM PR AUC: {pr_auc}\")\n",
    "\n",
    "# Log the metrics to W&B\n",
    "wandb.log({\"best_svm_roc_auc\": roc_auc})\n",
    "wandb.log({\"best_svm_pr_auc\": pr_auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d2b80817-91fe-4911-bf77-287a7c6d2cfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features for Hyperopt SVM Model:\n",
      "Feature 70: ['lag_1week_to_2weeks_Мои штрафы/Оплата/Оплата не прошла'] - Weight: -1.1312\n",
      "Feature 538: ['lag_6weeks_to_7weeks_Мои штрафы/Оплата/Завершили оплату', 'lag_6weeks_to_7weeks_Пуш Локальный/Скидочный штраф/Показан', 'lag_6weeks_to_7weeks_Страхование/Главная/Лендинг/Ошибка загрузки лендинга'] - Weight: -0.6258\n",
      "Feature 1184: ['lag_12weeks_to_13weeks_Мои штрафы/Задолженность/Долг 10 т.р', 'lag_12weeks_to_13weeks_Справочник/ПДД/Открыли статью'] - Weight: 0.4250\n",
      "Feature 5: ['lag_1week_to_2weeks_Мои штрафы/Оплата/Завешили оплату', 'lag_1week_to_2weeks_Справочник/Поиск/Открыл КоАП из выдачи'] - Weight: 0.4221\n",
      "Feature 2211: ['lag_23weeks_to_24weeks_Мои штрафы/Оплата/Начали множественную оплату', 'lag_23weeks_to_24weeks_Push/Получен пуш от api', 'lag_23weeks_to_24weeks_Мои штрафы/Редактирование документа'] - Weight: 0.3908\n",
      "Feature 2015: ['lag_21weeks_to_22weeks_Проверка/История платежей/Детали'] - Weight: 0.3838\n",
      "Feature 1475: ['lag_15weeks_to_16weeks_Проверка/Список штрафов/Дата ОСАГО/Показ'] - Weight: 0.3807\n",
      "Feature 2246: ['lag_23weeks_to_24weeks_Мои штрафы/Оплата/Сберпей/Открыт'] - Weight: 0.3741\n",
      "Feature 1682: ['lag_17weeks_to_18weeks_Мои штрафы/Оплата/Ушел с ввода данных'] - Weight: 0.3663\n",
      "Feature 1096: ['lag_11weeks_to_12weeks_ОСАГО/Дата покупки/Форма/Показан'] - Weight: -0.3564\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients (feature weights) from the best SVM model\n",
    "best_svm_coef = best_svm_model.coefficients\n",
    "\n",
    "# Zip coefficients with feature names\n",
    "features_weights = list(zip(features_dict.keys(), best_svm_coef))\n",
    "\n",
    "# Sort features by absolute value of their weights (descending order)\n",
    "features_weights.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print top features and their weights\n",
    "print(\"Top Features for Hyperopt SVM Model:\")\n",
    "for feat_num, weight in features_weights[:10]:\n",
    "    print(f\"Feature {feat_num}: {features_dict[feat_num]} - Weight: {weight:.4f}\")\n",
    "\n",
    "# Create a DataFrame for feature importance (without weights)\n",
    "feature_importance_df = pd.DataFrame(features_weights, columns=[\"Feature Number\", \"Weight\"])\n",
    "\n",
    "# Log the DataFrame as a W&B Table\n",
    "wandb.log({\"best_svm_feature_importance\": wandb.Table(dataframe=feature_importance_df)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0debe27c-7633-4480-a9be-449e8a243f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate total absolute weight\n",
    "total_weight = sum(abs(weight) for _, weight in features_weights)\n",
    "\n",
    "# Create a list to store feature information with weight percentages\n",
    "feature_info = []\n",
    "\n",
    "# Extract information for top 20 features with weight percentages\n",
    "for feat_num, weight in features_weights[:20]:\n",
    "    event_names = features_dict[feat_num]\n",
    "    for event_name in event_names:\n",
    "        percentage_contribution = (abs(weight) / total_weight) * 100\n",
    "        feature_info.append({\n",
    "            \"Feature Number\": feat_num,\n",
    "            \"Event Name\": event_name,\n",
    "            \"Weight\": weight,\n",
    "            \"Percentage Contribution\": f\"{percentage_contribution:.2f}%\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame for weighted feature importance\n",
    "feature_weight_df = pd.DataFrame(feature_info)\n",
    "\n",
    "# Log the DataFrame as a W&B Table\n",
    "wandb.log({\"best_svm_feature_weight\": wandb.Table(dataframe=feature_weight_df)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b02af-6632-4fe3-ac09-79bcffba00a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76898c24-ba92-4be3-97ec-770e0b456b1d",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Below code will finish your W&B run</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbbbd2e-751e-46fd-a868-15a6003d2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()  # Finalize W&B run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
