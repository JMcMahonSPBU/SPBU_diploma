{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed9ee60-5a17-4547-bcab-959751b81a99",
   "metadata": {
    "id": "fed9ee60-5a17-4547-bcab-959751b81a99"
   },
   "source": [
    "## 1. Libraries and Spark setup\n",
    "\n",
    "\n",
    "This section imports necessary libraries and sets up the Spark environment:\n",
    "\n",
    "Libraries:\n",
    "* `o`, `sys`, `json`, `datetime`, `numpy`, `pandas`, `tqdm`, `matplotlib.pyplot`: General purpose libraries for file system access, system functionalities, JSON handling, date/time manipulation, numerical computation, data manipulation, progress bars, and plotting.\n",
    "\n",
    "pyspark libraries:\n",
    "* `SparkContext`, `SparkConf`: Core Spark functionalities for setting up the Spark context and configuration.\n",
    "* `SparkSession`: Entry point for interacting with Spark SQL.\n",
    "* `functions as F`: Provides various Spark SQL functions for data manipulation.\n",
    "* `types`: Defines data types for Spark DataFrames.\n",
    "* `Window`: Used for window functions in Spark SQL.\n",
    "* `ml.feature`: Provides feature engineering and transformation tools like `Word2Vec`, `Imputer`, `OneHotEncoder`, `StringIndexer`, `VectorAssembler`.\n",
    "* `ml.classification`: Provides classification algorithms like `LogisticRegression` and `RandomForestClassifier`.\n",
    "* `ml.evaluation`: Provides evaluation metrics like `BinaryClassificationEvaluator` and `BinaryClassificationMetrics`.\n",
    "* `ml.tuning`: Provides tools for hyperparameter tuning like `CrossValidator` and `ParamGridBuilder`.\n",
    "\n",
    "Spark Configuration:\n",
    "* `SparkConf`: Sets configuration parameters for the Spark application.\n",
    "* `spark.master`: Specifies the cluster manager; local[*] indicates using all available cores on the local machine.\n",
    "* `spark.driver.memory`, `spark.driver.maxResultSize`: Allocates memory for the driver process.\n",
    "* `SparkContext`, `SparkSession`: Creates the Spark context and session based on the configuration.\n",
    "\n",
    "Accessing Data:\n",
    "* `access_data`: Function to load JSON data from a local file.\n",
    "* `access_s3_data`: Loads AWS credentials from a local JSON file.\n",
    "\n",
    "Spark configuration is further set to access data from Yandex Cloud Storage (S3-compatible) using the loaded credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d90ad87-b0ec-418e-bfd5-b50aa0d9281c",
   "metadata": {
    "id": "5d90ad87-b0ec-418e-bfd5-b50aa0d9281c",
    "outputId": "ae26f0b2-b640-444f-e333-4cbc4ad2e103",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Using cached wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Collecting setproctitle\n",
      "  Using cached setproctitle-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.31.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from wandb) (4.11.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.9/site-packages (from wandb) (4.2.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.9.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Using cached sentry_sdk-2.4.0-py2.py3-none-any.whl (289 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Installing collected packages: urllib3, setproctitle, docker-pycreds, sentry-sdk, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.8\n",
      "    Uninstalling urllib3-1.26.8:\n",
      "      Successfully uninstalled urllib3-1.26.8\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.8.22 requires urllib3<2, but you have urllib3 2.2.1 which is incompatible.\n",
      "botocore 1.34.84 requires urllib3<1.27,>=1.25.4; python_version < \"3.10\", but you have urllib3 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed docker-pycreds-0.4.0 sentry-sdk-2.4.0 setproctitle-1.3.3 urllib3-2.2.1 wandb-0.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a78ba91-a8ad-43c3-806e-5fc23c39a35b",
   "metadata": {
    "id": "2a78ba91-a8ad-43c3-806e-5fc23c39a35b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Create filenames\n",
    "\n",
    "import datetime\n",
    "import uuid\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "803337b4-f484-46e1-9a9d-da8099c7b0c1",
   "metadata": {
    "id": "803337b4-f484-46e1-9a9d-da8099c7b0c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set name of notebook\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"full_pipeline_with_logging_hyperopt_Anna_1month.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897724d7-5ef5-4ff1-9b8e-a33ae8457ca2",
   "metadata": {
    "id": "897724d7-5ef5-4ff1-9b8e-a33ae8457ca2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pyspark general\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Pyspark pre-processing\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Pyspark vectorization\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "#\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Pyspark models\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "# Pyspark classifiers\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Pyspark cross-validation\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Pyspark reporting\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Pyspark other\n",
    "from pyspark.ml.functions import vector_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e1a5c85-715f-4d1c-b23b-c30165e31c4a",
   "metadata": {
    "id": "2e1a5c85-715f-4d1c-b23b-c30165e31c4a",
    "outputId": "248aa462-cbd4-4536-b7d9-7b6414ee9c38",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.9/site-packages (from hyperopt) (2.6.3)\n",
      "Collecting future\n",
      "  Using cached future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from hyperopt) (4.62.3)\n",
      "Requirement already satisfied: py4j in /opt/conda/lib/python3.9/site-packages (from hyperopt) (0.10.9.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from hyperopt) (1.21.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from hyperopt) (1.8.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.9/site-packages (from hyperopt) (2.0.0)\n",
      "Installing collected packages: future, hyperopt\n",
      "Successfully installed future-1.0.0 hyperopt-0.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab30d97-b956-49a8-866c-64c943e7bb3e",
   "metadata": {
    "id": "6ab30d97-b956-49a8-866c-64c943e7bb3e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperopt related\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll.base import scope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d7f747f-d28b-4b5a-a226-d0e2a344ea28",
   "metadata": {
    "id": "7d7f747f-d28b-4b5a-a226-d0e2a344ea28",
    "outputId": "51f0c94e-67d7-4b41-e404-a5807fff5cce",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Weights and biases login\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39mlogin()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "# Weights and biases login\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b23ca08e-c132-426e-be2b-acaee090a674",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "b23ca08e-c132-426e-be2b-acaee090a674",
    "outputId": "4cdf2a4e-a9cb-4a87-ece0-f3685bb0c05a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# W&B logging\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f214c4d-2142-4557-814f-4fb22a0370d4",
   "metadata": {
    "id": "2f214c4d-2142-4557-814f-4fb22a0370d4",
    "outputId": "032c55a8-9d4d-4d8e-90ef-e794a979a611",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: /user/st110923/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/user/st110923/proxy/4040/jobs/\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7efc215ef2e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set('spark.master', 'local[*]')\n",
    "conf.set('spark.driver.memory', '32G')\n",
    "conf.set('spark.driver.maxResultSize', '8G')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd2c2d3-3816-423f-b760-a70937e6b478",
   "metadata": {
    "id": "4fd2c2d3-3816-423f-b760-a70937e6b478",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def access_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        access_data = json.load(file)\n",
    "    return access_data\n",
    "\n",
    "access_s3_data = access_data('.access_jhub_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05eb3b93-0d2b-467f-9ef8-20dcadc89256",
   "metadata": {
    "id": "05eb3b93-0d2b-467f-9ef8-20dcadc89256",
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set('fs.s3a.access.key', access_s3_data['aws_access_key_id'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.secret.key', access_s3_data['aws_secret_access_key'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.impl','org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.multipart.size', '104857600')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.block.size', '33554432')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.threads.max', '256')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 'http://storage.yandexcloud.net')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.aws.credentials.provider',\n",
    "                                     'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d37a69-949f-44e2-bf89-ec7a726edc7e",
   "metadata": {
    "id": "54d37a69-949f-44e2-bf89-ec7a726edc7e",
    "tags": []
   },
   "source": [
    "v5 = data week\n",
    "2024, 1, 1, 0, 0, 0\n",
    "     2024, 4, 16, 23, 59, 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6671005-60e1-471a-9510-60ae3dad10b6",
   "metadata": {
    "id": "c6671005-60e1-471a-9510-60ae3dad10b6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "VER = 'vA1'\n",
    "PROC_DS = True\n",
    "PROC_LAGS = True\n",
    "FRAC_0 = .001 # used only if `PROC_LAGS = True`\n",
    "PROC_VECS = True\n",
    "BUCKET = 'pvc-84ea79a0-dc20-4a2d-86ab-f83c1f8d4a7b'\n",
    "PRJ_PATH = '/home/jovyan/__RAYPFP'\n",
    "\n",
    "files_path = 'data/events'\n",
    "files_mask = f'{files_path}/data_202*-*-*.csv'\n",
    "\n",
    "file_path_ds = f's3a://{BUCKET}/work/{VER}/data_raw.parquet'\n",
    "file_path_lags = f's3a://{BUCKET}/work/{VER}/data_lags.parquet'\n",
    "file_path_trn = f's3a://{BUCKET}/work/{VER}/data_vec_train.parquet'\n",
    "file_path_tst = f's3a://{BUCKET}/work/{VER}/data_vec_test.parquet'\n",
    "file_path_lags1 = f's3a://{BUCKET}/work/{VER}/data_lags1.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e5c4d7-0cbd-49c8-962b-26df684b9491",
   "metadata": {
    "id": "70e5c4d7-0cbd-49c8-962b-26df684b9491",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#file_path_lags1 = f's3a://{BUCKET}/work/{VER}/data_lags1.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7f1e5a2-7795-49d9-b139-3a63385380de",
   "metadata": {
    "id": "b7f1e5a2-7795-49d9-b139-3a63385380de",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_parquet(path):\n",
    "    cmd = path.replace(\n",
    "        f's3a://{BUCKET}',\n",
    "        f'rm -rf {PRJ_PATH}'\n",
    "    )\n",
    "    !{cmd}\n",
    "    return f'command to run: {cmd}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffaa2985-0b24-4b13-a90f-6584f77e709a",
   "metadata": {
    "id": "ffaa2985-0b24-4b13-a90f-6584f77e709a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create filenames that will be used later when saving predictions\n",
    "\n",
    "current_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "current_user = os.environ['JUPYTERHUB_SERVICE_PREFIX']\n",
    "current_user = current_user.split(\"/\")[2]\n",
    "unique_identifier = str(uuid.uuid4())[:8]  # Generate a unique identifier (first 8 characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3723b66f-323b-43ca-8ef1-95381160e23a",
   "metadata": {
    "id": "3723b66f-323b-43ca-8ef1-95381160e23a",
    "outputId": "7c8ee556-a819-4e35-ebfc-47a748e1d5b9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Filename Information:**\n",
      "- Current Date and Time: 20240605_115859\n",
      "- Current User: st110923\n",
      "- Unique Identifier: e2d6ed98\n"
     ]
    }
   ],
   "source": [
    "print(\"**Filename Information:**\")\n",
    "print(f\"- Current Date and Time: {current_datetime}\")\n",
    "print(f\"- Current User: {current_user}\")\n",
    "print(f\"- Unique Identifier: {unique_identifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1718da-1149-4422-9160-9a96d3ea1be7",
   "metadata": {
    "id": "bf1718da-1149-4422-9160-9a96d3ea1be7"
   },
   "source": [
    "### 2.1. Load or preprocess data - `raw` stage\n",
    "\n",
    "2.1. Load or Preprocess Data - raw Stage\n",
    "\n",
    "This section checks the `PROC_DS` flag.\n",
    "\n",
    "If True, it reads raw CSV data from S3, parses timestamps, filters and flags payment events within a specific timeframe, and selects relevant columns.\n",
    "\n",
    "The processed data is then saved as a parquet file in S3 and the DataFrame is unloaded from memory.\n",
    "\n",
    "Finally, it reads the processed data from the parquet file and displays a few rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "IZstNbeVJy57",
   "metadata": {
    "id": "IZstNbeVJy57",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "flag_min_datetime = datetime.datetime(2024, 3, 21, 0, 0, 0)\n",
    "\n",
    "flag_max_datetime = datetime.datetime(2024, 4, 18, 23, 59, 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec139b21-815a-4680-a53e-c43a5e24d355",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "ec139b21-815a-4680-a53e-c43a5e24d355",
    "outputId": "5366df1b-3966-4aef-fc0a-2c11b27a3ed9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-21 00:00:00 2024-04-18 23:59:59\n",
      "CPU times: user 16.5 ms, sys: 17.7 ms, total: 34.2 ms\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if PROC_DS:\n",
    "    sdf = spark.read.option('escape','\"').csv(f's3a://{BUCKET}/{files_mask}', header=True)\n",
    "    sdf = sdf.withColumn('event_datetime', F.to_timestamp(\"event_datetime\"))\n",
    "    flag_min_datetime = datetime.datetime(2024, 3, 21, 0, 0, 0)\n",
    "    flag_max_datetime = datetime.datetime(2024, 4, 18, 23, 59, 59)\n",
    "    print(flag_min_datetime, flag_max_datetime)\n",
    "    sdf = sdf.withColumn(\n",
    "        'payment_event_flag',\n",
    "        (\n",
    "            (F.col('event_name').like('%Мои штрафы/Оплата/Завершили оплату%') |\n",
    "            F.col('event_name').like('%Мои штрафы/Оплата/Платёж принят%')) &\n",
    "            F.col('event_datetime').between(flag_min_datetime, flag_max_datetime)\n",
    "        ).cast(\"int\")\n",
    "    )\n",
    "    sdf = sdf.select(\n",
    "        'profile_id',\n",
    "        'event_datetime',\n",
    "        'payment_event_flag',\n",
    "        'event_name'\n",
    "    )\n",
    "    #sdf.repartition(1).write.parquet(file_path_ds)\n",
    "    sdf.unpersist()\n",
    "sdf = spark.read.parquet(file_path_ds)\n",
    "sdf.limit(5).toPandas()\n",
    "\n",
    "\n",
    "\n",
    "if not PROC_DS:\n",
    "    # Code to execute if PROC_DS is False\n",
    "    flag_min_datetime = datetime.datetime(2024, 3, 21, 0, 0, 0)\n",
    "    flag_max_datetime = datetime.datetime(2024, 4, 18, 23, 59, 59)\n",
    "    print(\"flag min datetime: \", flag_min_datetime, '\\n',\n",
    "          \"flag max datetime: \", flag_max_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f106cea-7908-4444-966a-5b0a04bdeb4e",
   "metadata": {
    "id": "1f106cea-7908-4444-966a-5b0a04bdeb4e",
    "outputId": "a66145a3-6b6e-4000-cd04-20fe2bccaa2f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|payment_event_flag|    count|\n",
      "+------------------+---------+\n",
      "|                 0|752434695|\n",
      "|                 1|    45742|\n",
      "+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.groupBy('payment_event_flag').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8ba75-8dcf-418c-9019-dae245606403",
   "metadata": {
    "id": "40e8ba75-8dcf-418c-9019-dae245606403",
    "tags": []
   },
   "source": [
    "## 2. Dataset\n",
    "\n",
    "\n",
    "This section defines variables for data processing and file paths, then performs several data processing stages:\n",
    "Variables:\n",
    "\n",
    "* `VER`: Version identifier.\n",
    "* `PROC_DS`, `PROC_LAGS`, `PROC_VECS`: Flags to control data processing stages.\n",
    "* `FRAC_0`: Fraction of negative examples to sample when processing lags.\n",
    "* `BUCKET`: S3 bucket name.\n",
    "* `files_path`, `files_mask`: Local paths and masks for raw data files.\n",
    "* `file_path_ds`, `file_path_lags`, `file_path_trn`, `file_path_tst`: S3 paths for different stages of processed data.\n",
    "\n",
    "\n",
    "• `VER = 'v2'`: This defines the version of your data processing pipeline. You might increment this version number when you make significant changes to the processing steps.\n",
    "• `BUCKET` = 'pvc-84ea79a0-dc20-4a2d-86ab-f83c1f8d4a7b': This specifies the name of the bucket where your data is stored (likely on a cloud storage platform like AWS S3 or Yandex Object Storage).\n",
    "\n",
    "**Flags:**\n",
    "\n",
    "• `PROC_DS` (False): This flag controls whether to process the raw dataset. If True, the code will read the raw CSV files, extract relevant columns, filter by date, and create the data_raw.parquet file.\n",
    "\n",
    "* Change to True: When you have new raw data or need to reprocess the existing raw data due to changes in the extraction logic.\n",
    "* Keep as False: When you already have a processed data_raw.parquet file and don't need to re-process it.\n",
    "\n",
    "• `PROC_LAGS` (False): This flag controls whether to process and create lag features. If True, the code will calculate lag features based on event history for different time windows and store them in the data_lags.parquet file.\n",
    "\n",
    "* Change to True: When you need to recalculate lag features, such as when you've changed the time window definitions or added new events.\n",
    "* Keep as False: When you already have the desired lag features in data_lags.parquet and don't need to recompute them.\n",
    "\n",
    "• `FRAC_0` (.001): This variable sets the sampling fraction for events with payment_event_flag = 0 when processing lags. This is used to reduce the size of the dataset for faster processing while maintaining a representative sample.\n",
    "\n",
    "* Adjust Value: You might change this value depending on the size of your dataset and the desired balance between processing time and data representativeness.\n",
    "\n",
    "• `PROC_VECS` (True): This flag controls whether to vectorize the data using TF-IDF. If True, the code will transform the lag features into numerical vectors using TF-IDF and store them in data_vec_train.parquet and data_vec_test.parquet files.\n",
    "\n",
    "* Change to False: If you want to experiment with other vectorization methods or use the data in its raw form.\n",
    "* Keep as True: When TF-IDF vectorization is the desired approach for your modeling tasks.\n",
    "\n",
    "**File Paths:**\n",
    "\n",
    "• `files_path`, `files_mask`: These define the location and pattern of the raw data files.\n",
    "• `file_path_ds`, `file_path_lags`, `file_path_trn`, `file_path_tst`: These specify the storage locations for the processed datasets at different stages of the pipeline.\n",
    "\n",
    "By understanding these flags and variables, you can control which parts of the data processing pipeline are executed, allowing for efficient experimentation and iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a90e8d11-f747-4fca-8f00-043fc4726c56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "a90e8d11-f747-4fca-8f00-043fc4726c56",
    "outputId": "adc2f738-9053-4c8b-a0c5-4c6f78f887db",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mst110923\u001b[0m (\u001b[33mgsom-diploma-jap\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20240605_115918-2oeaz9xg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/2oeaz9xg' target=\"_blank\">fresh-lion-75</a></strong> to <a href='https://wandb.ai/gsom-diploma-jap/ray-diploma' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gsom-diploma-jap/ray-diploma' target=\"_blank\">https://wandb.ai/gsom-diploma-jap/ray-diploma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/2oeaz9xg' target=\"_blank\">https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/2oeaz9xg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/2oeaz9xg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7efc8d4434c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init Weights and Biases to begin storing data\n",
    "\n",
    "wandb.init(project=\"ray-diploma\", config={\n",
    "    \"version\": VER,\n",
    "    \"proc_ds\": PROC_DS,\n",
    "    \"proc_lags\": PROC_LAGS,\n",
    "    \"proc_vecs\": PROC_VECS,\n",
    "    \"frac_0\": FRAC_0,\n",
    "    \"min_datetime\": flag_min_datetime,\n",
    "    \"max_datetime\": flag_max_datetime,\n",
    "    \"current_user\": current_user,\n",
    "    \"uuid\": unique_identifier,\n",
    "\n",
    "},\n",
    "           mode=\"online\",\n",
    "           dir=\"/home/jovyan/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64c8ec84-705b-4e15-b180-28ce1a8e586a",
   "metadata": {
    "id": "64c8ec84-705b-4e15-b180-28ce1a8e586a",
    "outputId": "29abb686-a426-4f51-d449-d4b1c43b176a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Settings {'_args': None, '_aws_lambda': False, '_cli_only_mode': None, '_code_path_local': None, '_colab': False, '_cuda': None, '_disable_meta': False, '_disable_service': False, '_disable_setproctitle': False, '_disable_stats': False, '_disable_update_check': None, '_disable_viewer': None, '_disable_machine_info': False, '_executable': None, '_extra_http_headers': None, '_file_stream_retry_max': 125, '_file_stream_retry_wait_min_seconds': 2.0, '_file_stream_retry_wait_max_seconds': 60.0, '_file_stream_timeout_seconds': 180.0, '_file_transfer_retry_max': 20, '_file_transfer_retry_wait_min_seconds': 2.0, '_file_transfer_retry_wait_max_seconds': 60.0, '_file_transfer_timeout_seconds': 0.0, '_flow_control_custom': False, '_flow_control_disabled': False, '_graphql_retry_max': 20, '_graphql_retry_wait_min_seconds': 2.0, '_graphql_retry_wait_max_seconds': 60.0, '_graphql_timeout_seconds': 30.0, '_internal_check_process': 8.0, '_internal_queue_timeout': 2.0, '_ipython': False, '_jupyter': True, '_jupyter_name': None, '_jupyter_path': None, '_jupyter_root': None, '_kaggle': False, '_live_policy_rate_limit': None, '_live_policy_wait_time': None, '_log_level': 10, '_network_buffer': None, '_noop': False, '_notebook': True, '_offline': False, '_sync': False, '_os': None, '_platform': 'linux', '_proxies': None, '_python': None, '_runqueue_item_id': None, '_require_core': False, '_save_requirements': True, '_service_transport': None, '_service_wait': 30.0, '_shared': False, '_start_datetime': None, '_start_time': None, '_stats_pid': None, '_stats_sample_rate_seconds': 2.0, '_stats_samples_to_average': 15, '_stats_join_assets': True, '_stats_neuron_monitor_config_path': None, '_stats_open_metrics_endpoints': None, '_stats_open_metrics_filters': ('.*',), '_stats_disk_paths': ('/',), '_stats_buffer_size': 0, '_tmp_code_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None/tmp/code', '_tracelog': None, '_unsaved_keys': None, '_windows': False, 'allow_val_change': None, 'anonymous': None, 'api_key': '***REDACTED***', 'azure_account_url_to_access_key': None, 'base_url': 'https://api.wandb.ai', 'code_dir': None, 'colab_url': None, 'config_paths': None, 'console': 'wrap', 'deployment': 'cloud', 'disable_code': False, 'disable_git': False, 'disable_hints': None, 'disable_job_creation': False, 'disabled': False, 'docker': None, 'email': None, 'entity': None, 'files_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None/files', 'force': None, 'fork_from': None, 'git_commit': None, 'git_remote': 'origin', 'git_remote_url': None, 'git_root': None, 'heartbeat_seconds': 30, 'host': None, 'ignore_globs': (), 'init_timeout': 90.0, 'is_local': False, 'job_name': None, 'job_source': None, 'label_disable': None, 'launch': None, 'launch_config_path': None, 'log_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None/logs', 'log_internal': '/home/jovyan/__RAYPFP/wandb/run-None-None/logs/debug-internal.log', 'log_symlink_internal': '/home/jovyan/__RAYPFP/wandb/debug-internal.log', 'log_symlink_user': '/home/jovyan/__RAYPFP/wandb/debug.log', 'log_user': '/home/jovyan/__RAYPFP/wandb/run-None-None/logs/debug.log', 'login_timeout': None, 'mode': 'online', 'notebook_name': None, 'program': None, 'program_abspath': None, 'program_relpath': None, 'project': None, 'project_url': '', 'quiet': None, 'reinit': None, 'relogin': None, 'resume': None, 'resume_fname': '/home/jovyan/__RAYPFP/wandb/wandb-resume.json', 'resumed': False, 'root_dir': '/home/jovyan/__RAYPFP', 'run_group': None, 'run_id': None, 'run_job_type': None, 'run_mode': 'run', 'run_name': None, 'run_notes': None, 'run_tags': None, 'run_url': '', 'sagemaker_disable': None, 'save_code': None, 'settings_system': '/home/jovyan/.config/wandb/settings', 'settings_workspace': '/home/jovyan/__RAYPFP/wandb/settings', 'show_colors': None, 'show_emoji': None, 'show_errors': True, 'show_info': True, 'show_warnings': True, 'silent': False, 'start_method': None, 'strict': None, 'summary_errors': None, 'summary_timeout': 60, 'summary_warnings': 5, 'sweep_id': None, 'sweep_param_path': None, 'sweep_url': '', 'symlink': None, 'sync_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None', 'sync_file': '/home/jovyan/__RAYPFP/wandb/run-None-None/run-None.wandb', 'sync_symlink_latest': '/home/jovyan/__RAYPFP/wandb/latest-run', 'system_sample': 15, 'system_sample_seconds': 2, 'table_raise_on_max_row_limit_exceeded': False, 'timespec': None, 'tmp_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None/tmp', 'username': None, 'wandb_dir': '/home/jovyan/__RAYPFP/wandb/'}>\n"
     ]
    }
   ],
   "source": [
    "settings = wandb.Settings()\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02f3f918-4948-4dff-9f42-ac6a2768af30",
   "metadata": {
    "id": "02f3f918-4948-4dff-9f42-ac6a2768af30",
    "outputId": "5663aa1e-1508-4ab1-a687-79ce57bb5bc1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**After Initial Data Loading**\n",
      "   payment_event_flag      count\n",
      "0                   0  752434695\n",
      "1                   1      45742\n"
     ]
    }
   ],
   "source": [
    "# Weights and Biases logging here:\n",
    "\n",
    "counts = sdf.groupBy('payment_event_flag').count().toPandas()\n",
    "wandb.log({\"data_count\": counts})\n",
    "print(\"**After Initial Data Loading**\")\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c54c5-820c-4822-825f-4557981ad2b6",
   "metadata": {
    "id": "c22c54c5-820c-4822-825f-4557981ad2b6"
   },
   "source": [
    "### 2.2. Load or preprocess data - `lags` stage\n",
    "\n",
    "2.2. Load or Preprocess Data - lags Stage\n",
    "\n",
    "The `dataset_lags` function defines window specifications for different time intervals (e.g., 10 minutes to 1 hour, 1 day to 3 days).\n",
    "\n",
    "It then uses these windows to calculate the list of event names within each time interval for each profile, creating lag features.\n",
    "\n",
    "If `PROC_LAGS` is True, the function samples the data based on the `payment_event_flag` and the specified fraction for negative examples.\n",
    "\n",
    "The processed data with lag features is saved as a parquet file and unloaded from memory.\n",
    "\n",
    "Finally, it reads the data with lags and displays the count of positive and negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546adde-a046-46d8-8fe9-1457ccd281e0",
   "metadata": {
    "id": "e546adde-a046-46d8-8fe9-1457ccd281e0"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fe103c1-8be5-4d1e-9cff-cf52060d5c97",
   "metadata": {
    "id": "1fe103c1-8be5-4d1e-9cff-cf52060d5c97"
   },
   "source": [
    "Lags new implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d69585d-eb48-4a8a-a1b3-e87618a5e488",
   "metadata": {
    "id": "8d69585d-eb48-4a8a-a1b3-e87618a5e488",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataset_lags(sdf, shift=0):\n",
    "    hour = 60 * 60\n",
    "    day = 24 * 60 * 60\n",
    "\n",
    "    w_10min_to_1week = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-7 * day + shift, -10 * 60 + shift))\n",
    "    w_1week_to_2weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-14 * day + shift, -7 * day + shift))\n",
    "    w_2weeks_to_3weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-21 * day + shift, -14 * day + shift))\n",
    "    w_3weeks_to_4weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-28 * day + shift, -21 * day + shift))\n",
    "    w_4weeks_to_5weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-35 * day + shift, -28 * day + shift))\n",
    "    w_5weeks_to_6weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-42 * day + shift, -35 * day + shift))\n",
    "    w_6weeks_to_7weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-49 * day + shift, -42 * day + shift))\n",
    "    w_7weeks_to_8weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-56 * day + shift, -49 * day + shift))\n",
    "    w_8weeks_to_9weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-63 * day + shift, -56 * day + shift))\n",
    "    w_9weeks_to_10weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-70 * day + shift, -63 * day + shift))\n",
    "    w_10weeks_to_11weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-77 * day + shift, -70 * day + shift))\n",
    "    w_11weeks_to_12weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-84 * day + shift, -77 * day + shift))\n",
    "    w_12weeks_to_13weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-91 * day + shift, -84 * day + shift))\n",
    "    w_13weeks_to_14weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-98 * day + shift, -91 * day + shift))\n",
    "    w_14weeks_to_15weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-105 * day + shift, -98 * day + shift))\n",
    "    w_15weeks_to_16weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-112 * day + shift, -105 * day + shift))\n",
    "    w_16weeks_to_17weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-119 * day + shift, -112 * day + shift))\n",
    "    w_17weeks_to_18weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-126 * day + shift, -119 * day + shift))\n",
    "    w_18weeks_to_19weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-133 * day + shift, -126 * day + shift))\n",
    "    w_19weeks_to_20weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-140 * day + shift, -133 * day + shift))\n",
    "    w_20weeks_to_21weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-147 * day + shift, -140 * day + shift))\n",
    "    w_21weeks_to_22weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-154 * day + shift, -147 * day + shift))\n",
    "    w_22weeks_to_23weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-161 * day + shift, -154 * day + shift))\n",
    "    w_23weeks_to_24weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-168 * day + shift, -161 * day + shift))\n",
    "    w_24weeks_to_25weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-175 * day + shift, -168 * day + shift))\n",
    "    w_25weeks_to_26weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-182 * day + shift, -175 * day + shift))\n",
    "    w_26weeks_to_27weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-189 * day + shift, -182 * day + shift))\n",
    "\n",
    "    return (\n",
    "        sdf\n",
    "            #.withColumn('lag_10min_to_1week', F.collect_list('event_name').over(w_10min_to_1week))\n",
    "            .withColumn('lag_1week_to_2weeks', F.collect_list('event_name').over(w_1week_to_2weeks))\n",
    "            .withColumn('lag_2weeks_to_3weeks', F.collect_list('event_name').over(w_2weeks_to_3weeks))\n",
    "            .withColumn('lag_3weeks_to_4weeks', F.collect_list('event_name').over(w_3weeks_to_4weeks))\n",
    "            .withColumn('lag_4weeks_to_5weeks', F.collect_list('event_name').over(w_4weeks_to_5weeks))\n",
    "            .withColumn('lag_5weeks_to_6weeks', F.collect_list('event_name').over(w_5weeks_to_6weeks))\n",
    "            .withColumn('lag_6weeks_to_7weeks', F.collect_list('event_name').over(w_6weeks_to_7weeks))\n",
    "            .withColumn('lag_7weeks_to_8weeks', F.collect_list('event_name').over(w_7weeks_to_8weeks))\n",
    "            .withColumn('lag_8weeks_to_9weeks', F.collect_list('event_name').over(w_8weeks_to_9weeks))\n",
    "            .withColumn('lag_9weeks_to_10weeks', F.collect_list('event_name').over(w_9weeks_to_10weeks))\n",
    "            .withColumn('lag_10weeks_to_11weeks', F.collect_list('event_name').over(w_10weeks_to_11weeks))\n",
    "            .withColumn('lag_11weeks_to_12weeks', F.collect_list('event_name').over(w_11weeks_to_12weeks))\n",
    "            .withColumn('lag_12weeks_to_13weeks', F.collect_list('event_name').over(w_12weeks_to_13weeks))\n",
    "            .withColumn('lag_13weeks_to_14weeks', F.collect_list('event_name').over(w_13weeks_to_14weeks))\n",
    "            .withColumn('lag_14weeks_to_15weeks', F.collect_list('event_name').over(w_14weeks_to_15weeks))\n",
    "            .withColumn('lag_15weeks_to_16weeks', F.collect_list('event_name').over(w_15weeks_to_16weeks))\n",
    "            .withColumn('lag_16weeks_to_17weeks', F.collect_list('event_name').over(w_16weeks_to_17weeks))\n",
    "            .withColumn('lag_17weeks_to_18weeks', F.collect_list('event_name').over(w_17weeks_to_18weeks))\n",
    "            .withColumn('lag_18weeks_to_19weeks', F.collect_list('event_name').over(w_18weeks_to_19weeks))\n",
    "            .withColumn('lag_19weeks_to_20weeks', F.collect_list('event_name').over(w_19weeks_to_20weeks))\n",
    "            .withColumn('lag_20weeks_to_21weeks', F.collect_list('event_name').over(w_20weeks_to_21weeks))\n",
    "            .withColumn('lag_21weeks_to_22weeks', F.collect_list('event_name').over(w_21weeks_to_22weeks))\n",
    "            .withColumn('lag_22weeks_to_23weeks', F.collect_list('event_name').over(w_22weeks_to_23weeks))\n",
    "            .withColumn('lag_23weeks_to_24weeks', F.collect_list('event_name').over(w_23weeks_to_24weeks))\n",
    "            .withColumn('lag_24weeks_to_25weeks', F.collect_list('event_name').over(w_24weeks_to_25weeks))\n",
    "            .withColumn('lag_25weeks_to_26weeks', F.collect_list('event_name').over(w_25weeks_to_26weeks))\n",
    "            .withColumn('lag_26weeks_to_27weeks', F.collect_list('event_name').over(w_26weeks_to_27weeks))\n",
    "            .select(\n",
    "                'profile_id',\n",
    "                'event_datetime',\n",
    "                'payment_event_flag',\n",
    "                'event_name',\n",
    "                #'lag_10min_to_1week',\n",
    "                'lag_1week_to_2weeks',\n",
    "                'lag_2weeks_to_3weeks',\n",
    "                'lag_3weeks_to_4weeks',\n",
    "                'lag_4weeks_to_5weeks',\n",
    "                'lag_5weeks_to_6weeks',\n",
    "                'lag_6weeks_to_7weeks',\n",
    "                'lag_7weeks_to_8weeks',\n",
    "                'lag_8weeks_to_9weeks',\n",
    "                'lag_9weeks_to_10weeks',\n",
    "                'lag_10weeks_to_11weeks',\n",
    "                'lag_11weeks_to_12weeks',\n",
    "                'lag_12weeks_to_13weeks',\n",
    "                'lag_13weeks_to_14weeks',\n",
    "                'lag_14weeks_to_15weeks',\n",
    "                'lag_15weeks_to_16weeks',\n",
    "                'lag_16weeks_to_17weeks',\n",
    "                'lag_17weeks_to_18weeks',\n",
    "                'lag_18weeks_to_19weeks',\n",
    "                'lag_19weeks_to_20weeks',\n",
    "                'lag_20weeks_to_21weeks',\n",
    "                'lag_21weeks_to_22weeks',\n",
    "                'lag_22weeks_to_23weeks',\n",
    "                'lag_23weeks_to_24weeks',\n",
    "                'lag_24weeks_to_25weeks',\n",
    "                'lag_25weeks_to_26weeks',\n",
    "                'lag_26weeks_to_27weeks'\n",
    "            )\n",
    "        .orderBy(F.col('event_datetime'), ascending=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d70e8-d262-456b-8c7a-c7845b7adfe7",
   "metadata": {
    "id": "ed1d70e8-d262-456b-8c7a-c7845b7adfe7"
   },
   "source": [
    "\n",
    "Added the following code;\n",
    "\n",
    "*     clean_parquet(file_path_lags)\n",
    "*     dates  = (flag_min_datetime, flag_max_datetime)\n",
    "\n",
    "1. `clean_parquet(file_path_lags)`:\n",
    "\n",
    "This line of code is used to clean or remove any existing parquet files at the specified file_path_lags location before writing new data.\n",
    "\n",
    "By calling `clean_parquet(file_path_lags)` before writing the new data with time lag windows, it ensures that any previous data stored at the same location is removed, preventing any conflicts or data inconsistencies.\n",
    "\n",
    "**we have commented it out for this because we actually want to reuse the existing data when training and grid searching our models**\n",
    "\n",
    "2. `dates = (flag_min_datetime, flag_max_datetime):`\n",
    "\n",
    "This line creates a tuple named dates that contains two datetime values: flag_min_datetime and flag_max_datetime.\n",
    "\n",
    "The purpose of dates = (flag_min_datetime, flag_max_datetime) is to create a tuple that represents a date range for filtering the data. The values of flag_min_datetime and flag_max_datetime are defined earlier in the notebook – I.e.\n",
    "\n",
    "\"\"\"flag_min_datetime = datetime.datetime(2023, 8, 1, 0, 0, 0)\n",
    "flag_max_datetime = datetime.datetime(2023, 8, 31, 23, 59, 59)\n",
    "print(flag_min_datetime, flag_max_datetime)\"\"\"\n",
    "\n",
    "After creating the dates tuple, the code uses it to filter the sdf DataFrame based on the event_datetime column.\n",
    "\n",
    "The asterisk (*) before dates is used to unpack the tuple and pass the individual datetime values as arguments to the between function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1096c4e5-1885-4cd3-adde-38d6e407a93c",
   "metadata": {
    "id": "1096c4e5-1885-4cd3-adde-38d6e407a93c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Old version before VG update to filter for dates\n",
    "\n",
    "\n",
    "\n",
    "if PROC_LAGS:\n",
    "    sdf = sdf.sampleBy(\n",
    "        'payment_event_flag',\n",
    "        fractions={0: FRAC_0, 1: 1},\n",
    "        seed=2023\n",
    "    )\n",
    "    sdf = dataset_lags(sdf)\n",
    "    dates  = (flag_min_datetime, flag_max_datetime)\n",
    "    sdf = sdf.filter(sdf.event_datetime.between(*dates))\n",
    "    sdf = sdf.filter(\n",
    "        #(F.size('lag_10min_to_1week')      > 0) |\n",
    "        (F.size('lag_1week_to_2weeks')     > 0) |\n",
    "        (F.size('lag_2weeks_to_3weeks')    > 0) |\n",
    "        (F.size('lag_3weeks_to_4weeks')    > 0) |\n",
    "        (F.size('lag_4weeks_to_5weeks')    > 0) |\n",
    "        (F.size('lag_5weeks_to_6weeks')    > 0) |\n",
    "        (F.size('lag_6weeks_to_7weeks')    > 0) |\n",
    "        (F.size('lag_7weeks_to_8weeks')    > 0) |\n",
    "        (F.size('lag_8weeks_to_9weeks')    > 0) |\n",
    "        (F.size('lag_9weeks_to_10weeks')   > 0) |\n",
    "        (F.size('lag_10weeks_to_11weeks')  > 0) |\n",
    "        (F.size('lag_11weeks_to_12weeks')  > 0) |\n",
    "        (F.size('lag_12weeks_to_13weeks')  > 0) |\n",
    "        (F.size('lag_13weeks_to_14weeks')  > 0) |\n",
    "        (F.size('lag_14weeks_to_15weeks')  > 0) |\n",
    "        (F.size('lag_15weeks_to_16weeks')  > 0) |\n",
    "        (F.size('lag_16weeks_to_17weeks')  > 0) |\n",
    "        (F.size('lag_17weeks_to_18weeks')  > 0) |\n",
    "        (F.size('lag_18weeks_to_19weeks')  > 0) |\n",
    "        (F.size('lag_19weeks_to_20weeks')  > 0) |\n",
    "        (F.size('lag_20weeks_to_21weeks')  > 0) |\n",
    "        (F.size('lag_21weeks_to_22weeks')  > 0) |\n",
    "        (F.size('lag_22weeks_to_23weeks')  > 0) |\n",
    "        (F.size('lag_23weeks_to_24weeks')  > 0) |\n",
    "        (F.size('lag_24weeks_to_25weeks')  > 0) |\n",
    "        (F.size('lag_25weeks_to_26weeks')  > 0) |\n",
    "        (F.size('lag_26weeks_to_27weeks')  > 0)\n",
    "    )\n",
    "    clean_parquet(file_path_lags)\n",
    "    sdf.repartition(8).write.parquet(file_path_lags)\n",
    "    sdf.unpersist()\n",
    "sdf = spark.read.parquet(file_path_lags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad738be7-ee7f-44d1-b7f4-8d7d32a99835",
   "metadata": {
    "id": "ad738be7-ee7f-44d1-b7f4-8d7d32a99835",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weights and Biases logging here:\n",
    "counts = sdf.groupBy('payment_event_flag').count().toPandas()\n",
    "wandb.log({\"data_count_with_lags\": counts})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f346f86-6a48-468d-967f-88c67de8d420",
   "metadata": {
    "id": "8f346f86-6a48-468d-967f-88c67de8d420"
   },
   "source": [
    "Check your dataset has been reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "039dc7dd-985f-42c9-b2a7-455d3c586d49",
   "metadata": {
    "id": "039dc7dd-985f-42c9-b2a7-455d3c586d49",
    "outputId": "996f66e4-3e9b-4f0e-801b-cff90bbae484",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Dataset Size After Lag Feature Creation** /n    payment_event_flag  count\n",
      "0                   1  23109\n",
      "1                   0  19594\n"
     ]
    }
   ],
   "source": [
    "print(\"**Dataset Size After Lag Feature Creation**\", '/n', counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92ef1637-35b4-43bc-85e3-badc3fdf5eab",
   "metadata": {
    "id": "92ef1637-35b4-43bc-85e3-badc3fdf5eab",
    "outputId": "33fc46ba-c63f-4e9c-99cf-8c8880c1b045",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- profile_id: string (nullable = true)\n",
      " |-- event_datetime: timestamp (nullable = true)\n",
      " |-- payment_event_flag: integer (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- lag_1week_to_2weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_2weeks_to_3weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_3weeks_to_4weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_4weeks_to_5weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_5weeks_to_6weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_6weeks_to_7weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_7weeks_to_8weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_8weeks_to_9weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_9weeks_to_10weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_10weeks_to_11weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_11weeks_to_12weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_12weeks_to_13weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_13weeks_to_14weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_14weeks_to_15weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_15weeks_to_16weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_16weeks_to_17weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_17weeks_to_18weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_18weeks_to_19weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_19weeks_to_20weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_20weeks_to_21weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_21weeks_to_22weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_22weeks_to_23weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_23weeks_to_24weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_24weeks_to_25weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_25weeks_to_26weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_26weeks_to_27weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3b490-5702-4c20-883e-613a7d5fa3c0",
   "metadata": {
    "id": "1dd3b490-5702-4c20-883e-613a7d5fa3c0"
   },
   "source": [
    "### Train test split process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8de83425-0fbd-46bc-978a-cfad7ce0c311",
   "metadata": {
    "id": "8de83425-0fbd-46bc-978a-cfad7ce0c311",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define train test split function\n",
    "\n",
    "def stratified_split(sdf, frac, label, seed=2023):\n",
    "    zeros = sdf.filter(sdf[label] == 0)\n",
    "    ones = sdf.filter(sdf[label] == 1)\n",
    "    train_, test_ = zeros.randomSplit([1 - frac, frac], seed=seed)\n",
    "    train, test = ones.randomSplit([1 - frac, frac], seed=seed)\n",
    "    train = train.union(train_)\n",
    "    test = test.union(test_)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa94f255-3f3a-453b-aad9-8772ce8f5a33",
   "metadata": {
    "id": "fa94f255-3f3a-453b-aad9-8772ce8f5a33",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conduct train test split\n",
    "\n",
    "sdf_train, sdf_test = stratified_split(\n",
    "    sdf,\n",
    "    frac=.2, # Size of the test dataset\n",
    "    label='payment_event_flag',\n",
    "    seed=2023\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4c0e6-8257-4ef1-8110-8128ea5dbaf8",
   "metadata": {
    "id": "eda4c0e6-8257-4ef1-8110-8128ea5dbaf8",
    "tags": []
   },
   "source": [
    "Check the dataset has been split into aproximately equal classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "501e35b0-bd8f-4318-a973-155b4fd72a3b",
   "metadata": {
    "id": "501e35b0-bd8f-4318-a973-155b4fd72a3b",
    "outputId": "765bf0c8-51fa-42ef-9926-06fb56a61919",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>18440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>15649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag  count\n",
       "0                   1  18440\n",
       "1                   0  15649"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_train.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "776b6d5a-10bc-4e1c-9460-65e0e204ef82",
   "metadata": {
    "id": "776b6d5a-10bc-4e1c-9460-65e0e204ef82",
    "outputId": "4b6be7f4-6ee2-41e0-c4fb-ce534aa3967d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag  count\n",
       "0                   1   4669\n",
       "1                   0   3945"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_test.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf9adc0b-6be7-4e8c-a092-19f282425ded",
   "metadata": {
    "id": "cf9adc0b-6be7-4e8c-a092-19f282425ded",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weights and Biases logging here:\n",
    "\n",
    "train_counts = sdf_train.groupBy('payment_event_flag').count().toPandas()\n",
    "wandb.log({\"train_data_count\": train_counts})\n",
    "\n",
    "test_counts = sdf_test.groupBy('payment_event_flag').count().toPandas()\n",
    "wandb.log({\"test_data_count\": test_counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ed66acb-2642-463d-870b-d3d0aef7e33a",
   "metadata": {
    "id": "0ed66acb-2642-463d-870b-d3d0aef7e33a",
    "outputId": "80c9e9f4-4ab4-445a-d8bf-d0d4ef49c724",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Date: 2024-03-21 00:00:17\n",
      "Maximum Date: 2024-04-18 23:59:23\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "# Find the minimum and maximum dates\n",
    "min_date = sdf.agg(min(\"event_datetime\")).collect()[0][0]\n",
    "max_date = sdf.agg(max(\"event_datetime\")).collect()[0][0]\n",
    "\n",
    "print(f\"Minimum Date: {min_date}\")\n",
    "print(f\"Maximum Date: {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6541e050-fb85-43bc-9e12-bb25cfbb7a0b",
   "metadata": {
    "id": "6541e050-fb85-43bc-9e12-bb25cfbb7a0b",
    "outputId": "90b8bbd2-a549-4eb8-b8ec-55c80fddd593",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Minimum Date: 2024-03-21 00:00:17\n",
      "Training Set Maximum Date: 2024-04-18 23:57:47\n"
     ]
    }
   ],
   "source": [
    "# Training set\n",
    "train_min_date = sdf_train.agg(min(\"event_datetime\")).collect()[0][0]\n",
    "train_max_date = sdf_train.agg(max(\"event_datetime\")).collect()[0][0]\n",
    "\n",
    "print(f\"Training Set Minimum Date: {train_min_date}\")\n",
    "print(f\"Training Set Maximum Date: {train_max_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80751428-6358-472c-a4f0-65e9f14ceeae",
   "metadata": {
    "id": "80751428-6358-472c-a4f0-65e9f14ceeae",
    "outputId": "99a3d706-47ca-4565-8587-e1463fcd8cf1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Minimum Date: 2024-03-21 00:05:21\n",
      "Test Set Maximum Date: 2024-04-18 23:59:23\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "test_min_date = sdf_test.agg(min(\"event_datetime\")).collect()[0][0]\n",
    "test_max_date = sdf_test.agg(max(\"event_datetime\")).collect()[0][0]\n",
    "\n",
    "print(f\"Test Set Minimum Date: {test_min_date}\")\n",
    "print(f\"Test Set Maximum Date: {test_max_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9856ef-8571-467e-871f-70cbf66e6e6e",
   "metadata": {
    "id": "5f9856ef-8571-467e-871f-70cbf66e6e6e",
    "tags": []
   },
   "source": [
    "### 2.3. Load or preprocess data - `vectorize` stage\n",
    "\n",
    "2.3. Load or Preprocess Data - vectorize Stage\n",
    "\n",
    "This section defines a list of lag features to be used.\n",
    "\n",
    "The datasets_tfidf function performs `TF-IDF` vectorization on the lag features for both training and test datasets.\n",
    "\n",
    "It uses `HashingTF` to convert lists of event names into numerical feature vectors and `IDF` to rescale the features based on their document frequency.\n",
    "\n",
    "The function also creates a dictionary mapping feature indices to the corresponding event names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62b8a249-c54f-493f-9679-baaa4e3dff6f",
   "metadata": {
    "id": "62b8a249-c54f-493f-9679-baaa4e3dff6f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "lags = [\n",
    "#    'lag_10min_to_1week',\n",
    "    'lag_1week_to_2weeks',\n",
    "    'lag_2weeks_to_3weeks',\n",
    "    'lag_3weeks_to_4weeks',\n",
    "    'lag_4weeks_to_5weeks',\n",
    "    'lag_5weeks_to_6weeks',\n",
    "    'lag_6weeks_to_7weeks',\n",
    "    'lag_7weeks_to_8weeks',\n",
    "    'lag_8weeks_to_9weeks',\n",
    "    'lag_9weeks_to_10weeks',\n",
    "    'lag_10weeks_to_11weeks',\n",
    "    'lag_11weeks_to_12weeks',\n",
    "    'lag_12weeks_to_13weeks',\n",
    "    'lag_13weeks_to_14weeks',\n",
    "    'lag_14weeks_to_15weeks',\n",
    "    'lag_15weeks_to_16weeks',\n",
    "    'lag_16weeks_to_17weeks',\n",
    "    'lag_17weeks_to_18weeks',\n",
    "    'lag_18weeks_to_19weeks',\n",
    "    'lag_19weeks_to_20weeks',\n",
    "    'lag_20weeks_to_21weeks',\n",
    "    'lag_21weeks_to_22weeks',\n",
    "    'lag_22weeks_to_23weeks',\n",
    "    'lag_23weeks_to_24weeks',\n",
    "    'lag_24weeks_to_25weeks',\n",
    "    'lag_25weeks_to_26weeks',\n",
    "    'lag_26weeks_to_27weeks'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02591fe2-1f8b-4331-868a-96560fdb17b9",
   "metadata": {
    "id": "02591fe2-1f8b-4331-868a-96560fdb17b9"
   },
   "source": [
    "## TF-IDF implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ddc7853f-0b02-4982-84ca-a58d91b8e6c8",
   "metadata": {
    "id": "ddc7853f-0b02-4982-84ca-a58d91b8e6c8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datasets_tfidf(sdf_train, sdf_test, lags, min_freq=3, num_features=10):\n",
    "    features_dict = {}\n",
    "    count = 0\n",
    "    for lag in tqdm(lags):\n",
    "        hashingTF = HashingTF(\n",
    "            inputCol=lag,\n",
    "            outputCol=lag + '_tf',\n",
    "            numFeatures=num_features\n",
    "        )\n",
    "        featurizedData = hashingTF.transform(sdf_train)\n",
    "        idf = IDF(\n",
    "            inputCol=lag + '_tf',\n",
    "            outputCol=lag + '_tfidf',\n",
    "            minDocFreq=min_freq\n",
    "        )\n",
    "        idfModel = idf.fit(featurizedData)\n",
    "        sdf_train = idfModel.transform(featurizedData)\n",
    "        sdf_test = idfModel.transform(\n",
    "            hashingTF.transform(sdf_test)\n",
    "        )\n",
    "        events = [\n",
    "            x\n",
    "            for xs in sdf_train.select(lag).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            for x in xs\n",
    "        ]\n",
    "        hash_dict = {}\n",
    "        for e in events:\n",
    "            hash_dict[lag + '_' + e] = hashingTF.indexOf(e)\n",
    "        for feat_num in range(num_features):\n",
    "            tmp_list = []\n",
    "            for k, v in hash_dict.items():\n",
    "                if v == feat_num: tmp_list.append(k)\n",
    "            features_dict[count * num_features + feat_num] = tmp_list\n",
    "        count += 1\n",
    "    return sdf_train, sdf_test, features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "697b33f8-e2de-4978-a650-2f82872af511",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19d27bd816340a78d05158deea51380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NEW version with clean_parquet (TF-IDF)\n",
    "\n",
    "if PROC_VECS:\n",
    "    sdf_train, sdf_test, features_dict = datasets_tfidf(\n",
    "        sdf_train,\n",
    "        sdf_test,\n",
    "        lags,\n",
    "        #vec_size=10,\n",
    "        min_freq=3,\n",
    "        num_features=100\n",
    "    )\n",
    "    clean_parquet(file_path_trn)\n",
    "    sdf_train.repartition(8).write.parquet(file_path_trn)\n",
    "    clean_parquet(file_path_tst)\n",
    "    sdf_test.repartition(8).write.parquet(file_path_tst)\n",
    "    sdf_train.unpersist()\n",
    "    sdf_test.unpersist()\n",
    "sdf_train = spark.read.parquet(file_path_trn)\n",
    "sdf_test = spark.read.parquet(file_path_tst)\n",
    "\n",
    "\n",
    "if not PROC_VECS:\n",
    "    sdf_train, sdf_test, features_dict = datasets_tfidf(\n",
    "        sdf_train, \n",
    "        sdf_test, \n",
    "        lags, \n",
    "        min_freq=3,\n",
    "        num_features=100\n",
    "    )\n",
    "    print(len(features_dict.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0ac4bc9-2538-4137-a240-7469094499cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Training Set After TF-IDF**\n",
      "+------------------+-----+\n",
      "|payment_event_flag|count|\n",
      "+------------------+-----+\n",
      "|                 1|18440|\n",
      "|                 0|15649|\n",
      "+------------------+-----+\n",
      "\n",
      "**Testing Set After TF-IDF**\n",
      "+------------------+-----+\n",
      "|payment_event_flag|count|\n",
      "+------------------+-----+\n",
      "|                 1| 4669|\n",
      "|                 0| 3945|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check data size after reloading\n",
    "print(\"**Training Set After TF-IDF**\")\n",
    "sdf_train.groupBy('payment_event_flag').count().show()\n",
    "print(\"**Testing Set After TF-IDF**\")\n",
    "sdf_test.groupBy('payment_event_flag').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4529f5cb-774a-41c5-b067-5e595ccec4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- profile_id: string (nullable = true)\n",
      " |-- event_datetime: timestamp (nullable = true)\n",
      " |-- payment_event_flag: integer (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- lag_1week_to_2weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_2weeks_to_3weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_3weeks_to_4weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_4weeks_to_5weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_5weeks_to_6weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_6weeks_to_7weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_7weeks_to_8weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_8weeks_to_9weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_9weeks_to_10weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_10weeks_to_11weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_11weeks_to_12weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_12weeks_to_13weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_13weeks_to_14weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_14weeks_to_15weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_15weeks_to_16weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_16weeks_to_17weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_17weeks_to_18weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_18weeks_to_19weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_19weeks_to_20weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_20weeks_to_21weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_21weeks_to_22weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_22weeks_to_23weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_23weeks_to_24weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_24weeks_to_25weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_25weeks_to_26weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_26weeks_to_27weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_1week_to_2weeks_tf: vector (nullable = true)\n",
      " |-- lag_1week_to_2weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_2weeks_to_3weeks_tf: vector (nullable = true)\n",
      " |-- lag_2weeks_to_3weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_3weeks_to_4weeks_tf: vector (nullable = true)\n",
      " |-- lag_3weeks_to_4weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_4weeks_to_5weeks_tf: vector (nullable = true)\n",
      " |-- lag_4weeks_to_5weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_5weeks_to_6weeks_tf: vector (nullable = true)\n",
      " |-- lag_5weeks_to_6weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_6weeks_to_7weeks_tf: vector (nullable = true)\n",
      " |-- lag_6weeks_to_7weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_7weeks_to_8weeks_tf: vector (nullable = true)\n",
      " |-- lag_7weeks_to_8weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_8weeks_to_9weeks_tf: vector (nullable = true)\n",
      " |-- lag_8weeks_to_9weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_9weeks_to_10weeks_tf: vector (nullable = true)\n",
      " |-- lag_9weeks_to_10weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_10weeks_to_11weeks_tf: vector (nullable = true)\n",
      " |-- lag_10weeks_to_11weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_11weeks_to_12weeks_tf: vector (nullable = true)\n",
      " |-- lag_11weeks_to_12weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_12weeks_to_13weeks_tf: vector (nullable = true)\n",
      " |-- lag_12weeks_to_13weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_13weeks_to_14weeks_tf: vector (nullable = true)\n",
      " |-- lag_13weeks_to_14weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_14weeks_to_15weeks_tf: vector (nullable = true)\n",
      " |-- lag_14weeks_to_15weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_15weeks_to_16weeks_tf: vector (nullable = true)\n",
      " |-- lag_15weeks_to_16weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_16weeks_to_17weeks_tf: vector (nullable = true)\n",
      " |-- lag_16weeks_to_17weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_17weeks_to_18weeks_tf: vector (nullable = true)\n",
      " |-- lag_17weeks_to_18weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_18weeks_to_19weeks_tf: vector (nullable = true)\n",
      " |-- lag_18weeks_to_19weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_19weeks_to_20weeks_tf: vector (nullable = true)\n",
      " |-- lag_19weeks_to_20weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_20weeks_to_21weeks_tf: vector (nullable = true)\n",
      " |-- lag_20weeks_to_21weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_21weeks_to_22weeks_tf: vector (nullable = true)\n",
      " |-- lag_21weeks_to_22weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_22weeks_to_23weeks_tf: vector (nullable = true)\n",
      " |-- lag_22weeks_to_23weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_23weeks_to_24weeks_tf: vector (nullable = true)\n",
      " |-- lag_23weeks_to_24weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_24weeks_to_25weeks_tf: vector (nullable = true)\n",
      " |-- lag_24weeks_to_25weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_25weeks_to_26weeks_tf: vector (nullable = true)\n",
      " |-- lag_25weeks_to_26weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_26weeks_to_27weeks_tf: vector (nullable = true)\n",
      " |-- lag_26weeks_to_27weeks_tfidf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea01aa-4607-4d22-b162-78486a801ed3",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6ac7c-8823-4888-8125-ffc07e1f7d32",
   "metadata": {},
   "source": [
    "### 3.1. Features assembling\n",
    "\n",
    "3.1. Features Assembling\n",
    "\n",
    "The features_assembled function prepares the data for model training:\n",
    "\n",
    "* It selects the `TF-IDF` features and the target variable (`payment_event_flag`).\n",
    "* It uses VectorAssembler to combine the `TF-IDF` features into a single vector column named `features`.\n",
    "* It returns a DataFrame with the target variable and the assembled feature vector.\n",
    "\n",
    "The upsampled function can be used to address class imbalance:\n",
    "\n",
    "* It separates the data into positive and negative examples.\n",
    "* It duplicates the positive examples to achieve a balanced class distribution based on the `UPSAMPLE` setting.\n",
    "\n",
    "The code then:\n",
    "* Defines a list of lag features to be used.\n",
    "* Assembles features for both training and test sets.\n",
    "* Optionally performs **upsampling** on the training set (and potentially the test set) if `UPSAMPLE` is enabled.\n",
    "* Displays the class distribution after upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e94fdf25-833b-480e-9ce8-26ae7ba3f9ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def features_assembled(sdf, feats):\n",
    "    cols_to_model = [x + '_tfidf' for x in feats]\n",
    "    cols_to_model.extend(['payment_event_flag'])\n",
    "    print('columns to model:', cols_to_model)\n",
    "    vecAssembler = VectorAssembler(\n",
    "        inputCols=[c for c in cols_to_model if c != 'payment_event_flag'], \n",
    "        outputCol='features'\n",
    "    )\n",
    "    features = sdf.select(cols_to_model)\n",
    "    features_vec = vecAssembler.transform(features)\n",
    "    features_data = features_vec.select('payment_event_flag', 'features')\n",
    "    return features_data\n",
    "\n",
    "def upsampled(sdf, label, upsample='max'):\n",
    "    zeros = sdf.filter(sdf[label] == 0)\n",
    "    ones = sdf.filter(sdf[label] == 1)\n",
    "    res = zeros.union(ones)\n",
    "    if upsample == 'max':\n",
    "        up_count = int(zeros.count() / ones.count())\n",
    "        for _ in range(up_count - 1):\n",
    "            res = res.union(ones)\n",
    "    else:\n",
    "        for _ in range(upsample - 1):\n",
    "            res = res.union(ones)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30357662-e766-4b6f-8c76-30f07d7856a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"MAX\" Strategy: Setting UPSAMPLE = 'max' instructs the upsampled function to duplicate the minority class examples\n",
    "# They are upsampled until their count becomes equal to the count of the majority class. \n",
    "# In other words, it aims for a 1:1 class ratio.\n",
    "\n",
    "UPSAMPLE = 'max' # Can be either 'none' or 'max'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b92508ca-27b0-4142-a74e-0ab56f895f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns to model: ['lag_1week_to_2weeks_tfidf', 'lag_2weeks_to_3weeks_tfidf', 'lag_3weeks_to_4weeks_tfidf', 'lag_4weeks_to_5weeks_tfidf', 'lag_5weeks_to_6weeks_tfidf', 'lag_6weeks_to_7weeks_tfidf', 'lag_7weeks_to_8weeks_tfidf', 'lag_8weeks_to_9weeks_tfidf', 'lag_9weeks_to_10weeks_tfidf', 'lag_10weeks_to_11weeks_tfidf', 'lag_11weeks_to_12weeks_tfidf', 'lag_12weeks_to_13weeks_tfidf', 'lag_13weeks_to_14weeks_tfidf', 'lag_14weeks_to_15weeks_tfidf', 'lag_15weeks_to_16weeks_tfidf', 'lag_16weeks_to_17weeks_tfidf', 'lag_17weeks_to_18weeks_tfidf', 'lag_18weeks_to_19weeks_tfidf', 'lag_19weeks_to_20weeks_tfidf', 'lag_20weeks_to_21weeks_tfidf', 'lag_21weeks_to_22weeks_tfidf', 'lag_22weeks_to_23weeks_tfidf', 'lag_23weeks_to_24weeks_tfidf', 'lag_24weeks_to_25weeks_tfidf', 'lag_25weeks_to_26weeks_tfidf', 'lag_26weeks_to_27weeks_tfidf', 'payment_event_flag']\n",
      "columns to model: ['lag_1week_to_2weeks_tfidf', 'lag_2weeks_to_3weeks_tfidf', 'lag_3weeks_to_4weeks_tfidf', 'lag_4weeks_to_5weeks_tfidf', 'lag_5weeks_to_6weeks_tfidf', 'lag_6weeks_to_7weeks_tfidf', 'lag_7weeks_to_8weeks_tfidf', 'lag_8weeks_to_9weeks_tfidf', 'lag_9weeks_to_10weeks_tfidf', 'lag_10weeks_to_11weeks_tfidf', 'lag_11weeks_to_12weeks_tfidf', 'lag_12weeks_to_13weeks_tfidf', 'lag_13weeks_to_14weeks_tfidf', 'lag_14weeks_to_15weeks_tfidf', 'lag_15weeks_to_16weeks_tfidf', 'lag_16weeks_to_17weeks_tfidf', 'lag_17weeks_to_18weeks_tfidf', 'lag_18weeks_to_19weeks_tfidf', 'lag_19weeks_to_20weeks_tfidf', 'lag_20weeks_to_21weeks_tfidf', 'lag_21weeks_to_22weeks_tfidf', 'lag_22weeks_to_23weeks_tfidf', 'lag_23weeks_to_24weeks_tfidf', 'lag_24weeks_to_25weeks_tfidf', 'lag_25weeks_to_26weeks_tfidf', 'lag_26weeks_to_27weeks_tfidf', 'payment_event_flag']\n"
     ]
    }
   ],
   "source": [
    "# Setting feats, make sure to comment out (#) any lags we will not use for train/pred \n",
    "\n",
    "feats = [\n",
    "#    'lag_10min_to_1week',\n",
    "    'lag_1week_to_2weeks',\n",
    "    'lag_2weeks_to_3weeks',\n",
    "    'lag_3weeks_to_4weeks',\n",
    "    'lag_4weeks_to_5weeks',\n",
    "    'lag_5weeks_to_6weeks',\n",
    "    'lag_6weeks_to_7weeks',\n",
    "    'lag_7weeks_to_8weeks',\n",
    "    'lag_8weeks_to_9weeks',\n",
    "    'lag_9weeks_to_10weeks',\n",
    "    'lag_10weeks_to_11weeks',\n",
    "    'lag_11weeks_to_12weeks',\n",
    "    'lag_12weeks_to_13weeks',\n",
    "    'lag_13weeks_to_14weeks',\n",
    "    'lag_14weeks_to_15weeks',\n",
    "    'lag_15weeks_to_16weeks',\n",
    "    'lag_16weeks_to_17weeks',\n",
    "    'lag_17weeks_to_18weeks',\n",
    "    'lag_18weeks_to_19weeks',\n",
    "    'lag_19weeks_to_20weeks',\n",
    "    'lag_20weeks_to_21weeks',\n",
    "    'lag_21weeks_to_22weeks',\n",
    "    'lag_22weeks_to_23weeks',\n",
    "    'lag_23weeks_to_24weeks',\n",
    "    'lag_24weeks_to_25weeks',\n",
    "    'lag_25weeks_to_26weeks',\n",
    "    'lag_26weeks_to_27weeks'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "features_train = features_assembled(sdf_train, feats=feats)\n",
    "features_test = features_assembled(sdf_test, feats=feats)\n",
    "\n",
    "if UPSAMPLE:\n",
    "    features_train = upsampled(\n",
    "        features_train, \n",
    "        label='payment_event_flag', \n",
    "        upsample=UPSAMPLE\n",
    "    )\n",
    "    # Use to upsample test set\n",
    "    #features_test = upsampled(\n",
    "    #    features_test, \n",
    "    #    label='payment_event_flag', \n",
    "    #    upsample=UPSAMPLE\n",
    "    #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64d93700-6697-4c1d-9f19-7de6ae0270ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag  count\n",
       "0                   0  15649\n",
       "1                   1  18440"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3104330c-fe44-484b-b87f-2c98e299ec98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag  count\n",
       "0                   1   4669\n",
       "1                   0   3945"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "859e41fd-1eed-40e4-bd64-0219b540e496",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag                                           features\n",
       "0                   0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "1                   0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2                   0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9b1e3-5cf2-423c-aec1-31b66090c69c",
   "metadata": {},
   "source": [
    "### 3.2. Training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1934bf4-6454-4c2d-a40d-461b360773ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "58b745d1-cfd7-455b-8d37-b39fbc2825d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Prediction File Path: s3a://BUCKET/work/vA1/preds/GBT/22.04.24/st110923/1.csv\n"
     ]
    }
   ],
   "source": [
    "# Define model filepath to link predictions results with the actual CSV \n",
    "from pyspark.ml.classification import GBTClassifier, LogisticRegression, LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"payment_event_flag\", featuresCol=\"features\")\n",
    "\n",
    "file_path_pred = f's3a://BUCKET/work/vA1/preds/GBT/22.04.24/st110923/1.csv'\n",
    "print(f\"- Prediction File Path: {file_path_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98d54abf-a4ec-4905-927b-feeafca5c707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_event_flag: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32749aab-2e97-4396-ae44-56031a9a10eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.9 ms, sys: 1.54 ms, total: 35.4 ms\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = gbt.fit(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18dd5276-634b-4ec0-a7dc-be1a6f73c3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.6616269540406332\n",
      "Area under PR-curve: 0.7618419991852285\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(features_test)\n",
    "payment_event_flag_preds = predictions.select('prediction', 'payment_event_flag')\n",
    "metrics = BinaryClassificationMetrics(\n",
    "    payment_event_flag_preds.rdd.map(\n",
    "        lambda lines: [float(x) for x in lines]\n",
    "    )\n",
    ")\n",
    "print('ROC AUC:', metrics.areaUnderROC)\n",
    "print('Area under PR-curve:', metrics.areaUnderPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be7dd7be-d802-4dad-9b04-7f74755304a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weights and Biases logging here\n",
    "wandb.log({\"gbt_roc_auc\": metrics.areaUnderROC})\n",
    "wandb.log({\"gbt_pr_auc\": metrics.areaUnderPR})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "582c9ba2-e0cd-43a4-bb88-f5f5e6fb69a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.92      0.70      3945\n",
      "           1       0.86      0.40      0.55      4669\n",
      "\n",
      "    accuracy                           0.64      8614\n",
      "   macro avg       0.71      0.66      0.62      8614\n",
      "weighted avg       0.72      0.64      0.62      8614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predictions = model.transform(features_test)\n",
    "\n",
    "y_true = predictions.select('payment_event_flag').rdd.map(lambda x: x['payment_event_flag']).collect()\n",
    "y_pred = predictions.select('prediction').rdd.map(lambda x: x['prediction']).collect()\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e371ee7-926f-4548-8d5b-72425dd7cda3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gbt_report = classification_report(y_true, y_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d71e35b3-5e05-48ef-bad3-a536bc86a2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.log({\"gbt_report\": gbt_report})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c3d872-8216-45e8-a793-15f5e5bca866",
   "metadata": {},
   "source": [
    "Hyperopt optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2070db29-5855-407c-9ffd-25aa075fefd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d0feac06-7f2a-44d6-b5e3-518f1848cbff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:59<00:00, 10.79s/trial, best loss: 0.3016841117735536]\n"
     ]
    }
   ],
   "source": [
    "# Define the space of hyperparameters to search\n",
    "space = {\n",
    "    #'maxDepth': hp.choice('maxDepth', [5, 10, 20]),\n",
    "    #'maxBins': hp.choice('maxBins', [32, 64, 128]),\n",
    "    'maxIter': hp.choice('maxIter', [10, 20, 50]),\n",
    "    'stepSize': hp.uniform('stepSize', 0.01, 0.1),\n",
    "    'minInstancesPerNode': hp.choice('minInstancesPerNode', [1, 2, 4]),\n",
    "    'minInfoGain': hp.uniform('minInfoGain', 0.0, 0.1)\n",
    "}\n",
    "\n",
    "# Define the objective function to minimize\n",
    "def objective(params):\n",
    "    # Create a GBTClassifier with the current set of hyperparameters\n",
    "    gbt = GBTClassifier(featuresCol='features',\n",
    "                        labelCol='payment_event_flag',\n",
    "                        seed=42,\n",
    "                        #maxDepth=params['maxDepth'],\n",
    "                        #maxBins=params['maxBins'],\n",
    "                        maxIter=params['maxIter'],\n",
    "                        stepSize=params['stepSize'],\n",
    "                        minInstancesPerNode=params['minInstancesPerNode'],\n",
    "                        minInfoGain=params['minInfoGain'])\n",
    "    \n",
    "    # Create a Pipeline with the GBTClassifier\n",
    "    pipeline = Pipeline(stages=[gbt])\n",
    "    \n",
    "    # Fit the model\n",
    "    model = pipeline.fit(features_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.transform(features_test)\n",
    "    \n",
    "    # Evaluate the model using AUC\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol='payment_event_flag', metricName='areaUnderROC')\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "\n",
    "    # Hyperopt minimizes the objective function, so return 1 - AUC as a loss to minimize\n",
    "    return {'loss': 1 - auc, 'status': STATUS_OK}\n",
    "\n",
    "# Create a trials object to store the results\n",
    "trials = Trials()\n",
    "\n",
    "# Run the hyperparameter search using the Tree of Parzen Estimators (TPE) algorithm\n",
    "best_params = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,  # Set to the number of evaluations you want to run\n",
    "    trials=trials\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d8697a4-72d7-4dbb-b000-18bbe535c6b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'maxIter': 2, 'stepSize': 0.08809824834577663, 'minInstancesPerNode': 1, 'minInfoGain': 7.100061569996724e-05}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9174b374-71da-4dbe-9b7d-6b62f0d2a477",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC ROC of the best model:  0.6729618623605091\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "best_params = {\n",
    "    'maxIter': 2,\n",
    "    'stepSize':  0.08809824834577663,\n",
    "    'minInstancesPerNode': 1,\n",
    "    'minInfoGain':7.100061569996724e-05\n",
    "}\n",
    "\n",
    "# Create a GBTClassifier with the best set of hyperparameters\n",
    "gbt_best = GBTClassifier(featuresCol='features',\n",
    "                         labelCol='payment_event_flag',\n",
    "                         seed=42,\n",
    "                         maxDepth=10,\n",
    "                         maxBins=32,\n",
    "                         maxIter=best_params['maxIter'],\n",
    "                         stepSize=best_params['stepSize'],\n",
    "                         minInstancesPerNode=best_params['minInstancesPerNode'],\n",
    "                         minInfoGain=best_params['minInfoGain'])\n",
    "\n",
    "# Create a Pipeline with the GBTClassifier\n",
    "pipeline_best = Pipeline(stages=[gbt_best])\n",
    "\n",
    "# Fit the model on the entire dataset\n",
    "model_best = pipeline_best.fit(features_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_best = model_best.transform(features_test)\n",
    "\n",
    "# Evaluate the model using AUC\n",
    "evaluator_best = BinaryClassificationEvaluator(labelCol='payment_event_flag', metricName='areaUnderROC')\n",
    "auc_best = evaluator_best.evaluate(predictions_best)\n",
    "\n",
    "print(\"AUC ROC of the best model: \", auc_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "09f75bc9-6a6c-47d4-9a0d-e600c87143c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC ROC of the best model:  0.6729618623605091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a GBTClassifier with the best set of hyperparameters\n",
    "gbt_best = GBTClassifier(featuresCol='features',\n",
    "                         labelCol='payment_event_flag',\n",
    "                         seed=42,\n",
    "                         maxDepth=10,\n",
    "                         maxBins=32,\n",
    "                         maxIter=2,\n",
    "                         stepSize=0.08809824834577663,\n",
    "                         minInstancesPerNode=1,\n",
    "                         minInfoGain=7.100061569996724e-05)\n",
    "\n",
    "# Create a Pipeline with the GBTClassifier\n",
    "pipeline_best = Pipeline(stages=[gbt_best])\n",
    "\n",
    "# Fit the model on the entire dataset\n",
    "model_best = pipeline_best.fit(features_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_best = model_best.transform(features_test)\n",
    "\n",
    "# Evaluate the model using AUC\n",
    "evaluator_best = BinaryClassificationEvaluator(labelCol='payment_event_flag', metricName='areaUnderROC')\n",
    "auc_best = evaluator_best.evaluate(predictions_best)\n",
    "\n",
    "print(\"AUC ROC of the best model: \", auc_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f22d0-dbb9-43ee-ac62-e7ef185e6bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Log metrics and params to W&B\n",
    "wandb.log({\"roc_auc_hyperopt\": auc_best, \"params_hyperopt\":  best_params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "829c617b-a15e-43fa-8ff6-dfc3e538dc62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the best model\n",
    "\n",
    "best_model = gbt_best.fit(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d8f68c08-16dc-49a7-9af8-8554944b81f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.6550680390386013\n",
      "Area under PR-curve: 0.7596454077166834\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.transform(features_test)\n",
    "payment_event_flag_preds = predictions.select('prediction', 'payment_event_flag')\n",
    "metrics = BinaryClassificationMetrics(\n",
    "    payment_event_flag_preds.rdd.map(\n",
    "        lambda lines: [float(x) for x in lines]\n",
    "    )\n",
    ")\n",
    "print('ROC AUC:', metrics.areaUnderROC)\n",
    "print('Area under PR-curve:', metrics.areaUnderPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "438d0412-c410-47cd-9200-3add15442945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.92      0.70      3945\n",
      "           1       0.86      0.39      0.53      4669\n",
      "\n",
      "    accuracy                           0.63      8614\n",
      "   macro avg       0.71      0.66      0.62      8614\n",
      "weighted avg       0.72      0.63      0.61      8614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predictions = model.transform(features_test)\n",
    "\n",
    "y_true = predictions.select('payment_event_flag').rdd.map(lambda x: x['payment_event_flag']).collect()\n",
    "y_pred = predictions.select('prediction').rdd.map(lambda x: x['prediction']).collect()\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f130c008-e040-418d-95cc-8e08a1c6d707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "predictions = best_model.transform(features_test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"payment_event_flag\")\n",
    "best_auc = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "df54e085-cb52-4842-bee4-e40b62e5178c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "y_true = predictions.select('payment_event_flag').rdd.map(lambda x: x['payment_event_flag']).collect()\n",
    "y_pred = predictions.select('prediction').rdd.map(lambda x: x['prediction']).collect()\n",
    "report = classification_report(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ae2a6ac8-c2d0-4083-877b-85cbc9f27d10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Calculate PR AUC\n",
    "payment_event_flag_preds = predictions.select('probability', 'payment_event_flag')\n",
    "metrics = BinaryClassificationMetrics(payment_event_flag_preds.rdd.map(lambda lp: (float(lp[0][1]), float(lp[1]))))\n",
    "pr_auc = metrics.areaUnderPR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "daeb27dc-72ba-467e-9dd6-8b83aa24fca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.6729618623605091\n",
      "PR AUC: 0.7630205704423072\n"
     ]
    }
   ],
   "source": [
    "print(f\"ROC AUC: {best_auc}\")\n",
    "print(f\"PR AUC: {pr_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8354817-efa6-475e-88c3-2cf389676223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log additional metrics to W&B\n",
    "wandb.log({\"best_model_classification_report\": wandb.Html(report)})\n",
    "wandb.log({\"best_model_roc_auc\": best_auc})\n",
    "wandb.log({\"best_model_pr_auc\": pr_auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0806685-723d-46b3-be33-87814f226d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()  # Finalize W&B run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db075ae0-d0ae-49ac-aa6b-34f00eda3c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
