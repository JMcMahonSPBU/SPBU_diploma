{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f188b1-1256-462a-a1d4-1de2ba4aac80",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark modelling - optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78ba91-a8ad-43c3-806e-5fc23c39a35b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Libraries and Spark setup\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2c2d3-3816-423f-b760-a70937e6b478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def access_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        access_data = json.load(file)\n",
    "    return access_data\n",
    "\n",
    "access_s3_data = access_data('.access_jhub_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f214c4d-2142-4557-814f-4fb22a0370d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "conf = SparkConf()\n",
    "conf.set('spark.master', 'local[5]')\n",
    "conf.set('spark.driver.memory', '32G')\n",
    "conf.set('spark.driver.maxResultSize', '8G')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.access.key', access_s3_data['aws_access_key_id'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.secret.key', access_s3_data['aws_secret_access_key'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.multipart.size', '104857600')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.block.size', '33554432')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.threads.max', '256')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 'http://storage.yandexcloud.net')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.aws.credentials.provider', \n",
    "                                     'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4f36b1-5adc-49c3-9e1d-1f1122d783d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd1e73-b51d-4d50-845a-374954d14228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This section defines variables for data processing and file paths, then performs several data processing stages:\n",
    "Variables:\n",
    "\n",
    "* `VER`: Version identifier.\n",
    "* `PROC_DS`, `PROC_LAGS`, `PROC_VECS`: Flags to control data processing stages.\n",
    "* `FRAC_0`: Fraction of negative examples to sample when processing lags.\n",
    "* `BUCKET`: S3 bucket name.\n",
    "* `files_path`, `files_mask`: Local paths and masks for raw data files.\n",
    "* `file_path_ds`, `file_path_lags`, `file_path_trn`, `file_path_tst`: S3 paths for different stages of processed data.\n",
    "\n",
    "\n",
    "• `VER = 'v2'`: This defines the version of your data processing pipeline. You might increment this version number when you make significant changes to the processing steps. V2 is just an example, you will choose your own version. \n",
    "• `BUCKET` = 'pvc-84ea79a0-dc20-4a2d-86ab-f83c1f8d4a7b': This specifies the name of the bucket where your data is stored (likely on a cloud storage platform like AWS S3 or Yandex Object Storage).\n",
    "\n",
    "**Flags:**\n",
    "\n",
    "• `PROC_DS` (False): This flag controls whether to process the raw dataset. If True, the code will read the raw CSV files, extract relevant columns, filter by date, and create the data_raw.parquet file.\n",
    "\n",
    "* Change to True: When you have new raw data or need to reprocess the existing raw data due to changes in the extraction logic.\n",
    "* Keep as False: When you already have a processed data_raw.parquet file and don't need to re-process it.\n",
    "\n",
    "• `PROC_LAGS` (False): This flag controls whether to process and create lag features. If True, the code will calculate lag features based on event history for different time windows and store them in the data_lags.parquet file.\n",
    "\n",
    "* Change to True: When you need to recalculate lag features, such as when you've changed the time window definitions or added new events.\n",
    "* Keep as False: When you already have the desired lag features in data_lags.parquet and don't need to recompute them.\n",
    "\n",
    "• `FRAC_0` (.001): This variable sets the sampling fraction for events with payment_event_flag = 0 when processing lags. This is used to reduce the size of the dataset for faster processing while maintaining a representative sample.\n",
    "\n",
    "* Adjust Value: You might change this value depending on the size of your dataset and the desired balance between processing time and data representativeness.\n",
    "\n",
    "• `PROC_VECS` (True): This flag controls whether to vectorize the data using TF-IDF. If True, the code will transform the lag features into numerical vectors using TF-IDF and store them in data_vec_train.parquet and data_vec_test.parquet files.\n",
    "\n",
    "* Change to False: If you want to experiment with other vectorization methods or use the data in its raw form.\n",
    "* Keep as True: When TF-IDF vectorization is the desired approach for your modeling tasks.\n",
    "\n",
    "**File Paths:**\n",
    "\n",
    "• `files_path`, `files_mask`: These define the location and pattern of the raw data files.\n",
    "• `file_path_ds`, `file_path_lags`, `file_path_trn`, `file_path_tst`: These specify the storage locations for the processed datasets at different stages of the pipeline.\n",
    "\n",
    "By understanding these flags and variables, you can control which parts of the data processing pipeline are executed, allowing for efficient experimentation and iteration.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6671005-60e1-471a-9510-60ae3dad10b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VER = 'vPP2'\n",
    "PROC_DS = True\n",
    "PROC_LAGS = True\n",
    "FRAC_0 = .002  # used only if `PROC_LAGS = True`\n",
    "PROC_VECS = True\n",
    "BUCKET = 'pvc-84ea79a0-dc20-4a2d-86ab-f83c1f8d4a7b'\n",
    "PRJ_PATH = '/home/jovyan/__RAYPFP'\n",
    "\n",
    "files_path = 'data/events'\n",
    "files_mask = f'{files_path}/data_202*-*-*.csv'\n",
    "\n",
    "file_path_ds = f's3a://{BUCKET}/work/{VER}/data_raw.parquet'\n",
    "file_path_lags = f's3a://{BUCKET}/work/{VER}/data_lags.parquet'\n",
    "file_path_trn = f's3a://{BUCKET}/work/{VER}/data_vec_train.parquet'\n",
    "file_path_tst = f's3a://{BUCKET}/work/{VER}/data_vec_test.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70582610-f1b6-4083-be79-655baf34f001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_parquet(path):\n",
    "    cmd = path.replace(\n",
    "        f's3a://{BUCKET}',\n",
    "        f'rm -rf {PRJ_PATH}'\n",
    "    )\n",
    "    !{cmd}\n",
    "    return f'command to run: {cmd}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ab6e0-27fd-4f4a-9fa2-82bd9ed371d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1. Load or preprocess data - `raw` stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4522409f-f0fd-49b5-bc60-69fad15b05f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "flag_min_datetime = datetime.datetime(2024, 4, 30, 0, 0, 0)\n",
    "flag_max_datetime = datetime.datetime(2024, 5, 21, 23, 59, 59)\n",
    "print(flag_min_datetime, flag_max_datetime)\n",
    "\n",
    "if PROC_DS:\n",
    "    sdf = spark.read.option('escape','\"').csv(f's3a://{BUCKET}/{files_mask}', header=True)\n",
    "    sdf = sdf.withColumn('event_datetime', F.to_timestamp(\"event_datetime\"))\n",
    "    sdf = sdf.withColumn(\n",
    "        'payment_event_flag', \n",
    "        (\n",
    "            (F.col('event_name').like('%Мои штрафы/Оплата/Завершили оплату%') | \n",
    "            F.col('event_name').like('%Мои штрафы/Оплата/Платёж принят%')) &\n",
    "            F.col('event_datetime').between(flag_min_datetime, flag_max_datetime)\n",
    "        ).cast(\"int\")\n",
    "    )\n",
    "    sdf = sdf.select(\n",
    "        'profile_id',\n",
    "        'event_datetime',\n",
    "        'payment_event_flag',\n",
    "        'event_name'\n",
    "    )\n",
    "    cmd = file_path_ds.replace(\n",
    "        f's3a://{BUCKET}',\n",
    "        f'ls -la {PRJ_PATH}'\n",
    "    )\n",
    "    clean_parquet(file_path_ds)\n",
    "    sdf.repartition(1).write.parquet(file_path_ds)\n",
    "    sdf.unpersist()\n",
    "sdf = spark.read.parquet(file_path_ds)\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a3863-f393-40e4-8f93-d3711fdd077e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#checking that all profile_id columns have full data \n",
    "\n",
    "sdf.filter(F.col('profile_id').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42651a87-e278-4efb-9495-7fcddaaebd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2. Load or preprocess data - `lags` stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ce99e-a387-498c-83b7-ed30e8845234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataset_lags(sdf, shift=0):\n",
    "    hour = 60 * 60\n",
    "    day = 24 * 60 * 60\n",
    "\n",
    "    w_10min_to_1week = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-7 * day + shift, -10 * 60 + shift))\n",
    "    w_1week_to_2weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-14 * day + shift, -7 * day + shift))\n",
    "    w_2weeks_to_3weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-21 * day + shift, -14 * day + shift))\n",
    "    w_3weeks_to_4weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-28 * day + shift, -21 * day + shift))\n",
    "    w_4weeks_to_5weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-35 * day + shift, -28 * day + shift))\n",
    "    w_5weeks_to_6weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-42 * day + shift, -35 * day + shift))\n",
    "    w_6weeks_to_7weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-49 * day + shift, -42 * day + shift))\n",
    "    w_7weeks_to_8weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-56 * day + shift, -49 * day + shift))\n",
    "    w_8weeks_to_9weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-63 * day + shift, -56 * day + shift))\n",
    "    w_9weeks_to_10weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-70 * day + shift, -63 * day + shift))\n",
    "    w_10weeks_to_11weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-77 * day + shift, -70 * day + shift))\n",
    "    w_11weeks_to_12weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-84 * day + shift, -77 * day + shift))\n",
    "    w_12weeks_to_13weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-91 * day + shift, -84 * day + shift))\n",
    "    w_13weeks_to_14weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-98 * day + shift, -91 * day + shift))   \n",
    "    w_14weeks_to_15weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-105 * day + shift, -98 * day + shift))\n",
    "    w_15weeks_to_16weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-112 * day + shift, -105 * day + shift))\n",
    "    w_16weeks_to_17weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-119 * day + shift, -112 * day + shift))\n",
    "    w_17weeks_to_18weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-126 * day + shift, -119 * day + shift))\n",
    "    w_18weeks_to_19weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-133 * day + shift, -126 * day + shift))\n",
    "    w_19weeks_to_20weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-140 * day + shift, -133 * day + shift))\n",
    "    w_20weeks_to_21weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-147 * day + shift, -140 * day + shift))\n",
    "    w_21weeks_to_22weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-154 * day + shift, -147 * day + shift))\n",
    "    w_22weeks_to_23weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-161 * day + shift, -154 * day + shift))\n",
    "    w_23weeks_to_24weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-168 * day + shift, -161 * day + shift))\n",
    "    w_24weeks_to_25weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-175 * day + shift, -168 * day + shift))\n",
    "    w_25weeks_to_26weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-182 * day + shift, -175 * day + shift)) \n",
    "    w_26weeks_to_27weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-189 * day + shift, -182 * day + shift)) \n",
    "    \n",
    "    return (\n",
    "        sdf\n",
    "            #.withColumn('lag_10min_to_1week', F.collect_list('event_name').over(w_10min_to_1week))\n",
    "            .withColumn('lag_1week_to_2weeks', F.collect_list('event_name').over(w_1week_to_2weeks))\n",
    "            .withColumn('lag_2weeks_to_3weeks', F.collect_list('event_name').over(w_2weeks_to_3weeks))\n",
    "            .withColumn('lag_3weeks_to_4weeks', F.collect_list('event_name').over(w_3weeks_to_4weeks))\n",
    "            .withColumn('lag_4weeks_to_5weeks', F.collect_list('event_name').over(w_4weeks_to_5weeks))\n",
    "            .withColumn('lag_5weeks_to_6weeks', F.collect_list('event_name').over(w_5weeks_to_6weeks))\n",
    "            .withColumn('lag_6weeks_to_7weeks', F.collect_list('event_name').over(w_6weeks_to_7weeks))\n",
    "            .withColumn('lag_7weeks_to_8weeks', F.collect_list('event_name').over(w_7weeks_to_8weeks))\n",
    "            .withColumn('lag_8weeks_to_9weeks', F.collect_list('event_name').over(w_8weeks_to_9weeks))\n",
    "            .withColumn('lag_9weeks_to_10weeks', F.collect_list('event_name').over(w_9weeks_to_10weeks))\n",
    "            .withColumn('lag_10weeks_to_11weeks', F.collect_list('event_name').over(w_10weeks_to_11weeks))\n",
    "            .withColumn('lag_11weeks_to_12weeks', F.collect_list('event_name').over(w_11weeks_to_12weeks))\n",
    "            .withColumn('lag_12weeks_to_13weeks', F.collect_list('event_name').over(w_12weeks_to_13weeks))\n",
    "            .withColumn('lag_13weeks_to_14weeks', F.collect_list('event_name').over(w_13weeks_to_14weeks))\n",
    "            .withColumn('lag_14weeks_to_15weeks', F.collect_list('event_name').over(w_14weeks_to_15weeks))\n",
    "            .withColumn('lag_15weeks_to_16weeks', F.collect_list('event_name').over(w_15weeks_to_16weeks))\n",
    "            .withColumn('lag_16weeks_to_17weeks', F.collect_list('event_name').over(w_16weeks_to_17weeks))\n",
    "            .withColumn('lag_17weeks_to_18weeks', F.collect_list('event_name').over(w_17weeks_to_18weeks))\n",
    "            .withColumn('lag_18weeks_to_19weeks', F.collect_list('event_name').over(w_18weeks_to_19weeks))\n",
    "            .withColumn('lag_19weeks_to_20weeks', F.collect_list('event_name').over(w_19weeks_to_20weeks))\n",
    "            .withColumn('lag_20weeks_to_21weeks', F.collect_list('event_name').over(w_20weeks_to_21weeks))\n",
    "            .withColumn('lag_21weeks_to_22weeks', F.collect_list('event_name').over(w_21weeks_to_22weeks))\n",
    "            .withColumn('lag_22weeks_to_23weeks', F.collect_list('event_name').over(w_22weeks_to_23weeks))\n",
    "            .withColumn('lag_23weeks_to_24weeks', F.collect_list('event_name').over(w_23weeks_to_24weeks))\n",
    "            .withColumn('lag_24weeks_to_25weeks', F.collect_list('event_name').over(w_24weeks_to_25weeks))\n",
    "            .withColumn('lag_25weeks_to_26weeks', F.collect_list('event_name').over(w_25weeks_to_26weeks))\n",
    "            .withColumn('lag_26weeks_to_27weeks', F.collect_list('event_name').over(w_26weeks_to_27weeks))\n",
    "            .select(\n",
    "                'profile_id',\n",
    "                'event_datetime',\n",
    "                'payment_event_flag',\n",
    "                'event_name',\n",
    "                #'lag_10min_to_1week',\n",
    "                'lag_1week_to_2weeks',\n",
    "                'lag_2weeks_to_3weeks',\n",
    "                'lag_3weeks_to_4weeks',\n",
    "                'lag_4weeks_to_5weeks',\n",
    "                'lag_5weeks_to_6weeks',\n",
    "                'lag_6weeks_to_7weeks',\n",
    "                'lag_7weeks_to_8weeks',\n",
    "                'lag_8weeks_to_9weeks',\n",
    "                'lag_9weeks_to_10weeks',\n",
    "                'lag_10weeks_to_11weeks',\n",
    "                'lag_11weeks_to_12weeks',\n",
    "                'lag_12weeks_to_13weeks',\n",
    "                'lag_13weeks_to_14weeks',\n",
    "                'lag_14weeks_to_15weeks',\n",
    "                'lag_15weeks_to_16weeks',\n",
    "                'lag_16weeks_to_17weeks',\n",
    "                'lag_17weeks_to_18weeks',\n",
    "                'lag_18weeks_to_19weeks',\n",
    "                'lag_19weeks_to_20weeks',\n",
    "                'lag_20weeks_to_21weeks',\n",
    "                'lag_21weeks_to_22weeks',\n",
    "                'lag_22weeks_to_23weeks',\n",
    "                'lag_23weeks_to_24weeks',\n",
    "                'lag_24weeks_to_25weeks',\n",
    "                'lag_25weeks_to_26weeks',\n",
    "                'lag_26weeks_to_27weeks'\n",
    "            )\n",
    "        .orderBy(F.col('event_datetime'), ascending=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a89843d-9c63-48b4-808d-82c5fa084102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This section defines a list of lag features to be used.\n",
    "\n",
    "The datasets_tfidf function performs `TF-IDF` vectorization on the lag features for both training and test datasets.\n",
    "\n",
    "It uses `HashingTF` to convert lists of event names into numerical feature vectors and `IDF` to rescale the features based on their document frequency.\n",
    "\n",
    "The function also creates a dictionary mapping feature indices to the corresponding event names.\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096c4e5-1885-4cd3-adde-38d6e407a93c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if PROC_LAGS:\n",
    "    sdf = sdf.sampleBy(\n",
    "        'payment_event_flag', \n",
    "        fractions={0: FRAC_0, 1: 1}, \n",
    "        seed=2023\n",
    "    )\n",
    "    sdf = dataset_lags(sdf)\n",
    "    dates  = (flag_min_datetime, flag_max_datetime)\n",
    "    sdf = sdf.filter(sdf.event_datetime.between(*dates))\n",
    "    sdf = sdf.filter(\n",
    "        #(F.size('lag_10min_to_1week')      > 0) |\n",
    "        (F.size('lag_1week_to_2weeks')     > 0) |\n",
    "        (F.size('lag_2weeks_to_3weeks')    > 0) |\n",
    "        (F.size('lag_3weeks_to_4weeks')    > 0) |\n",
    "        (F.size('lag_4weeks_to_5weeks')    > 0) |\n",
    "        (F.size('lag_5weeks_to_6weeks')    > 0) |\n",
    "        (F.size('lag_6weeks_to_7weeks')    > 0) |\n",
    "        (F.size('lag_7weeks_to_8weeks')    > 0) |\n",
    "        (F.size('lag_8weeks_to_9weeks')    > 0) |\n",
    "        (F.size('lag_9weeks_to_10weeks')   > 0) |\n",
    "        (F.size('lag_10weeks_to_11weeks')  > 0) |\n",
    "        (F.size('lag_11weeks_to_12weeks')  > 0) |\n",
    "        (F.size('lag_12weeks_to_13weeks')  > 0) |\n",
    "        (F.size('lag_13weeks_to_14weeks')  > 0) |\n",
    "        (F.size('lag_14weeks_to_15weeks')  > 0) |\n",
    "        (F.size('lag_15weeks_to_16weeks')  > 0) |\n",
    "        (F.size('lag_16weeks_to_17weeks')  > 0) |\n",
    "        (F.size('lag_17weeks_to_18weeks')  > 0) |\n",
    "        (F.size('lag_18weeks_to_19weeks')  > 0) |\n",
    "        (F.size('lag_19weeks_to_20weeks')  > 0) |\n",
    "        (F.size('lag_20weeks_to_21weeks')  > 0) |\n",
    "        (F.size('lag_21weeks_to_22weeks')  > 0) |\n",
    "        (F.size('lag_22weeks_to_23weeks')  > 0) |\n",
    "        (F.size('lag_23weeks_to_24weeks')  > 0) |\n",
    "        (F.size('lag_24weeks_to_25weeks')  > 0) |\n",
    "        (F.size('lag_25weeks_to_26weeks')  > 0) |\n",
    "        (F.size('lag_26weeks_to_27weeks')  > 0)\n",
    "    )\n",
    "    clean_parquet(file_path_lags)\n",
    "    sdf.repartition(8).write.parquet(file_path_lags)\n",
    "    sdf.unpersist()\n",
    "sdf = spark.read.parquet(file_path_lags)\n",
    "sdf.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de83425-0fbd-46bc-978a-cfad7ce0c311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stratified_split(sdf, frac, label, seed=2023):\n",
    "    zeros = sdf.filter(sdf[label] == 0)\n",
    "    ones = sdf.filter(sdf[label] == 1)\n",
    "    train_, test_ = zeros.randomSplit([1 - frac, frac], seed=seed)\n",
    "    train, test = ones.randomSplit([1 - frac, frac], seed=seed)\n",
    "    train = train.union(train_)\n",
    "    test = test.union(test_)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94f255-3f3a-453b-aad9-8772ce8f5a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf_train, sdf_test = stratified_split(\n",
    "    sdf,\n",
    "    frac=.2,\n",
    "    label='payment_event_flag',\n",
    "    seed=2023\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e35b0-bd8f-4318-a973-155b4fd72a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf_train.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c012b-f4ec-46bd-a0c3-7270d31d6b14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf_test.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f90d9-6705-4a5d-a948-3abd1954fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.3. Load or preprocess data - `vectorize` stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8a249-c54f-493f-9679-baaa4e3dff6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lags = [\n",
    "    #'lag_10min_to_1week',\n",
    "    'lag_1week_to_2weeks',\n",
    "    'lag_2weeks_to_3weeks',\n",
    "    'lag_3weeks_to_4weeks',\n",
    "    'lag_4weeks_to_5weeks',\n",
    "    'lag_5weeks_to_6weeks',\n",
    "    'lag_6weeks_to_7weeks',\n",
    "    'lag_7weeks_to_8weeks',\n",
    "    'lag_8weeks_to_9weeks',\n",
    "    'lag_9weeks_to_10weeks',\n",
    "    'lag_10weeks_to_11weeks',\n",
    "    'lag_11weeks_to_12weeks',\n",
    "    'lag_12weeks_to_13weeks',\n",
    "    'lag_13weeks_to_14weeks',\n",
    "    'lag_14weeks_to_15weeks',\n",
    "    'lag_15weeks_to_16weeks',\n",
    "    'lag_16weeks_to_17weeks',\n",
    "    'lag_17weeks_to_18weeks',\n",
    "    'lag_18weeks_to_19weeks',\n",
    "    'lag_19weeks_to_20weeks',\n",
    "    'lag_20weeks_to_21weeks',\n",
    "    'lag_21weeks_to_22weeks',\n",
    "    'lag_22weeks_to_23weeks',\n",
    "    'lag_23weeks_to_24weeks',\n",
    "    'lag_24weeks_to_25weeks',\n",
    "    'lag_25weeks_to_26weeks',\n",
    "    'lag_26weeks_to_27weeks'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fccc4-837d-495b-8d14-38c3ce8d69c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datasets_vecorized(sdf_train, sdf_test, lags, vec_size=10):\n",
    "    vectorizers = []\n",
    "    for lag in tqdm(lags):\n",
    "        word2Vec = Word2Vec(\n",
    "            vectorSize=vec_size,\n",
    "            minCount=0,\n",
    "            inputCol=lag,\n",
    "            outputCol=lag + '_vec'\n",
    "        )\n",
    "        vectorizer = word2Vec.fit(sdf_train)\n",
    "        sdf_train = vectorizer.transform(sdf_train)\n",
    "        sdf_test = vectorizer.transform(sdf_test)\n",
    "        vectorizers.append(vectorizer)\n",
    "    return sdf_train, sdf_test, vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5d579-25ef-470a-9f88-20112894f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Breakdown of if PROC_VECS code: \n",
    "\n",
    "\n",
    "**Conditional Execution:**\n",
    "\n",
    "• if PROC_VECS:: The code within this block is executed only if the PROC_VECS flag is set to True. This flag controls whether TF-IDF vectorization is performed on the data.\n",
    "\n",
    "**TF-IDF Vectorization:**\n",
    "\n",
    "`sdf_train, sdf_test, vectorizers = datasets_tfidf(...)`: This line calls the datasets_tfidf function, which performs TF-IDF vectorization on the lag features present in the sdf_train and sdf_test DataFrames.\n",
    "* The lags argument provides the list of lag feature column names to be vectorized.\n",
    "* The vec_size=10 argument specifies the desired dimensionality (number of features) of the resulting TF-IDF vectors.\n",
    "\n",
    "**The function returns three values:**\n",
    "\n",
    "* `sdf_train`: The training DataFrame with the added TF-IDF vector columns.\n",
    "* `sdf_test`: The test DataFrame with the added TF-IDF vector columns.\n",
    "* `vectorizers`: A list of fitted TF-IDF vectorizer models (one for each lag feature).\n",
    "\n",
    "**Cleaning and Saving Parquet Files:**\n",
    "\n",
    "`clean_parquet(file_path_trn)`: This line calls a function (not shown) to clean up any existing Parquet files at the specified path (file_path_trn) before saving the new data.\n",
    "\n",
    "`sdf_train.repartition(8).write.parquet(file_path_trn)`: The training DataFrame (sdf_train) is repartitioned into 8 partitions for optimized writing.\n",
    "\n",
    "* The write.parquet method saves the DataFrame as a Parquet file at the specified path (file_path_trn).\n",
    "* The same process is repeated for the test DataFrame (sdf_test) using file_path_tst.\n",
    "\n",
    "**Unpersisting DataFrames:**\n",
    "\n",
    "* `sdf_train.unpersist(), sdf_test.unpersist()`: These lines remove the DataFrames from Spark's memory. Since the data has been saved to disk, it can be reloaded later if needed, freeing up memory for subsequent processing.\n",
    "\n",
    "**Reloading DataFrames (if necessary):**\n",
    "\n",
    "* `sdf_train = spark.read.parquet(file_path_trn)`: This line reloads the training data from the saved Parquet file if it's not already in memory.\n",
    "\n",
    "The same is done for the test data using file_path_tst.\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84729a95-0c69-43fa-bc61-a0955645720f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if PROC_VECS:\n",
    "    sdf_train, sdf_test, vectorizers = datasets_vecorized(\n",
    "        sdf_train,\n",
    "        sdf_test,\n",
    "        lags,\n",
    "        vec_size=10\n",
    "    )\n",
    "    clean_parquet(file_path_trn)\n",
    "    sdf_train.repartition(8).write.parquet(file_path_trn)\n",
    "    clean_parquet(file_path_tst)\n",
    "    sdf_test.repartition(8).write.parquet(file_path_tst)\n",
    "    sdf_train.unpersist()\n",
    "    sdf_test.unpersist()\n",
    "sdf_train = spark.read.parquet(file_path_trn)\n",
    "sdf_test = spark.read.parquet(file_path_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b2590-13e2-4cce-b5b7-6232a21fe981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6b9e0-0e7b-4459-ab37-e970f26fb3d6",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd83edd-91ab-4364-b7a3-64abcd9def7f",
   "metadata": {},
   "source": [
    "### 3.1. Features assembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662437f-0848-4e87-83f6-28edba536c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def features_assembled(sdf, feats):\n",
    "    cols_to_model = [x + '_vec' for x in feats]\n",
    "    cols_to_model.extend(['payment_event_flag'])\n",
    "    print('columns to model:', cols_to_model)\n",
    "    vecAssembler = VectorAssembler(\n",
    "        inputCols=[c for c in cols_to_model if c != 'payment_event_flag'], \n",
    "        outputCol='features'\n",
    "    )\n",
    "    features = sdf.select(cols_to_model)\n",
    "    features_vec = vecAssembler.transform(features)\n",
    "    features_data = features_vec.select('payment_event_flag', 'features')\n",
    "    return features_data\n",
    "\n",
    "\n",
    "def upsampled(sdf, label, upsample='max'):\n",
    "    zeros = sdf.filter(sdf[label] == 0)\n",
    "    ones = sdf.filter(sdf[label] == 1)\n",
    "    res = zeros.union(ones)\n",
    "    if upsample == 'max':\n",
    "        up_count = int(zeros.count() / ones.count())\n",
    "        for _ in range(up_count - 1):\n",
    "            res = res.union(ones)\n",
    "    else:\n",
    "        for _ in range(upsample - 1):\n",
    "            res = res.union(ones)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bfde6-70c1-4272-8bc6-0e317a673776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "UPSAMPLE = None  # can be None or 'max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefefcd5-8048-4120-93fa-9687290ada7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats = [\n",
    "#    'lag_10min_to_1week',\n",
    "    'lag_1week_to_2weeks',\n",
    "    'lag_2weeks_to_3weeks',\n",
    "    'lag_3weeks_to_4weeks',\n",
    "    'lag_4weeks_to_5weeks',\n",
    "    'lag_5weeks_to_6weeks',\n",
    "    'lag_6weeks_to_7weeks',\n",
    "    'lag_7weeks_to_8weeks',\n",
    "    'lag_8weeks_to_9weeks',\n",
    "    'lag_9weeks_to_10weeks',\n",
    "    'lag_10weeks_to_11weeks',\n",
    "    'lag_11weeks_to_12weeks',\n",
    "    'lag_12weeks_to_13weeks',\n",
    "    'lag_13weeks_to_14weeks',\n",
    "    'lag_14weeks_to_15weeks',\n",
    "    'lag_15weeks_to_16weeks',\n",
    "    'lag_16weeks_to_17weeks',\n",
    "    'lag_17weeks_to_18weeks',\n",
    "    'lag_18weeks_to_19weeks',\n",
    "    'lag_19weeks_to_20weeks',\n",
    "    'lag_20weeks_to_21weeks',\n",
    "    'lag_21weeks_to_22weeks',\n",
    "    'lag_22weeks_to_23weeks',\n",
    "    'lag_23weeks_to_24weeks',\n",
    "    'lag_24weeks_to_25weeks',\n",
    "    'lag_25weeks_to_26weeks',\n",
    "    'lag_26weeks_to_27weeks'\n",
    "]\n",
    "features_train = features_assembled(sdf_train, feats=feats)\n",
    "features_test = features_assembled(sdf_test, feats=feats)\n",
    "if UPSAMPLE:\n",
    "    features_train = upsampled(\n",
    "        features_train,\n",
    "        label='payment_event_flag',\n",
    "        upsample=UPSAMPLE\n",
    "    )\n",
    "    # Use to upsample test set\n",
    "    features_test = upsampled(\n",
    "        features_test,\n",
    "        label='payment_event_flag',\n",
    "        upsample=UPSAMPLE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d38ed-6ced-4935-9762-3d0941a29e58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_train.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2dfc9-ed4f-4139-9074-c986b12cdc7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_test.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f649e-0b02-4d09-83ce-7ac6e001a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2. Training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9167c4-b21e-4ccc-8004-570635f27ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    labelCol='payment_event_flag',\n",
    "    featuresCol='features',\n",
    "    numTrees=100,\n",
    "    maxDepth=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f90b6a-6bd3-469f-9052-1c1e1a52fbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = rf.fit(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f761a-bc67-45e9-a6d4-cce0e0771604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(features_test)\n",
    "payment_event_flag_preds = predictions.select('prediction', 'payment_event_flag')\n",
    "metrics = BinaryClassificationMetrics(\n",
    "    payment_event_flag_preds.rdd.map(\n",
    "        lambda lines: [float(x) for x in lines]\n",
    "    )\n",
    ")\n",
    "print('ROC AUC:', metrics.areaUnderROC)\n",
    "print('Area under PR-curve:', metrics.areaUnderPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57497976-416b-4644-8dc5-ee6010a6c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.3. Future look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714aea4-840b-40b5-8370-1b0fa7abca81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf_pred = spark.read.parquet(file_path_ds)\n",
    "sdf_pred.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78669fa1-926a-43a0-83de-4621a2dc1b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SHIFT = 7 * 24 * 60 * 60  # 7 days ahead\n",
    "\n",
    "sdf_pred = sdf_pred.sample(fraction=.0001)\n",
    "sdf_pred = dataset_lags(sdf_pred, shift=SHIFT)\n",
    "sdf = sdf.filter(\n",
    "       #(F.size('lag_10min_to_1week')      > 0) |\n",
    "        (F.size('lag_1week_to_2weeks')     > 0) |\n",
    "        (F.size('lag_2weeks_to_3weeks')    > 0) |\n",
    "        (F.size('lag_3weeks_to_4weeks')    > 0) |\n",
    "        (F.size('lag_4weeks_to_5weeks')    > 0) |\n",
    "        (F.size('lag_5weeks_to_6weeks')    > 0) |\n",
    "        (F.size('lag_6weeks_to_7weeks')    > 0) |\n",
    "        (F.size('lag_7weeks_to_8weeks')    > 0) |\n",
    "        (F.size('lag_8weeks_to_9weeks')    > 0) |\n",
    "        (F.size('lag_9weeks_to_10weeks')   > 0) |\n",
    "        (F.size('lag_10weeks_to_11weeks')  > 0) |\n",
    "        (F.size('lag_11weeks_to_12weeks')  > 0) |\n",
    "        (F.size('lag_12weeks_to_13weeks')  > 0) |\n",
    "        (F.size('lag_13weeks_to_14weeks')  > 0) |\n",
    "        (F.size('lag_14weeks_to_15weeks')  > 0) |\n",
    "        (F.size('lag_15weeks_to_16weeks')  > 0) |\n",
    "        (F.size('lag_16weeks_to_17weeks')  > 0) |\n",
    "        (F.size('lag_17weeks_to_18weeks')  > 0) |\n",
    "        (F.size('lag_18weeks_to_19weeks')  > 0) |\n",
    "        (F.size('lag_19weeks_to_20weeks')  > 0) |\n",
    "        (F.size('lag_20weeks_to_21weeks')  > 0) |\n",
    "        (F.size('lag_21weeks_to_22weeks')  > 0) |\n",
    "        (F.size('lag_22weeks_to_23weeks')  > 0) |\n",
    "        (F.size('lag_23weeks_to_24weeks')  > 0) |\n",
    "        (F.size('lag_24weeks_to_25weeks')  > 0) |\n",
    "        (F.size('lag_25weeks_to_26weeks')  > 0) |\n",
    "        (F.size('lag_26weeks_to_27weeks')  > 0)\n",
    ")\n",
    "sdf_pred.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1bd830-4ead-4da7-aa76-859851cd32d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de6eb7-f7be-43c0-83d4-804a4ba596f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, lag in enumerate(lags):\n",
    "    sdf_pred = vectorizers[i].transform(sdf_pred)\n",
    "    print(lag, '-> done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec04052-748d-45df-9a97-31bafcff6e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_pred = features_assembled(sdf_pred, feats=feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be7484-19d8-4ae9-88bd-56c1c30f6483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_future = model.transform(features_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1309a-fab6-4cf8-8807-5d41862cac6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_pred = sdf_pred.select(sdf_pred.profile_id).toPandas()\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3d324-494b-4f86-a80c-c3a4c239c610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_predictions_future = predictions_future.withColumn(\n",
    "    'tmp',\n",
    "    vector_to_array('probability')\n",
    ").select(\n",
    "    F.col('tmp')[1].alias('prob_next7days')\n",
    ").toPandas()\n",
    "df_predictions_future.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b186876a-3a6f-4428-bb63-f004fa435411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\"\"\"\n",
    "You should change the days=6 to match how many days ahead you are making the predictions. \n",
    "You will find this in your SHIFT variable. \n",
    "\n",
    "Your starting date (from which the X days will be added, is your datetime in the variable: \n",
    "\n",
    "'flag_max_datetime'\n",
    "\n",
    "For example, flag_max_datetime = datetime.datetime(2024, 5, 21, 23, 59, 59) with the future_datetime added will give predictions for the 28th May 2024. \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Get the current datetime\n",
    "pred_max_datetime = flag_max_datetime\n",
    "\n",
    "# Add 7 days to the current datetime\n",
    "future_datetime = pred_max_datetime + datetime.timedelta(days=6)\n",
    "\n",
    "# Format the future datetime as \"YYYYMMDD_HHMMSS\"\n",
    "future_datetime_str = future_datetime.strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cac821-fbaf-4d3f-a2de-d14df3741e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path_preds = f'{PRJ_PATH}/work/{VER}/preds_{future_datetime_str}.csv'\n",
    "df_pred.join(df_predictions_future).to_csv(file_path_preds, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
