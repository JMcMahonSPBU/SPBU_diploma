{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f188b1-1256-462a-a1d4-1de2ba4aac80",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark modelling - optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed9ee60-5a17-4547-bcab-959751b81a99",
   "metadata": {},
   "source": [
    "## 1. Libraries and Spark setup\n",
    "\n",
    "\n",
    "This section imports necessary libraries and sets up the Spark environment:\n",
    "\n",
    "Libraries:\n",
    "* `o`, `sys`, `json`, `datetime`, `numpy`, `pandas`, `tqdm`, `matplotlib.pyplot`: General purpose libraries for file system access, system functionalities, JSON handling, date/time manipulation, numerical computation, data manipulation, progress bars, and plotting.\n",
    "\n",
    "pyspark libraries:\n",
    "* `SparkContext`, `SparkConf`: Core Spark functionalities for setting up the Spark context and configuration.\n",
    "* `SparkSession`: Entry point for interacting with Spark SQL.\n",
    "* `functions as F`: Provides various Spark SQL functions for data manipulation.\n",
    "* `types`: Defines data types for Spark DataFrames.\n",
    "* `Window`: Used for window functions in Spark SQL.\n",
    "* `ml.feature`: Provides feature engineering and transformation tools like `Word2Vec`, `Imputer`, `OneHotEncoder`, `StringIndexer`, `VectorAssembler`.\n",
    "* `ml.classification`: Provides classification algorithms like `LogisticRegression` and `RandomForestClassifier`.\n",
    "* `ml.evaluation`: Provides evaluation metrics like `BinaryClassificationEvaluator` and `BinaryClassificationMetrics`.\n",
    "* `ml.tuning`: Provides tools for hyperparameter tuning like `CrossValidator` and `ParamGridBuilder`.\n",
    "\n",
    "Spark Configuration:\n",
    "* `SparkConf`: Sets configuration parameters for the Spark application.\n",
    "* `spark.master`: Specifies the cluster manager; local[*] indicates using all available cores on the local machine.\n",
    "* `spark.driver.memory`, `spark.driver.maxResultSize`: Allocates memory for the driver process.\n",
    "* `SparkContext`, `SparkSession`: Creates the Spark context and session based on the configuration.\n",
    "\n",
    "Accessing Data:\n",
    "* `access_data`: Function to load JSON data from a local file.\n",
    "* `access_s3_data`: Loads AWS credentials from a local JSON file.\n",
    "\n",
    "Spark configuration is further set to access data from Yandex Cloud Storage (S3-compatible) using the loaded credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a78ba91-a8ad-43c3-806e-5fc23c39a35b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Create filenames \n",
    "\n",
    "import datetime\n",
    "import uuid\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d90ad87-b0ec-418e-bfd5-b50aa0d9281c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (0.16.6)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from wandb) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (1.45.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23ca08e-c132-426e-be2b-acaee090a674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# W&B logging \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff1426-b038-4c06-a75d-d03a7f7f4383",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Insert the name of my notebook here below </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803337b4-f484-46e1-9a9d-da8099c7b0c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set name of notebook\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"full_pipeline_with_logging_hyperopt_PK.ipynb\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897724d7-5ef5-4ff1-9b8e-a33ae8457ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pyspark general \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Pyspark pre-processing \n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Pyspark vectorization\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Pyspark models \n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "# Pyspark classifiers \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Pyspark cross-validation\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Pyspark reporting \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Pyspark other\n",
    "from pyspark.ml.functions import vector_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1a5c85-715f-4d1c-b23b-c30165e31c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperopt in /opt/conda/lib/python3.9/site-packages (0.2.7)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from hyperopt) (4.62.3)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.9/site-packages (from hyperopt) (2.0.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from hyperopt) (1.8.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.9/site-packages (from hyperopt) (1.0.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.9/site-packages (from hyperopt) (2.6.3)\n",
      "Requirement already satisfied: py4j in /opt/conda/lib/python3.9/site-packages (from hyperopt) (0.10.9.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from hyperopt) (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab30d97-b956-49a8-866c-64c943e7bb3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperopt related \n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll.base import scope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e146c-d9b3-429c-809d-8626be9a19db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color: red;\">Get your w&b API for the next section</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d7f747f-d28b-4b5a-a226-d0e2a344ea28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkapetan\u001b[0m (\u001b[33mgsom-diploma-jap\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights and biases login \n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f214c4d-2142-4557-814f-4fb22a0370d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: /user/st095435/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/user/st095435/proxy/4040/jobs/\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0ba9122cd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set('spark.master', 'local[*]')\n",
    "conf.set('spark.driver.memory', '32G')\n",
    "conf.set('spark.driver.maxResultSize', '8G')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd2c2d3-3816-423f-b760-a70937e6b478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def access_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        access_data = json.load(file)\n",
    "    return access_data\n",
    "\n",
    "access_s3_data = access_data('.access_jhub_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05eb3b93-0d2b-467f-9ef8-20dcadc89256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set('fs.s3a.access.key', access_s3_data['aws_access_key_id'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.secret.key', access_s3_data['aws_secret_access_key'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.impl','org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.multipart.size', '104857600')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.block.size', '33554432')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.threads.max', '256')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 'http://storage.yandexcloud.net')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.aws.credentials.provider', \n",
    "                                     'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8ba75-8dcf-418c-9019-dae245606403",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Dataset\n",
    "\n",
    "\n",
    "This section defines variables for data processing and file paths, then performs several data processing stages:\n",
    "Variables:\n",
    "\n",
    "* `VER`: Version identifier.\n",
    "* `PROC_DS`, `PROC_LAGS`, `PROC_VECS`: Flags to control data processing stages.\n",
    "* `FRAC_0`: Fraction of negative examples to sample when processing lags.\n",
    "* `BUCKET`: S3 bucket name.\n",
    "* `files_path`, `files_mask`: Local paths and masks for raw data files.\n",
    "* `file_path_ds`, `file_path_lags`, `file_path_trn`, `file_path_tst`: S3 paths for different stages of processed data.\n",
    "\n",
    "\n",
    "• `VER = 'v2'`: This defines the version of your data processing pipeline. You might increment this version number when you make significant changes to the processing steps.\n",
    "• `BUCKET` = 'pvc-84ea79a0-dc20-4a2d-86ab-f83c1f8d4a7b': This specifies the name of the bucket where your data is stored (likely on a cloud storage platform like AWS S3 or Yandex Object Storage).\n",
    "\n",
    "**Flags:**\n",
    "\n",
    "• `PROC_DS` (False): This flag controls whether to process the raw dataset. If True, the code will read the raw CSV files, extract relevant columns, filter by date, and create the data_raw.parquet file.\n",
    "\n",
    "* Change to True: When you have new raw data or need to reprocess the existing raw data due to changes in the extraction logic.\n",
    "* Keep as False: When you already have a processed data_raw.parquet file and don't need to re-process it.\n",
    "\n",
    "• `PROC_LAGS` (False): This flag controls whether to process and create lag features. If True, the code will calculate lag features based on event history for different time windows and store them in the data_lags.parquet file.\n",
    "\n",
    "* Change to True: When you need to recalculate lag features, such as when you've changed the time window definitions or added new events.\n",
    "* Keep as False: When you already have the desired lag features in data_lags.parquet and don't need to recompute them.\n",
    "\n",
    "• `FRAC_0` (.001): This variable sets the sampling fraction for events with payment_event_flag = 0 when processing lags. This is used to reduce the size of the dataset for faster processing while maintaining a representative sample.\n",
    "\n",
    "* Adjust Value: You might change this value depending on the size of your dataset and the desired balance between processing time and data representativeness.\n",
    "\n",
    "• `PROC_VECS` (True): This flag controls whether to vectorize the data using TF-IDF. If True, the code will transform the lag features into numerical vectors using TF-IDF and store them in data_vec_train.parquet and data_vec_test.parquet files.\n",
    "\n",
    "* Change to False: If you want to experiment with other vectorization methods or use the data in its raw form.\n",
    "* Keep as True: When TF-IDF vectorization is the desired approach for your modeling tasks.\n",
    "\n",
    "**File Paths:**\n",
    "\n",
    "• `files_path`, `files_mask`: These define the location and pattern of the raw data files.\n",
    "• `file_path_ds`, `file_path_lags`, `file_path_trn`, `file_path_tst`: These specify the storage locations for the processed datasets at different stages of the pipeline.\n",
    "\n",
    "By understanding these flags and variables, you can control which parts of the data processing pipeline are executed, allowing for efficient experimentation and iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d37a69-949f-44e2-bf89-ec7a726edc7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "v5 = data week \n",
    "2024, 1, 1, 0, 0, 0 \n",
    "     2024, 4, 16, 23, 59, 59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468306f1-b367-4460-b1ec-75c1401bea43",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Insering my version here below, I.e. V9. If creating NEW data, set all to TRUE</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6671005-60e1-471a-9510-60ae3dad10b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VER = 'vP1' # <-- insert YOUR version here \n",
    "PROC_DS = False # <-- set to true \n",
    "PROC_LAGS = False # <-- set to true \n",
    "FRAC_0 = .001 # used only if `PROC_LAGS = True`\n",
    "PROC_VECS = True\n",
    "BUCKET = 'pvc-84ea79a0-dc20-4a2d-86ab-f83c1f8d4a7b'\n",
    "PRJ_PATH = '/home/jovyan/__RAYPFP'\n",
    "\n",
    "files_path = 'data/events'\n",
    "files_mask = f'{files_path}/data_202*-*-*.csv'\n",
    "\n",
    "file_path_ds = f's3a://{BUCKET}/work/{VER}/data_raw.parquet'\n",
    "file_path_lags = f's3a://{BUCKET}/work/{VER}/data_lags.parquet'\n",
    "file_path_trn = f's3a://{BUCKET}/work/{VER}/data_vec_train.parquet'\n",
    "file_path_tst = f's3a://{BUCKET}/work/{VER}/data_vec_test.parquet'\n",
    "file_path_lags1 = f's3a://{BUCKET}/work/{VER}/data_lags1.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e5c4d7-0cbd-49c8-962b-26df684b9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path_lags1 = f's3a://{BUCKET}/work/{VER}/data_lags1.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7f1e5a2-7795-49d9-b139-3a63385380de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_parquet(path):\n",
    "    cmd = path.replace(\n",
    "        f's3a://{BUCKET}',\n",
    "        f'rm -rf {PRJ_PATH}'\n",
    "    )\n",
    "    !{cmd}\n",
    "    return f'command to run: {cmd}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffaa2985-0b24-4b13-a90f-6584f77e709a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create filenames that will be used later when saving predictions \n",
    "\n",
    "current_datetime = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "current_user = os.environ['JUPYTERHUB_SERVICE_PREFIX']\n",
    "current_user = current_user.split(\"/\")[2]  \n",
    "unique_identifier = str(uuid.uuid4())[:8]  # Generate a unique identifier (first 8 characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3723b66f-323b-43ca-8ef1-95381160e23a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Filename Information:**\n",
      "- Current Date and Time: 20240423_125752\n",
      "- Current User: st095435\n",
      "- Unique Identifier: 3fb7a821\n"
     ]
    }
   ],
   "source": [
    "print(\"**Filename Information:**\")\n",
    "print(f\"- Current Date and Time: {current_datetime}\")\n",
    "print(f\"- Current User: {current_user}\")\n",
    "print(f\"- Unique Identifier: {unique_identifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1718da-1149-4422-9160-9a96d3ea1be7",
   "metadata": {},
   "source": [
    "### 2.1. Load or preprocess data - `raw` stage\n",
    "\n",
    "2.1. Load or Preprocess Data - raw Stage\n",
    "\n",
    "This section checks the `PROC_DS` flag.\n",
    "\n",
    "If True, it reads raw CSV data from S3, parses timestamps, filters and flags payment events within a specific timeframe, and selects relevant columns.\n",
    "\n",
    "The processed data is then saved as a parquet file in S3 and the DataFrame is unloaded from memory.\n",
    "\n",
    "Finally, it reads the processed data from the parquet file and displays a few rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b1c50-3c9f-426b-9b07-321026d78c96",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Ensure dates used are as follows</span>\n",
    "\n",
    "flag_min_datetime = datetime.datetime(2024, 3, 21, 0, 0, 0)\n",
    "\n",
    "flag_max_datetime = datetime.datetime(2024, 4, 18, 23, 59, 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec139b21-815a-4680-a53e-c43a5e24d355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag min datetime:  2024-03-21 00:00:00 \n",
      " flag max datetime:  2024-04-18 23:59:59\n",
      "CPU times: user 13.6 ms, sys: 9.68 ms, total: 23.2 ms\n",
      "Wall time: 6.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if PROC_DS:\n",
    "    sdf = spark.read.option('escape','\"').csv(f's3a://{BUCKET}/{files_mask}', header=True)\n",
    "    sdf = sdf.withColumn('event_datetime', F.to_timestamp(\"event_datetime\"))\n",
    "    flag_min_datetime = datetime.datetime(2024, 3, 21, 0, 0, 0)\n",
    "    flag_max_datetime = datetime.datetime(2024, 4, 18, 23, 59, 59)\n",
    "    print(flag_min_datetime, flag_max_datetime)\n",
    "    sdf = sdf.withColumn(\n",
    "        'payment_event_flag', \n",
    "        (\n",
    "            (F.col('event_name').like('%Мои штрафы/Оплата/Завершили оплату%') | \n",
    "            F.col('event_name').like('%Мои штрафы/Оплата/Платёж принят%')) &\n",
    "            F.col('event_datetime').between(flag_min_datetime, flag_max_datetime)\n",
    "        ).cast(\"int\")\n",
    "    )\n",
    "    sdf = sdf.select(\n",
    "        'profile_id',\n",
    "        'event_datetime',\n",
    "        'payment_event_flag',\n",
    "        'event_name'\n",
    "    )\n",
    "    sdf.repartition(1).write.parquet(file_path_ds)\n",
    "    sdf.unpersist()\n",
    "sdf = spark.read.parquet(file_path_ds)\n",
    "sdf.limit(5).toPandas()\n",
    "\n",
    "\n",
    "\n",
    "if not PROC_DS:\n",
    "    # Code to execute if PROC_DS is False\n",
    "    flag_min_datetime = datetime.datetime(2024, 3, 21, 0, 0, 0)\n",
    "    flag_max_datetime = datetime.datetime(2024, 4, 18, 23, 59, 59)\n",
    "    print(\"flag min datetime: \", flag_min_datetime, '\\n', \n",
    "          \"flag max datetime: \", flag_max_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f106cea-7908-4444-966a-5b0a04bdeb4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|payment_event_flag|    count|\n",
      "+------------------+---------+\n",
      "|                 0|752434695|\n",
      "|                 1|    45742|\n",
      "+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.groupBy('payment_event_flag').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a90e8d11-f747-4fca-8f00-043fc4726c56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20240423_125806-e7f99lil</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/e7f99lil' target=\"_blank\">stellar-glade-17</a></strong> to <a href='https://wandb.ai/gsom-diploma-jap/ray-diploma' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gsom-diploma-jap/ray-diploma' target=\"_blank\">https://wandb.ai/gsom-diploma-jap/ray-diploma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/e7f99lil' target=\"_blank\">https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/e7f99lil</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/e7f99lil?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0b9b094190>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init Weights and Biases to begin storing data \n",
    "\n",
    "wandb.init(project=\"ray-diploma\", config={\n",
    "    \"version\": VER,\n",
    "    \"proc_ds\": PROC_DS,\n",
    "    \"proc_lags\": PROC_LAGS,\n",
    "    \"proc_vecs\": PROC_VECS,\n",
    "    \"frac_0\": FRAC_0,\n",
    "    \"min_datetime\": flag_min_datetime,\n",
    "    \"max_datetime\": flag_max_datetime,\n",
    "    \"current_user\": current_user, \n",
    "    \"uuid\": unique_identifier,\n",
    "    # ... other common parameters ...\n",
    "}, \n",
    "           mode=\"online\", \n",
    "           dir=\"/home/jovyan/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64c8ec84-705b-4e15-b180-28ce1a8e586a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Settings {'_args': None, '_aws_lambda': False, '_async_upload_concurrency_limit': None, '_cli_only_mode': None, '_code_path_local': None, '_colab': False, '_cuda': None, '_disable_meta': False, '_disable_service': False, '_disable_setproctitle': False, '_disable_stats': False, '_disable_update_check': None, '_disable_viewer': None, '_disable_machine_info': False, '_except_exit': None, '_executable': None, '_extra_http_headers': None, '_file_stream_retry_max': 125, '_file_stream_retry_wait_min_seconds': 2.0, '_file_stream_retry_wait_max_seconds': 60.0, '_file_stream_timeout_seconds': 180.0, '_file_transfer_retry_max': 20, '_file_transfer_retry_wait_min_seconds': 2.0, '_file_transfer_retry_wait_max_seconds': 60.0, '_file_transfer_timeout_seconds': 0.0, '_flow_control_custom': False, '_flow_control_disabled': False, '_graphql_retry_max': 20, '_graphql_retry_wait_min_seconds': 2.0, '_graphql_retry_wait_max_seconds': 60.0, '_graphql_timeout_seconds': 30.0, '_internal_check_process': 8.0, '_internal_queue_timeout': 2.0, '_ipython': False, '_jupyter': True, '_jupyter_name': None, '_jupyter_path': None, '_jupyter_root': None, '_kaggle': False, '_live_policy_rate_limit': None, '_live_policy_wait_time': None, '_log_level': 10, '_network_buffer': None, '_noop': False, '_notebook': True, '_offline': False, '_sync': False, '_os': None, '_platform': 'linux', '_proxies': None, '_python': None, '_runqueue_item_id': None, '_require_core': False, '_save_requirements': True, '_service_transport': None, '_service_wait': 30.0, '_shared': False, '_start_datetime': None, '_start_time': None, '_stats_pid': None, '_stats_sample_rate_seconds': 2.0, '_stats_samples_to_average': 15, '_stats_join_assets': True, '_stats_neuron_monitor_config_path': None, '_stats_open_metrics_endpoints': None, '_stats_open_metrics_filters': ('.*',), '_stats_disk_paths': ('/',), '_stats_buffer_size': 0, '_tmp_code_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None/tmp/code', '_tracelog': None, '_unsaved_keys': None, '_windows': False, 'allow_val_change': None, 'anonymous': None, 'api_key': '***REDACTED***', 'azure_account_url_to_access_key': None, 'base_url': 'https://api.wandb.ai', 'code_dir': None, 'colab_url': None, 'config_paths': None, 'console': 'wrap', 'deployment': 'cloud', 'disable_code': False, 'disable_git': False, 'disable_hints': None, 'disable_job_creation': False, 'disabled': False, 'docker': None, 'email': None, 'entity': None, 'files_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None/files', 'force': None, 'fork_from': None, 'git_commit': None, 'git_remote': 'origin', 'git_remote_url': None, 'git_root': None, 'heartbeat_seconds': 30, 'host': None, 'ignore_globs': (), 'init_timeout': 90.0, 'is_local': False, 'job_name': None, 'job_source': None, 'label_disable': None, 'launch': None, 'launch_config_path': None, 'log_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None/logs', 'log_internal': '/home/jovyan/__RAYPFP/wandb/run-None-None/logs/debug-internal.log', 'log_symlink_internal': '/home/jovyan/__RAYPFP/wandb/debug-internal.log', 'log_symlink_user': '/home/jovyan/__RAYPFP/wandb/debug.log', 'log_user': '/home/jovyan/__RAYPFP/wandb/run-None-None/logs/debug.log', 'login_timeout': None, 'mode': 'online', 'notebook_name': None, 'problem': 'fatal', 'program': None, 'program_abspath': None, 'program_relpath': None, 'project': None, 'project_url': '', 'quiet': None, 'reinit': None, 'relogin': None, 'resume': None, 'resume_fname': '/home/jovyan/__RAYPFP/wandb/wandb-resume.json', 'resumed': False, 'root_dir': '/home/jovyan/__RAYPFP', 'run_group': None, 'run_id': None, 'run_job_type': None, 'run_mode': 'run', 'run_name': None, 'run_notes': None, 'run_tags': None, 'run_url': '', 'sagemaker_disable': None, 'save_code': None, 'settings_system': '/home/jovyan/.config/wandb/settings', 'settings_workspace': '/home/jovyan/__RAYPFP/wandb/settings', 'show_colors': None, 'show_emoji': None, 'show_errors': True, 'show_info': True, 'show_warnings': True, 'silent': False, 'start_method': None, 'strict': None, 'summary_errors': None, 'summary_timeout': 60, 'summary_warnings': 5, 'sweep_id': None, 'sweep_param_path': None, 'sweep_url': '', 'symlink': None, 'sync_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None', 'sync_file': '/home/jovyan/__RAYPFP/wandb/run-None-None/run-None.wandb', 'sync_symlink_latest': '/home/jovyan/__RAYPFP/wandb/latest-run', 'system_sample': 15, 'system_sample_seconds': 2, 'table_raise_on_max_row_limit_exceeded': False, 'timespec': None, 'tmp_dir': '/home/jovyan/__RAYPFP/wandb/run-None-None/tmp', 'username': None, 'wandb_dir': '/home/jovyan/__RAYPFP/wandb/'}>\n"
     ]
    }
   ],
   "source": [
    "settings = wandb.Settings()\n",
    "print(settings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02f3f918-4948-4dff-9f42-ac6a2768af30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**After Initial Data Loading**\n",
      "   payment_event_flag      count\n",
      "0                   0  752434695\n",
      "1                   1      45742\n"
     ]
    }
   ],
   "source": [
    "# Weights and Biases logging here:\n",
    "\n",
    "counts = sdf.groupBy('payment_event_flag').count().toPandas()\n",
    "wandb.log({\"data_count\": counts})\n",
    "print(\"**After Initial Data Loading**\")\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c54c5-820c-4822-825f-4557981ad2b6",
   "metadata": {},
   "source": [
    "### 2.2. Load or preprocess data - `lags` stage\n",
    "\n",
    "2.2. Load or Preprocess Data - lags Stage\n",
    "\n",
    "The `dataset_lags` function defines window specifications for different time intervals (e.g., 10 minutes to 1 hour, 1 day to 3 days).\n",
    "\n",
    "It then uses these windows to calculate the list of event names within each time interval for each profile, creating lag features.\n",
    "\n",
    "If `PROC_LAGS` is True, the function samples the data based on the `payment_event_flag` and the specified fraction for negative examples.\n",
    "\n",
    "The processed data with lag features is saved as a parquet file and unloaded from memory.\n",
    "\n",
    "Finally, it reads the data with lags and displays the count of positive and negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe103c1-8be5-4d1e-9cff-cf52060d5c97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lags new  implementation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d69585d-eb48-4a8a-a1b3-e87618a5e488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataset_lags(sdf, shift=0):\n",
    "    hour = 60 * 60\n",
    "    day = 24 * 60 * 60\n",
    "\n",
    "    w_10min_to_1week = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-7 * day + shift, -10 * 60 + shift))\n",
    "    w_1week_to_2weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-14 * day + shift, -7 * day + shift))\n",
    "    w_2weeks_to_3weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-21 * day + shift, -14 * day + shift))\n",
    "    w_3weeks_to_4weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-28 * day + shift, -21 * day + shift))\n",
    "    w_4weeks_to_5weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-35 * day + shift, -28 * day + shift))\n",
    "    w_5weeks_to_6weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-42 * day + shift, -35 * day + shift))\n",
    "    w_6weeks_to_7weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-49 * day + shift, -42 * day + shift))\n",
    "    w_7weeks_to_8weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-56 * day + shift, -49 * day + shift))\n",
    "    w_8weeks_to_9weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-63 * day + shift, -56 * day + shift))\n",
    "    w_9weeks_to_10weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-70 * day + shift, -63 * day + shift))\n",
    "    w_10weeks_to_11weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-77 * day + shift, -70 * day + shift))\n",
    "    w_11weeks_to_12weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-84 * day + shift, -77 * day + shift))\n",
    "    w_12weeks_to_13weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-91 * day + shift, -84 * day + shift))\n",
    "    w_13weeks_to_14weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-98 * day + shift, -91 * day + shift))   \n",
    "    w_14weeks_to_15weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-105 * day + shift, -98 * day + shift))\n",
    "    w_15weeks_to_16weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-112 * day + shift, -105 * day + shift))\n",
    "    w_16weeks_to_17weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-119 * day + shift, -112 * day + shift))\n",
    "    w_17weeks_to_18weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-126 * day + shift, -119 * day + shift))\n",
    "    w_18weeks_to_19weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-133 * day + shift, -126 * day + shift))\n",
    "    w_19weeks_to_20weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-140 * day + shift, -133 * day + shift))\n",
    "    w_20weeks_to_21weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-147 * day + shift, -140 * day + shift))\n",
    "    w_21weeks_to_22weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-154 * day + shift, -147 * day + shift))\n",
    "    w_22weeks_to_23weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-161 * day + shift, -154 * day + shift))\n",
    "    w_23weeks_to_24weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-168 * day + shift, -161 * day + shift))\n",
    "    w_24weeks_to_25weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-175 * day + shift, -168 * day + shift))\n",
    "    w_25weeks_to_26weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-182 * day + shift, -175 * day + shift)) \n",
    "    w_26weeks_to_27weeks = (Window()\n",
    "          .partitionBy(F.col('profile_id'))\n",
    "          .orderBy(F.col('event_datetime').cast('timestamp').cast('long'))\n",
    "          .rangeBetween(-189 * day + shift, -182 * day + shift)) \n",
    "    \n",
    "    return (\n",
    "        sdf\n",
    "            #.withColumn('lag_10min_to_1week', F.collect_list('event_name').over(w_10min_to_1week))\n",
    "            .withColumn('lag_1week_to_2weeks', F.collect_list('event_name').over(w_1week_to_2weeks))\n",
    "            .withColumn('lag_2weeks_to_3weeks', F.collect_list('event_name').over(w_2weeks_to_3weeks))\n",
    "            .withColumn('lag_3weeks_to_4weeks', F.collect_list('event_name').over(w_3weeks_to_4weeks))\n",
    "            .withColumn('lag_4weeks_to_5weeks', F.collect_list('event_name').over(w_4weeks_to_5weeks))\n",
    "            .withColumn('lag_5weeks_to_6weeks', F.collect_list('event_name').over(w_5weeks_to_6weeks))\n",
    "            .withColumn('lag_6weeks_to_7weeks', F.collect_list('event_name').over(w_6weeks_to_7weeks))\n",
    "            .withColumn('lag_7weeks_to_8weeks', F.collect_list('event_name').over(w_7weeks_to_8weeks))\n",
    "            .withColumn('lag_8weeks_to_9weeks', F.collect_list('event_name').over(w_8weeks_to_9weeks))\n",
    "            .withColumn('lag_9weeks_to_10weeks', F.collect_list('event_name').over(w_9weeks_to_10weeks))\n",
    "            .withColumn('lag_10weeks_to_11weeks', F.collect_list('event_name').over(w_10weeks_to_11weeks))\n",
    "            .withColumn('lag_11weeks_to_12weeks', F.collect_list('event_name').over(w_11weeks_to_12weeks))\n",
    "            .withColumn('lag_12weeks_to_13weeks', F.collect_list('event_name').over(w_12weeks_to_13weeks))\n",
    "            .withColumn('lag_13weeks_to_14weeks', F.collect_list('event_name').over(w_13weeks_to_14weeks))\n",
    "            .withColumn('lag_14weeks_to_15weeks', F.collect_list('event_name').over(w_14weeks_to_15weeks))\n",
    "            .withColumn('lag_15weeks_to_16weeks', F.collect_list('event_name').over(w_15weeks_to_16weeks))\n",
    "            .withColumn('lag_16weeks_to_17weeks', F.collect_list('event_name').over(w_16weeks_to_17weeks))\n",
    "            .withColumn('lag_17weeks_to_18weeks', F.collect_list('event_name').over(w_17weeks_to_18weeks))\n",
    "            .withColumn('lag_18weeks_to_19weeks', F.collect_list('event_name').over(w_18weeks_to_19weeks))\n",
    "            .withColumn('lag_19weeks_to_20weeks', F.collect_list('event_name').over(w_19weeks_to_20weeks))\n",
    "            .withColumn('lag_20weeks_to_21weeks', F.collect_list('event_name').over(w_20weeks_to_21weeks))\n",
    "            .withColumn('lag_21weeks_to_22weeks', F.collect_list('event_name').over(w_21weeks_to_22weeks))\n",
    "            .withColumn('lag_22weeks_to_23weeks', F.collect_list('event_name').over(w_22weeks_to_23weeks))\n",
    "            .withColumn('lag_23weeks_to_24weeks', F.collect_list('event_name').over(w_23weeks_to_24weeks))\n",
    "            .withColumn('lag_24weeks_to_25weeks', F.collect_list('event_name').over(w_24weeks_to_25weeks))\n",
    "            .withColumn('lag_25weeks_to_26weeks', F.collect_list('event_name').over(w_25weeks_to_26weeks))\n",
    "            .withColumn('lag_26weeks_to_27weeks', F.collect_list('event_name').over(w_26weeks_to_27weeks))\n",
    "            .select(\n",
    "                'profile_id',\n",
    "                'event_datetime',\n",
    "                'payment_event_flag',\n",
    "                'event_name',\n",
    "                #'lag_10min_to_1week',\n",
    "                'lag_1week_to_2weeks',\n",
    "                'lag_2weeks_to_3weeks',\n",
    "                'lag_3weeks_to_4weeks',\n",
    "                'lag_4weeks_to_5weeks',\n",
    "                'lag_5weeks_to_6weeks',\n",
    "                'lag_6weeks_to_7weeks',\n",
    "                'lag_7weeks_to_8weeks',\n",
    "                'lag_8weeks_to_9weeks',\n",
    "                'lag_9weeks_to_10weeks',\n",
    "                'lag_10weeks_to_11weeks',\n",
    "                'lag_11weeks_to_12weeks',\n",
    "                'lag_12weeks_to_13weeks',\n",
    "                'lag_13weeks_to_14weeks',\n",
    "                'lag_14weeks_to_15weeks',\n",
    "                'lag_15weeks_to_16weeks',\n",
    "                'lag_16weeks_to_17weeks',\n",
    "                'lag_17weeks_to_18weeks',\n",
    "                'lag_18weeks_to_19weeks',\n",
    "                'lag_19weeks_to_20weeks',\n",
    "                'lag_20weeks_to_21weeks',\n",
    "                'lag_21weeks_to_22weeks',\n",
    "                'lag_22weeks_to_23weeks',\n",
    "                'lag_23weeks_to_24weeks',\n",
    "                'lag_24weeks_to_25weeks',\n",
    "                'lag_25weeks_to_26weeks',\n",
    "                'lag_26weeks_to_27weeks'\n",
    "            )\n",
    "        .orderBy(F.col('event_datetime'), ascending=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d70e8-d262-456b-8c7a-c7845b7adfe7",
   "metadata": {},
   "source": [
    "## -- New implementation of time lag windows -- \n",
    "\n",
    "Added the following code; \n",
    "\n",
    "*     clean_parquet(file_path_lags)\n",
    "*     dates  = (flag_min_datetime, flag_max_datetime)\n",
    "\n",
    "1. `clean_parquet(file_path_lags)`:\n",
    "\n",
    "This line of code is used to clean or remove any existing parquet files at the specified file_path_lags location before writing new data.\n",
    "\n",
    "By calling `clean_parquet(file_path_lags)` before writing the new data with time lag windows, it ensures that any previous data stored at the same location is removed, preventing any conflicts or data inconsistencies.\n",
    "\n",
    "**we have commented it out for this because we actually want to reuse the existing data when training and grid searching our models** \n",
    "\n",
    "2. `dates = (flag_min_datetime, flag_max_datetime):`\n",
    "\n",
    "This line creates a tuple named dates that contains two datetime values: flag_min_datetime and flag_max_datetime.\n",
    "\n",
    "The purpose of dates = (flag_min_datetime, flag_max_datetime) is to create a tuple that represents a date range for filtering the data. The values of flag_min_datetime and flag_max_datetime are defined earlier in the notebook – I.e. \n",
    "\n",
    "\"\"\"flag_min_datetime = datetime.datetime(2023, 8, 1, 0, 0, 0)\n",
    "flag_max_datetime = datetime.datetime(2023, 8, 31, 23, 59, 59)\n",
    "print(flag_min_datetime, flag_max_datetime)\"\"\" \n",
    "\n",
    "After creating the dates tuple, the code uses it to filter the sdf DataFrame based on the event_datetime column. \n",
    "\n",
    "The asterisk (*) before dates is used to unpack the tuple and pass the individual datetime values as arguments to the between function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1096c4e5-1885-4cd3-adde-38d6e407a93c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Old version before VG update to filter for dates\n",
    "\n",
    "\n",
    "\n",
    "if PROC_LAGS:\n",
    "    sdf = sdf.sampleBy(\n",
    "        'payment_event_flag', \n",
    "        fractions={0: FRAC_0, 1: 1}, \n",
    "        seed=2023\n",
    "    )\n",
    "    sdf = dataset_lags(sdf)\n",
    "    dates  = (flag_min_datetime, flag_max_datetime)\n",
    "    sdf = sdf.filter(sdf.event_datetime.between(*dates))\n",
    "    sdf = sdf.filter(\n",
    "        #(F.size('lag_10min_to_1week')      > 0) |\n",
    "        (F.size('lag_1week_to_2weeks')     > 0) |\n",
    "        (F.size('lag_2weeks_to_3weeks')    > 0) |\n",
    "        (F.size('lag_3weeks_to_4weeks')    > 0) |\n",
    "        (F.size('lag_4weeks_to_5weeks')    > 0) |\n",
    "        (F.size('lag_5weeks_to_6weeks')    > 0) |\n",
    "        (F.size('lag_6weeks_to_7weeks')    > 0) |\n",
    "        (F.size('lag_7weeks_to_8weeks')    > 0) |\n",
    "        (F.size('lag_8weeks_to_9weeks')    > 0) |\n",
    "        (F.size('lag_9weeks_to_10weeks')   > 0) |\n",
    "        (F.size('lag_10weeks_to_11weeks')  > 0) |\n",
    "        (F.size('lag_11weeks_to_12weeks')  > 0) |\n",
    "        (F.size('lag_12weeks_to_13weeks')  > 0) |\n",
    "        (F.size('lag_13weeks_to_14weeks')  > 0) |\n",
    "        (F.size('lag_14weeks_to_15weeks')  > 0) |\n",
    "        (F.size('lag_15weeks_to_16weeks')  > 0) |\n",
    "        (F.size('lag_16weeks_to_17weeks')  > 0) |\n",
    "        (F.size('lag_17weeks_to_18weeks')  > 0) |\n",
    "        (F.size('lag_18weeks_to_19weeks')  > 0) |\n",
    "        (F.size('lag_19weeks_to_20weeks')  > 0) |\n",
    "        (F.size('lag_20weeks_to_21weeks')  > 0) |\n",
    "        (F.size('lag_21weeks_to_22weeks')  > 0) |\n",
    "        (F.size('lag_22weeks_to_23weeks')  > 0) |\n",
    "        (F.size('lag_23weeks_to_24weeks')  > 0) |\n",
    "        (F.size('lag_24weeks_to_25weeks')  > 0) |\n",
    "        (F.size('lag_25weeks_to_26weeks')  > 0) |\n",
    "        (F.size('lag_26weeks_to_27weeks')  > 0) \n",
    "    )\n",
    "    clean_parquet(file_path_lags)\n",
    "    sdf.repartition(8).write.parquet(file_path_lags)\n",
    "    sdf.unpersist()\n",
    "sdf = spark.read.parquet(file_path_lags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad738be7-ee7f-44d1-b7f4-8d7d32a99835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weights and Biases logging here:\n",
    "counts = sdf.groupBy('payment_event_flag').count().toPandas()\n",
    "wandb.log({\"data_count_with_lags\": counts})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f346f86-6a48-468d-967f-88c67de8d420",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Double check the dataset has been reduced in the following cell</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "039dc7dd-985f-42c9-b2a7-455d3c586d49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Dataset Size After Lag Feature Creation** /n    payment_event_flag  count\n",
      "0                   1  23109\n",
      "1                   0  19594\n"
     ]
    }
   ],
   "source": [
    "print(\"**Dataset Size After Lag Feature Creation**\", '/n', counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92ef1637-35b4-43bc-85e3-badc3fdf5eab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- profile_id: string (nullable = true)\n",
      " |-- event_datetime: timestamp (nullable = true)\n",
      " |-- payment_event_flag: integer (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- lag_1week_to_2weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_2weeks_to_3weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_3weeks_to_4weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_4weeks_to_5weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_5weeks_to_6weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_6weeks_to_7weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_7weeks_to_8weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_8weeks_to_9weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_9weeks_to_10weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_10weeks_to_11weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_11weeks_to_12weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_12weeks_to_13weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_13weeks_to_14weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_14weeks_to_15weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_15weeks_to_16weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_16weeks_to_17weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_17weeks_to_18weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_18weeks_to_19weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_19weeks_to_20weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_20weeks_to_21weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_21weeks_to_22weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_22weeks_to_23weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_23weeks_to_24weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_24weeks_to_25weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_25weeks_to_26weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_26weeks_to_27weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3b490-5702-4c20-883e-613a7d5fa3c0",
   "metadata": {},
   "source": [
    "### Train test split process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8de83425-0fbd-46bc-978a-cfad7ce0c311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define train test split function \n",
    "\n",
    "def stratified_split(sdf, frac, label, seed=2023):\n",
    "    zeros = sdf.filter(sdf[label] == 0)\n",
    "    ones = sdf.filter(sdf[label] == 1)\n",
    "    train_, test_ = zeros.randomSplit([1 - frac, frac], seed=seed)\n",
    "    train, test = ones.randomSplit([1 - frac, frac], seed=seed)\n",
    "    train = train.union(train_)\n",
    "    test = test.union(test_)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa94f255-3f3a-453b-aad9-8772ce8f5a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conduct train test split \n",
    "\n",
    "sdf_train, sdf_test = stratified_split(\n",
    "    sdf, \n",
    "    frac=.2, # Size of the test dataset\n",
    "    label='payment_event_flag',\n",
    "    seed=2023\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4c0e6-8257-4ef1-8110-8128ea5dbaf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color: red;\">Check the data has been split and classes are approx equal</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "501e35b0-bd8f-4318-a973-155b4fd72a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>18440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>15649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag  count\n",
       "0                   1  18440\n",
       "1                   0  15649"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_train.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "776b6d5a-10bc-4e1c-9460-65e0e204ef82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag  count\n",
       "0                   1   4669\n",
       "1                   0   3945"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_test.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf9adc0b-6be7-4e8c-a092-19f282425ded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weights and Biases logging here:\n",
    "\n",
    "train_counts = sdf_train.groupBy('payment_event_flag').count().toPandas()\n",
    "wandb.log({\"train_data_count\": train_counts})\n",
    "\n",
    "test_counts = sdf_test.groupBy('payment_event_flag').count().toPandas()\n",
    "wandb.log({\"test_data_count\": test_counts})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf01e30-f5ef-40dc-99c7-99e99dc27974",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color: red;\">Ensure date ranges used are the ones we set earlier</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ed66acb-2642-463d-870b-d3d0aef7e33a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Date: 2024-03-21 00:00:17\n",
      "Maximum Date: 2024-04-18 23:59:23\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "# Find the minimum and maximum dates\n",
    "min_date = sdf.agg(min(\"event_datetime\")).collect()[0][0]\n",
    "max_date = sdf.agg(max(\"event_datetime\")).collect()[0][0]\n",
    "\n",
    "print(f\"Minimum Date: {min_date}\")\n",
    "print(f\"Maximum Date: {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6541e050-fb85-43bc-9e12-bb25cfbb7a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Minimum Date: 2024-03-21 00:00:17\n",
      "Training Set Maximum Date: 2024-04-18 23:57:47\n"
     ]
    }
   ],
   "source": [
    "# Training set\n",
    "train_min_date = sdf_train.agg(min(\"event_datetime\")).collect()[0][0]\n",
    "train_max_date = sdf_train.agg(max(\"event_datetime\")).collect()[0][0]\n",
    "\n",
    "print(f\"Training Set Minimum Date: {train_min_date}\")\n",
    "print(f\"Training Set Maximum Date: {train_max_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80751428-6358-472c-a4f0-65e9f14ceeae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Minimum Date: 2024-03-21 00:05:21\n",
      "Test Set Maximum Date: 2024-04-18 23:59:23\n"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "test_min_date = sdf_test.agg(min(\"event_datetime\")).collect()[0][0]\n",
    "test_max_date = sdf_test.agg(max(\"event_datetime\")).collect()[0][0]\n",
    "\n",
    "print(f\"Test Set Minimum Date: {test_min_date}\")\n",
    "print(f\"Test Set Maximum Date: {test_max_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9856ef-8571-467e-871f-70cbf66e6e6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3. Load or preprocess data - `vectorize` stage\n",
    "\n",
    "2.3. Load or Preprocess Data - vectorize Stage\n",
    "\n",
    "This section defines a list of lag features to be used.\n",
    "\n",
    "The datasets_tfidf function performs `TF-IDF` vectorization on the lag features for both training and test datasets.\n",
    "\n",
    "It uses `HashingTF` to convert lists of event names into numerical feature vectors and `IDF` to rescale the features based on their document frequency.\n",
    "\n",
    "The function also creates a dictionary mapping feature indices to the corresponding event names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62b8a249-c54f-493f-9679-baaa4e3dff6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lags = [\n",
    "#    'lag_10min_to_1week',\n",
    "    'lag_1week_to_2weeks',\n",
    "    'lag_2weeks_to_3weeks',\n",
    "    'lag_3weeks_to_4weeks',\n",
    "    'lag_4weeks_to_5weeks',\n",
    "    'lag_5weeks_to_6weeks',\n",
    "    'lag_6weeks_to_7weeks',\n",
    "    'lag_7weeks_to_8weeks',\n",
    "    'lag_8weeks_to_9weeks',\n",
    "    'lag_9weeks_to_10weeks',\n",
    "    'lag_10weeks_to_11weeks',\n",
    "    'lag_11weeks_to_12weeks',\n",
    "    'lag_12weeks_to_13weeks',\n",
    "    'lag_13weeks_to_14weeks',\n",
    "    'lag_14weeks_to_15weeks',\n",
    "    'lag_15weeks_to_16weeks',\n",
    "    'lag_16weeks_to_17weeks',\n",
    "    'lag_17weeks_to_18weeks',\n",
    "    'lag_18weeks_to_19weeks',\n",
    "    'lag_19weeks_to_20weeks',\n",
    "    'lag_20weeks_to_21weeks',\n",
    "    'lag_21weeks_to_22weeks',\n",
    "    'lag_22weeks_to_23weeks',\n",
    "    'lag_23weeks_to_24weeks',\n",
    "    'lag_24weeks_to_25weeks',\n",
    "    'lag_25weeks_to_26weeks',\n",
    "    'lag_26weeks_to_27weeks'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02591fe2-1f8b-4331-868a-96560fdb17b9",
   "metadata": {},
   "source": [
    "## TF-IDF implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddc7853f-0b02-4982-84ca-a58d91b8e6c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datasets_tfidf(sdf_train, sdf_test, lags, min_freq=3, num_features=10):\n",
    "    features_dict = {}\n",
    "    count = 0\n",
    "    for lag in tqdm(lags):\n",
    "        hashingTF = HashingTF(\n",
    "            inputCol=lag, \n",
    "            outputCol=lag + '_tf', \n",
    "            numFeatures=num_features\n",
    "        )\n",
    "        featurizedData = hashingTF.transform(sdf_train)\n",
    "        idf = IDF(\n",
    "            inputCol=lag + '_tf', \n",
    "            outputCol=lag + '_tfidf',\n",
    "            minDocFreq=min_freq  \n",
    "        )\n",
    "        idfModel = idf.fit(featurizedData)\n",
    "        sdf_train = idfModel.transform(featurizedData)\n",
    "        sdf_test = idfModel.transform(\n",
    "            hashingTF.transform(sdf_test)\n",
    "        )\n",
    "        events = [\n",
    "            x\n",
    "            for xs in sdf_train.select(lag).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            for x in xs\n",
    "        ]\n",
    "        hash_dict = {}\n",
    "        for e in events:\n",
    "            hash_dict[lag + '_' + e] = hashingTF.indexOf(e)\n",
    "        for feat_num in range(num_features):\n",
    "            tmp_list = []\n",
    "            for k, v in hash_dict.items():\n",
    "                if v == feat_num: tmp_list.append(k)\n",
    "            features_dict[count * num_features + feat_num] = tmp_list\n",
    "        count += 1\n",
    "    return sdf_train, sdf_test, features_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b0500-bcea-44f6-9b39-873654df40de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Breakdown of if PROC_VECS code: \n",
    "\n",
    "\n",
    "**Conditional Execution:**\n",
    "\n",
    "• if PROC_VECS:: The code within this block is executed only if the PROC_VECS flag is set to True. This flag controls whether TF-IDF vectorization is performed on the data.\n",
    "\n",
    "**TF-IDF Vectorization:**\n",
    "\n",
    "`sdf_train, sdf_test, vectorizers = datasets_tfidf(...)`: This line calls the datasets_tfidf function, which performs TF-IDF vectorization on the lag features present in the sdf_train and sdf_test DataFrames.\n",
    "* The lags argument provides the list of lag feature column names to be vectorized.\n",
    "* The vec_size=10 argument specifies the desired dimensionality (number of features) of the resulting TF-IDF vectors.\n",
    "\n",
    "**The function returns three values:**\n",
    "\n",
    "* `sdf_train`: The training DataFrame with the added TF-IDF vector columns.\n",
    "* `sdf_test`: The test DataFrame with the added TF-IDF vector columns.\n",
    "* `vectorizers`: A list of fitted TF-IDF vectorizer models (one for each lag feature).\n",
    "\n",
    "**Cleaning and Saving Parquet Files:**\n",
    "\n",
    "`clean_parquet(file_path_trn)`: This line calls a function (not shown) to clean up any existing Parquet files at the specified path (file_path_trn) before saving the new data.\n",
    "\n",
    "`sdf_train.repartition(8).write.parquet(file_path_trn)`: The training DataFrame (sdf_train) is repartitioned into 8 partitions for optimized writing.\n",
    "\n",
    "* The write.parquet method saves the DataFrame as a Parquet file at the specified path (file_path_trn).\n",
    "* The same process is repeated for the test DataFrame (sdf_test) using file_path_tst.\n",
    "\n",
    "**Unpersisting DataFrames:**\n",
    "\n",
    "* `sdf_train.unpersist(), sdf_test.unpersist()`: These lines remove the DataFrames from Spark's memory. Since the data has been saved to disk, it can be reloaded later if needed, freeing up memory for subsequent processing.\n",
    "\n",
    "**Reloading DataFrames (if necessary):**\n",
    "\n",
    "* `sdf_train = spark.read.parquet(file_path_trn)`: This line reloads the training data from the saved Parquet file if it's not already in memory.\n",
    "\n",
    "The same is done for the test data using file_path_tst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f0b3cb5-737c-4aed-a668-827b9ca58f66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a719106a6f499bbf1b6455554be69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NEW version with clean_parquet (TF-IDF)\n",
    "\n",
    "if PROC_VECS:\n",
    "    sdf_train, sdf_test, features_dict = datasets_tfidf(\n",
    "        sdf_train,\n",
    "        sdf_test,\n",
    "        lags,\n",
    "        #vec_size=10,\n",
    "        min_freq=3,\n",
    "        num_features=100\n",
    "    )\n",
    "    clean_parquet(file_path_trn)\n",
    "    sdf_train.repartition(8).write.parquet(file_path_trn)\n",
    "    clean_parquet(file_path_tst)\n",
    "    sdf_test.repartition(8).write.parquet(file_path_tst)\n",
    "    sdf_train.unpersist()\n",
    "    sdf_test.unpersist()\n",
    "sdf_train = spark.read.parquet(file_path_trn)\n",
    "sdf_test = spark.read.parquet(file_path_tst)\n",
    "\n",
    "\n",
    "if not PROC_VECS:\n",
    "    sdf_train, sdf_test, features_dict = datasets_tfidf(\n",
    "        sdf_train, \n",
    "        sdf_test, \n",
    "        lags, \n",
    "        min_freq=3,\n",
    "        num_features=100\n",
    "    )\n",
    "    print(len(features_dict.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf769561-8968-4530-83f0-4243e0c588d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Training Set After TF-IDF**\n",
      "+------------------+-----+\n",
      "|payment_event_flag|count|\n",
      "+------------------+-----+\n",
      "|                 1|18440|\n",
      "|                 0|15649|\n",
      "+------------------+-----+\n",
      "\n",
      "**Testing Set After TF-IDF**\n",
      "+------------------+-----+\n",
      "|payment_event_flag|count|\n",
      "+------------------+-----+\n",
      "|                 1| 4669|\n",
      "|                 0| 3945|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check data size after reloading\n",
    "print(\"**Training Set After TF-IDF**\")\n",
    "sdf_train.groupBy('payment_event_flag').count().show()\n",
    "print(\"**Testing Set After TF-IDF**\")\n",
    "sdf_test.groupBy('payment_event_flag').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a666c70-fcb7-4f5c-83fa-5c5c29d9d007",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- profile_id: string (nullable = true)\n",
      " |-- event_datetime: timestamp (nullable = true)\n",
      " |-- payment_event_flag: integer (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- lag_1week_to_2weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_2weeks_to_3weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_3weeks_to_4weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_4weeks_to_5weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_5weeks_to_6weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_6weeks_to_7weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_7weeks_to_8weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_8weeks_to_9weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_9weeks_to_10weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_10weeks_to_11weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_11weeks_to_12weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_12weeks_to_13weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_13weeks_to_14weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_14weeks_to_15weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_15weeks_to_16weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_16weeks_to_17weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_17weeks_to_18weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_18weeks_to_19weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_19weeks_to_20weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_20weeks_to_21weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_21weeks_to_22weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_22weeks_to_23weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_23weeks_to_24weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_24weeks_to_25weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_25weeks_to_26weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_26weeks_to_27weeks: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lag_1week_to_2weeks_tf: vector (nullable = true)\n",
      " |-- lag_1week_to_2weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_2weeks_to_3weeks_tf: vector (nullable = true)\n",
      " |-- lag_2weeks_to_3weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_3weeks_to_4weeks_tf: vector (nullable = true)\n",
      " |-- lag_3weeks_to_4weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_4weeks_to_5weeks_tf: vector (nullable = true)\n",
      " |-- lag_4weeks_to_5weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_5weeks_to_6weeks_tf: vector (nullable = true)\n",
      " |-- lag_5weeks_to_6weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_6weeks_to_7weeks_tf: vector (nullable = true)\n",
      " |-- lag_6weeks_to_7weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_7weeks_to_8weeks_tf: vector (nullable = true)\n",
      " |-- lag_7weeks_to_8weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_8weeks_to_9weeks_tf: vector (nullable = true)\n",
      " |-- lag_8weeks_to_9weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_9weeks_to_10weeks_tf: vector (nullable = true)\n",
      " |-- lag_9weeks_to_10weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_10weeks_to_11weeks_tf: vector (nullable = true)\n",
      " |-- lag_10weeks_to_11weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_11weeks_to_12weeks_tf: vector (nullable = true)\n",
      " |-- lag_11weeks_to_12weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_12weeks_to_13weeks_tf: vector (nullable = true)\n",
      " |-- lag_12weeks_to_13weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_13weeks_to_14weeks_tf: vector (nullable = true)\n",
      " |-- lag_13weeks_to_14weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_14weeks_to_15weeks_tf: vector (nullable = true)\n",
      " |-- lag_14weeks_to_15weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_15weeks_to_16weeks_tf: vector (nullable = true)\n",
      " |-- lag_15weeks_to_16weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_16weeks_to_17weeks_tf: vector (nullable = true)\n",
      " |-- lag_16weeks_to_17weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_17weeks_to_18weeks_tf: vector (nullable = true)\n",
      " |-- lag_17weeks_to_18weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_18weeks_to_19weeks_tf: vector (nullable = true)\n",
      " |-- lag_18weeks_to_19weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_19weeks_to_20weeks_tf: vector (nullable = true)\n",
      " |-- lag_19weeks_to_20weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_20weeks_to_21weeks_tf: vector (nullable = true)\n",
      " |-- lag_20weeks_to_21weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_21weeks_to_22weeks_tf: vector (nullable = true)\n",
      " |-- lag_21weeks_to_22weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_22weeks_to_23weeks_tf: vector (nullable = true)\n",
      " |-- lag_22weeks_to_23weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_23weeks_to_24weeks_tf: vector (nullable = true)\n",
      " |-- lag_23weeks_to_24weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_24weeks_to_25weeks_tf: vector (nullable = true)\n",
      " |-- lag_24weeks_to_25weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_25weeks_to_26weeks_tf: vector (nullable = true)\n",
      " |-- lag_25weeks_to_26weeks_tfidf: vector (nullable = true)\n",
      " |-- lag_26weeks_to_27weeks_tf: vector (nullable = true)\n",
      " |-- lag_26weeks_to_27weeks_tfidf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bbf517-2d00-449e-943a-966cacaea33d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "def datasets_vecorized(sdf_train, sdf_test, lags, vec_size=10):\n",
    "    vectorizers = []\n",
    "    for lag in tqdm(lags):\n",
    "        word2Vec = Word2Vec(\n",
    "            vectorSize=vec_size,\n",
    "            minCount=0,\n",
    "            inputCol=lag,\n",
    "            outputCol=lag + '_vec'\n",
    "        )\n",
    "        vectorizer = word2Vec.fit(sdf_train)\n",
    "        sdf_train = vectorizer.transform(sdf_train)\n",
    "        sdf_test = vectorizer.transform(sdf_test)\n",
    "        vectorizers.append(vectorizer)\n",
    "    return sdf_train, sdf_test, vectorizers\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6b9e0-0e7b-4459-ab37-e970f26fb3d6",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd83edd-91ab-4364-b7a3-64abcd9def7f",
   "metadata": {},
   "source": [
    "### 3.1. Features assembling\n",
    "\n",
    "3.1. Features Assembling\n",
    "\n",
    "The features_assembled function prepares the data for model training:\n",
    "\n",
    "* It selects the `TF-IDF` features and the target variable (`payment_event_flag`).\n",
    "* It uses VectorAssembler to combine the `TF-IDF` features into a single vector column named `features`.\n",
    "* It returns a DataFrame with the target variable and the assembled feature vector.\n",
    "\n",
    "The upsampled function can be used to address class imbalance:\n",
    "\n",
    "* It separates the data into positive and negative examples.\n",
    "* It duplicates the positive examples to achieve a balanced class distribution based on the `UPSAMPLE` setting.\n",
    "\n",
    "The code then:\n",
    "* Defines a list of lag features to be used.\n",
    "* Assembles features for both training and test sets.\n",
    "* Optionally performs **upsampling** on the training set (and potentially the test set) if `UPSAMPLE` is enabled.\n",
    "* Displays the class distribution after upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f662437f-0848-4e87-83f6-28edba536c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def features_assembled(sdf, feats):\n",
    "    cols_to_model = [x + '_tfidf' for x in feats]\n",
    "    cols_to_model.extend(['payment_event_flag'])\n",
    "    print('columns to model:', cols_to_model)\n",
    "    vecAssembler = VectorAssembler(\n",
    "        inputCols=[c for c in cols_to_model if c != 'payment_event_flag'], \n",
    "        outputCol='features'\n",
    "    )\n",
    "    features = sdf.select(cols_to_model)\n",
    "    features_vec = vecAssembler.transform(features)\n",
    "    features_data = features_vec.select('payment_event_flag', 'features')\n",
    "    return features_data\n",
    "\n",
    "def upsampled(sdf, label, upsample='max'):\n",
    "    zeros = sdf.filter(sdf[label] == 0)\n",
    "    ones = sdf.filter(sdf[label] == 1)\n",
    "    res = zeros.union(ones)\n",
    "    if upsample == 'max':\n",
    "        up_count = int(zeros.count() / ones.count())\n",
    "        for _ in range(up_count - 1):\n",
    "            res = res.union(ones)\n",
    "    else:\n",
    "        for _ in range(upsample - 1):\n",
    "            res = res.union(ones)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b29bfde6-70c1-4272-8bc6-0e317a673776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"MAX\" Strategy: Setting UPSAMPLE = 'max' instructs the upsampled function to duplicate the minority class examples\n",
    "# They are upsampled until their count becomes equal to the count of the majority class. \n",
    "# In other words, it aims for a 1:1 class ratio.\n",
    "\n",
    "UPSAMPLE = 'max' # Can be either 'none' or 'max'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aefefcd5-8048-4120-93fa-9687290ada7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns to model: ['lag_1week_to_2weeks_tfidf', 'lag_2weeks_to_3weeks_tfidf', 'lag_3weeks_to_4weeks_tfidf', 'lag_4weeks_to_5weeks_tfidf', 'lag_5weeks_to_6weeks_tfidf', 'lag_6weeks_to_7weeks_tfidf', 'lag_7weeks_to_8weeks_tfidf', 'lag_8weeks_to_9weeks_tfidf', 'lag_9weeks_to_10weeks_tfidf', 'lag_10weeks_to_11weeks_tfidf', 'lag_11weeks_to_12weeks_tfidf', 'lag_12weeks_to_13weeks_tfidf', 'lag_13weeks_to_14weeks_tfidf', 'lag_14weeks_to_15weeks_tfidf', 'lag_15weeks_to_16weeks_tfidf', 'lag_16weeks_to_17weeks_tfidf', 'lag_17weeks_to_18weeks_tfidf', 'lag_18weeks_to_19weeks_tfidf', 'lag_19weeks_to_20weeks_tfidf', 'lag_20weeks_to_21weeks_tfidf', 'lag_21weeks_to_22weeks_tfidf', 'lag_22weeks_to_23weeks_tfidf', 'lag_23weeks_to_24weeks_tfidf', 'lag_24weeks_to_25weeks_tfidf', 'lag_25weeks_to_26weeks_tfidf', 'lag_26weeks_to_27weeks_tfidf', 'payment_event_flag']\n",
      "columns to model: ['lag_1week_to_2weeks_tfidf', 'lag_2weeks_to_3weeks_tfidf', 'lag_3weeks_to_4weeks_tfidf', 'lag_4weeks_to_5weeks_tfidf', 'lag_5weeks_to_6weeks_tfidf', 'lag_6weeks_to_7weeks_tfidf', 'lag_7weeks_to_8weeks_tfidf', 'lag_8weeks_to_9weeks_tfidf', 'lag_9weeks_to_10weeks_tfidf', 'lag_10weeks_to_11weeks_tfidf', 'lag_11weeks_to_12weeks_tfidf', 'lag_12weeks_to_13weeks_tfidf', 'lag_13weeks_to_14weeks_tfidf', 'lag_14weeks_to_15weeks_tfidf', 'lag_15weeks_to_16weeks_tfidf', 'lag_16weeks_to_17weeks_tfidf', 'lag_17weeks_to_18weeks_tfidf', 'lag_18weeks_to_19weeks_tfidf', 'lag_19weeks_to_20weeks_tfidf', 'lag_20weeks_to_21weeks_tfidf', 'lag_21weeks_to_22weeks_tfidf', 'lag_22weeks_to_23weeks_tfidf', 'lag_23weeks_to_24weeks_tfidf', 'lag_24weeks_to_25weeks_tfidf', 'lag_25weeks_to_26weeks_tfidf', 'lag_26weeks_to_27weeks_tfidf', 'payment_event_flag']\n"
     ]
    }
   ],
   "source": [
    "# Setting feats, make sure to comment out (#) any lags we will not use for train/pred \n",
    "\n",
    "feats = [\n",
    "#    'lag_10min_to_1week',\n",
    "    'lag_1week_to_2weeks',\n",
    "    'lag_2weeks_to_3weeks',\n",
    "    'lag_3weeks_to_4weeks',\n",
    "    'lag_4weeks_to_5weeks',\n",
    "    'lag_5weeks_to_6weeks',\n",
    "    'lag_6weeks_to_7weeks',\n",
    "    'lag_7weeks_to_8weeks',\n",
    "    'lag_8weeks_to_9weeks',\n",
    "    'lag_9weeks_to_10weeks',\n",
    "    'lag_10weeks_to_11weeks',\n",
    "    'lag_11weeks_to_12weeks',\n",
    "    'lag_12weeks_to_13weeks',\n",
    "    'lag_13weeks_to_14weeks',\n",
    "    'lag_14weeks_to_15weeks',\n",
    "    'lag_15weeks_to_16weeks',\n",
    "    'lag_16weeks_to_17weeks',\n",
    "    'lag_17weeks_to_18weeks',\n",
    "    'lag_18weeks_to_19weeks',\n",
    "    'lag_19weeks_to_20weeks',\n",
    "    'lag_20weeks_to_21weeks',\n",
    "    'lag_21weeks_to_22weeks',\n",
    "    'lag_22weeks_to_23weeks',\n",
    "    'lag_23weeks_to_24weeks',\n",
    "    'lag_24weeks_to_25weeks',\n",
    "    'lag_25weeks_to_26weeks',\n",
    "    'lag_26weeks_to_27weeks'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "features_train = features_assembled(sdf_train, feats=feats)\n",
    "features_test = features_assembled(sdf_test, feats=feats)\n",
    "\n",
    "if UPSAMPLE:\n",
    "    features_train = upsampled(\n",
    "        features_train, \n",
    "        label='payment_event_flag', \n",
    "        upsample=UPSAMPLE\n",
    "    )\n",
    "    # Use to upsample test set\n",
    "    #features_test = upsampled(\n",
    "    #    features_test, \n",
    "    #    label='payment_event_flag', \n",
    "    #    upsample=UPSAMPLE\n",
    "    #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "388d38ed-6ced-4935-9762-3d0941a29e58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag  count\n",
       "0                   0  15649\n",
       "1                   1  18440"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3f2dfc9-ed4f-4139-9074-c986b12cdc7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag  count\n",
       "0                   1   4669\n",
       "1                   0   3945"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.groupBy('payment_event_flag').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2864416b-80b2-4117-9938-044d98cd51a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_event_flag</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   payment_event_flag                                           features\n",
       "0                   0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "1                   0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2                   0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d82501c0-c4f1-43f8-a2fe-2b2b643f273d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(2600, {200: 4.7984, 352: 7.6642, 401: 5.2438})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.limit(3).toPandas()['features'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4814c-5ddf-46e7-bea6-03c45f94cac7",
   "metadata": {},
   "source": [
    "### 3.2. Training and evaluating\n",
    "\n",
    "#### <span style=\"color: red;\">This will train and evaluate WITHOUT hyperopt</span>\n",
    "\n",
    "\n",
    "\n",
    "* `labelCol`: Specifies the target variable column (\"`payment_event_flag`\").\n",
    "* `featuresCol`: Specifies the feature vector column (\"`features`\").\n",
    "\n",
    "\n",
    "* The model is trained using the fit method on the training data.\n",
    "* The trained model is used to make predictions on the test data.\n",
    "\n",
    "The `BinaryClassificationMetrics` class is used to calculate evaluation metrics:\n",
    "\n",
    "* `areaUnderROC`: Area under the ROC curve, which measures the model's ability to distinguish between classes.\n",
    "* `areaUnderPR`: Area under the Precision-Recall curve, which is more informative for imbalanced datasets.\n",
    "\n",
    "The code also uses `classification_report` from `scikit-learn` to get a detailed report including precision, recall, F1-score, and support for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b82127ac-bfc4-4e16-9833-a713fd405d28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Prediction File Path: s3a://pvc-84ea79a0-dc20-4a2d-86ab-f83c1f8d4a7b/work/vP1/preds_LogisticRegressionModel_20240423_105252_st095435_fd66151b.csv\n"
     ]
    }
   ],
   "source": [
    "# Define model filepath to link predictions results with the actual CSV \n",
    "\n",
    "current_model = \"LogisticRegressionModel\"\n",
    "\n",
    "file_path_pred = f's3a://{BUCKET}/work/{VER}/preds_{current_model}_{current_datetime}_{current_user}_{unique_identifier}.csv'\n",
    "print(f\"- Prediction File Path: {file_path_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d129528-5ec0-4594-9acd-9732a927351f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17c8ca0e-d355-446d-bf67-539fbe24d1e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_event_flag: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6382aa88-e3bc-4f38-944c-a2180ab5ab8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"payment_event_flag\", featuresCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24f90b6a-6bd3-469f-9052-1c1e1a52fbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 ms, sys: 899 µs, total: 11.7 ms\n",
      "Wall time: 7.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = lr.fit(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef26d6cb-0f6b-4ad6-8a3d-fcbb69b8db36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|payment_event_flag|            features|\n",
      "+------------------+--------------------+\n",
      "|                 0|(2600,[200,352,40...|\n",
      "|                 0|(2600,[445,1126,2...|\n",
      "|                 0|(2600,[193,488],[...|\n",
      "|                 0|(2600,[2500],[4.8...|\n",
      "|                 0|(2600,[114,1191,1...|\n",
      "+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_train.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694c5ec-0978-4a95-8479-160fdcd504a7",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">---------------</span>\n",
    "NB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc5f761a-bc67-45e9-a6d4-cce0e0771604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.6563167085658691\n",
      "Area under PR-curve: 0.6695454803362192\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(features_test)\n",
    "payment_event_flag_preds = predictions.select('prediction', 'payment_event_flag')\n",
    "metrics = BinaryClassificationMetrics(\n",
    "    payment_event_flag_preds.rdd.map(\n",
    "        lambda lines: [float(x) for x in lines]\n",
    "    )\n",
    ")\n",
    "print('ROC AUC:', metrics.areaUnderROC)\n",
    "print('Area under PR-curve:', metrics.areaUnderPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c517dddf-b53d-4787-8e0a-d9c8110381ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weights and Biases logging here\n",
    "wandb.log({\"roc_auc\": metrics.areaUnderROC})\n",
    "wandb.log({\"pr_auc\": metrics.areaUnderPR})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e955f18-ebcc-4036-9464-d2918370d7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.67      0.64      3945\n",
      "           1       0.70      0.64      0.67      4669\n",
      "\n",
      "    accuracy                           0.66      8614\n",
      "   macro avg       0.66      0.66      0.65      8614\n",
      "weighted avg       0.66      0.66      0.66      8614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predictions = model.transform(features_test)\n",
    "\n",
    "# Получаем фактические метки классов и предсказанные значения\n",
    "y_true = predictions.select('payment_event_flag').rdd.map(lambda x: x['payment_event_flag']).collect()\n",
    "y_pred = predictions.select('prediction').rdd.map(lambda x: x['prediction']).collect()\n",
    "\n",
    "# Выводим classification_report\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22427382-cea7-4c7f-a382-baed64ba4bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weights and Biases logging here\n",
    "wandb.log({\"classification_report\": wandb.Html(report)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e6c72-ffa2-4f62-a0db-01c299615991",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Importance \n",
    "\n",
    "* The code extracts feature importances from the trained model and filters them based on a threshold (`TH`).\n",
    "* The feature importances are then sorted in descending order.\n",
    "* The code prints the feature number, importance value, and corresponding event names for the most important features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5413674-e3e4-4ee0-91fc-09bacaf4d1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available hyperparameters:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Выводим доступные гиперпараметры модели\n",
    "print(\"Available hyperparameters:\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90670aa0-313b-4049-bc06-2967dee797e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: payment_event_flag)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5689856d-cf96-41d4-ac9d-efc52a5a8e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f2c29-fe5f-44a8-b4b2-b1a73a638f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a06fbbc-58d8-48e1-acde-fc09b5a1b796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6ac3f6-ce18-4f98-b6f9-e3f385432a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TH = .01\n",
    "\n",
    "features_imps = {}\n",
    "for i, v in enumerate(model.featureImportances.toArray()):\n",
    "    if v >= TH: features_imps[i] = v\n",
    "features_imps = dict(sorted(features_imps.items(), key=lambda x: x[1], reverse=True))\n",
    "features_imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2c9512e6-8057-42f7-9e19-2463a99bfb58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k, v in features_imps.items():\n",
    "    print('-' * 100)\n",
    "    print('feature number:', k, '| feature importance:', v)\n",
    "    print('features:', features_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db076d2a-d965-4b55-9d8c-0e89804d40cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weights and biases logging here\n",
    "\n",
    "wandb.log({\"feature_importances\": wandb.Table(dataframe=pd.DataFrame.from_dict(features_imps, orient='index'))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59a741-a297-4aa8-a68a-ea93a5c3da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Преобразование данных из DataFrame в numpy массив соответствующей размерности\n",
    "#X_test = np.array(features_test.select('features').collect()).reshape(-1, len(feats))\n",
    "#\n",
    "#explainer = shap.TreeExplainer(model)  # Initialize explainer\n",
    "#\n",
    "## Получаем значения SHAP для данных\n",
    "#shap_values = explainer.shap_values(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f20ea53-0325-4c36-a2c5-532bb6b8bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.summary_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff8e1d-4627-4b57-bbe2-9c82ef65a977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weights and biases logging here \n",
    "\n",
    "#wandb.log({\"shap_values\": wandb.Table(dataframe=pd.DataFrame(shap_values))})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2656d72f-d12c-4317-93c6-912c7fb43fbe",
   "metadata": {},
   "source": [
    "# 5. Predictions old variant (base model, no hyperopt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4def4cb-6f8a-45f2-853c-1615efbd047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sdf_pred = spark.read.parquet(file_path_ds)\n",
    "#sdf_pred.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d776724-ce3d-47f7-818b-c71d24b559f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#SHIFT = 7 * 24 * 60 * 60  # 7 days ahead\n",
    "#\n",
    "#sdf_pred = sdf_pred.sample(fraction=.0001)\n",
    "#sdf_pred = dataset_lags(sdf_pred, shift=SHIFT)\n",
    "#sdf = sdf.filter(\n",
    "#        #(F.size('lag_10min_to_1week')      > 0) |\n",
    "#        (F.size('lag_1week_to_2weeks')     > 0) |\n",
    "#        (F.size('lag_2weeks_to_3weeks')    > 0) |\n",
    "#        (F.size('lag_3weeks_to_4weeks')    > 0) |\n",
    "#        (F.size('lag_4weeks_to_5weeks')    > 0) |\n",
    "#        (F.size('lag_5weeks_to_6weeks')    > 0) |\n",
    "#        (F.size('lag_6weeks_to_7weeks')    > 0) |\n",
    "#        (F.size('lag_7weeks_to_8weeks')    > 0) |\n",
    "#        (F.size('lag_8weeks_to_9weeks')    > 0) |\n",
    "#        (F.size('lag_9weeks_to_10weeks')   > 0) |\n",
    "#        (F.size('lag_10weeks_to_11weeks')  > 0) |\n",
    "#        (F.size('lag_11weeks_to_12weeks')  > 0) |\n",
    "#        (F.size('lag_12weeks_to_13weeks')  > 0) |\n",
    "#        (F.size('lag_13weeks_to_14weeks')  > 0) |\n",
    "#        (F.size('lag_14weeks_to_15weeks')  > 0) |\n",
    "#        (F.size('lag_15weeks_to_16weeks')  > 0) |\n",
    "#        (F.size('lag_16weeks_to_17weeks')  > 0) |\n",
    "#        (F.size('lag_17weeks_to_18weeks')  > 0) |\n",
    "#        (F.size('lag_18weeks_to_19weeks')  > 0) |\n",
    "#        (F.size('lag_19weeks_to_20weeks')  > 0) |\n",
    "#        (F.size('lag_20weeks_to_21weeks')  > 0) |\n",
    "#        (F.size('lag_21weeks_to_22weeks')  > 0) |\n",
    "#        (F.size('lag_22weeks_to_23weeks')  > 0) |\n",
    "#        (F.size('lag_23weeks_to_24weeks')  > 0) |\n",
    "#        (F.size('lag_24weeks_to_25weeks')  > 0) |\n",
    "#        (F.size('lag_25weeks_to_26weeks')  > 0) |\n",
    "#        (F.size('lag_26weeks_to_27weeks')  > 0) \n",
    "#    )\n",
    "#sdf_pred.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20b361-256b-4b29-abdc-954a40b59ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3c404-5f70-4da9-b180-602c9c0bca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if word2vec\n",
    "#\n",
    "#\"\"\"for i, lag in enumerate(lags):\n",
    "#    sdf_pred = vectorizers[i].transform(sdf_pred)\n",
    "#    print(lag, '-> done')\"\"\" \n",
    "#\n",
    "## If tf-idf\n",
    "#\n",
    "#for lag in lags:\n",
    "#    hashingTF = HashingTF(inputCol=lag, outputCol=lag + \"_tf\", numFeatures=100)\n",
    "#    idf = IDF(inputCol=lag + \"_tf\", outputCol=lag + \"_tfidf\", minDocFreq=3)\n",
    "#    sdf_pred = hashingTF.transform(sdf_pred)\n",
    "#    idfModel = idf.fit(sdf_pred)  # Fit the IDF transformer\n",
    "#    sdf_pred = idfModel.transform(sdf_pred)  # Use the fitted model for transformation\n",
    "#    print(lag, \"-> done\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8bfe8-87be-4532-bd7f-afa85c7bfa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_pred = features_assembled(sdf_pred, feats=feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5f86e-4853-403b-97c4-98d1b1e8db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions_future = model.transform(features_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79b8f4d-7dc7-479a-85a7-474e483ba764",
   "metadata": {
    "tags": []
   },
   "source": [
    "### OLD method for compiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8b8b2-d52d-4bc4-a22a-77f5dfab4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"%%time\n",
    "\n",
    "file_path_pred = f's3a://{BUCKET}/work/{VER}/preds.csv'\n",
    "clean_parquet(file_path_pred)\n",
    "\n",
    "sdf_pred = sdf_pred.join(predictions_future).select(\n",
    "    sdf_pred.profile_id,\n",
    "    F.col('probability')\n",
    ")\n",
    "sdf_pred.withColumn(\n",
    "    'tmp',\n",
    "    vector_to_array('probability')\n",
    ").select(\n",
    "    'profile_id',\n",
    "    F.col('tmp')[1].alias('prob_next7days')\n",
    ").write.csv(file_path_pred, header=True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7a4877-bf9c-412b-a544-5613f74e4bfa",
   "metadata": {},
   "source": [
    "### Our new method for compiling, with datetime and unique IDs\n",
    "\n",
    "In this updated code:\n",
    "\n",
    "**We import the necessary libraries:**\n",
    "* `datetime` for working with date and time objects\n",
    "* `uuid` for generating unique identifiers\n",
    "* `getpass` for retrieving the current user's username\n",
    "\n",
    "**We create variables to store the additional information:**\n",
    "* `current_datetime`: We use datetime.datetime.now() to get the current date and time and format it as a string in the format \"YYYYMMDD_HHMMSS\".\n",
    "* `current_user`: We use getpass.getuser() to retrieve the username of the currently signed-in user.\n",
    "* `unique_identifier`: We generate a unique identifier using uuid.uuid4() and take the first 8 characters of the string representation.\n",
    "\n",
    "**We update the file_path_pred to include the additional information:**\n",
    "\n",
    "We insert the current_datetime, current_user, and unique_identifier into the file path using f-string formatting.\n",
    "\n",
    "**The resulting file path will have the format:** \n",
    "\n",
    "`s3a://{BUCKET}/work/{VER}/preds_{current_datetime}_{current_user}_{unique_identifier}.csv`\n",
    "\n",
    "**The rest of the code remains the same, including:**\n",
    "\n",
    "Cleaning the parquet file at the specified file_path_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0168a-965f-4f12-9e66-7f4b1ba4ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "#clean_parquet(file_path_pred)\n",
    "#\n",
    "#sdf_pred = sdf_pred.join(predictions_future).select(\n",
    "#    sdf_pred.profile_id,\n",
    "#    F.col('probability')\n",
    "#)\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee27159-4064-444f-99c1-0fe9ad536e91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get model name (assuming your model variable is named \"model\")\n",
    "#current_model = type(model).__name__\n",
    "#\n",
    "## Create file path with model name\n",
    "#file_path_pred = f's3a://{BUCKET}/work/{VER}/preds_{model_name}_{current_datetime}_{current_user}_{unique_identifier}.csv'\n",
    "#print(f\"- Prediction File Path: {file_path_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d172b-44b5-422e-855c-94a2820f2154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sdf_pred.withColumn(\n",
    "#    'tmp',\n",
    "#    vector_to_array('probability')\n",
    "#).select(\n",
    "#    'profile_id',\n",
    "#    F.col('tmp')[1].alias('prob_next7days')\n",
    "#).write.csv(file_path_pred, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de25bdbc-d333-4e0f-a490-92939bb6a34e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Logging notebook characteristics\n",
    "\n",
    "The below code snippet prints a formatted summary with clear headings and labels for each variable or parameter.\n",
    "\n",
    "It includes information about data processing flags, lag feature details, date range used for filtering payment events, upsampling strategy, model type, and the output file path.\n",
    "\n",
    "You can further customize this code by adding more relevant variables or model-specific parameters based on your pipeline configuration.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "* Reproducibility: Having a summary of the pipeline parameters improves the reproducibility of your results and makes it easier to track the specific settings used for a particular experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "30657a17-5ff9-4c28-a6f8-c237c07cae50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Pipeline Summary ##\n",
      "- PROC_DS: True\n",
      "- PROC_LAGS: True\n",
      "- PROC_VECS: True\n",
      "- Lag Features Used: ['lag_1week_to_2weeks', 'lag_2weeks_to_3weeks', 'lag_3weeks_to_4weeks', 'lag_4weeks_to_5weeks', 'lag_5weeks_to_6weeks', 'lag_6weeks_to_7weeks', 'lag_7weeks_to_8weeks', 'lag_8weeks_to_9weeks', 'lag_9weeks_to_10weeks', 'lag_10weeks_to_11weeks', 'lag_11weeks_to_12weeks', 'lag_12weeks_to_13weeks', 'lag_13weeks_to_14weeks', 'lag_14weeks_to_15weeks', 'lag_15weeks_to_16weeks', 'lag_16weeks_to_17weeks', 'lag_17weeks_to_18weeks', 'lag_18weeks_to_19weeks', 'lag_19weeks_to_20weeks', 'lag_20weeks_to_21weeks', 'lag_21weeks_to_22weeks', 'lag_22weeks_to_23weeks', 'lag_23weeks_to_24weeks', 'lag_24weeks_to_25weeks', 'lag_25weeks_to_26weeks', 'lag_26weeks_to_27weeks']\n",
      "- Number of Features after TF-IDF: 2600\n",
      "- flag_min_datetime: 2024-03-21 00:00:00\n",
      "- flag_max_datetime: 2024-04-18 23:59:59\n",
      "- UPSAMPLE: max\n",
      "- Model Type: LogisticRegressionModel\n",
      "- Predictions saved to:  s3a://pvc-84ea79a0-dc20-4a2d-86ab-f83c1f8d4a7b/work/vP1/preds_LogisticRegressionModel_20240422_140555_st095435_3c6f3a35.csv\n",
      "## End of Summary ##\n"
     ]
    }
   ],
   "source": [
    "print(\"## Pipeline Summary ##\")\n",
    "\n",
    "# Data Processing Flags:\n",
    "print(f\"- PROC_DS: {PROC_DS}\")\n",
    "print(f\"- PROC_LAGS: {PROC_LAGS}\")\n",
    "print(f\"- PROC_VECS: {PROC_VECS}\")\n",
    "\n",
    "# Lag Feature Information:\n",
    "print(f\"- Lag Features Used: {lags}\")\n",
    "print(f\"- Number of Features after TF-IDF: {len(features_dict.items())}\")\n",
    "\n",
    "# Date Range:\n",
    "print(f\"- flag_min_datetime: {flag_min_datetime}\")\n",
    "print(f\"- flag_max_datetime: {flag_max_datetime}\")\n",
    "\n",
    "# Upsampling:\n",
    "print(f\"- UPSAMPLE: {UPSAMPLE}\")\n",
    "\n",
    "# Model:\n",
    "print(f\"- Model Type: {type(model).__name__}\")\n",
    "# Add more model-specific parameters if needed \n",
    "\n",
    "# Output:\n",
    "print(f\"- Predictions saved to: \", file_path_pred)\n",
    "\n",
    "print(\"## End of Summary ##\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce348f-3116-4cdb-a9a1-53bdcb25c6e9",
   "metadata": {},
   "source": [
    "# <span style=\"color: red;\">!!!!! BELOW WE WILL RUN HYPEROPT AGAIN WITHOUT TF-IDF !!!!!\n",
    "\n",
    "It also does not include the date code before running the model\n",
    "\n",
    "# *** Add filtering here ***\n",
    "    if PROC_LAGS:\n",
    "        dates  = (flag_min_datetime, flag_max_datetime)\n",
    "        features_train_filtered = features_train.filter(features_train.event_datetime.between(*dates))\n",
    "    else:\n",
    "        features_train_filtered = features_train \n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f33944c-9fe4-48d0-88c6-47966858660c",
   "metadata": {},
   "source": [
    "## Logistic regression WITHOUT tf-idf elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "83d0f0ae-c1e6-489a-b7ad-6ce60c900fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "search_space = {\n",
    "    'regParam': hp.loguniform('regParam', -7, 1),\n",
    "    'elasticNetParam': hp.uniform('elasticNetParam', 0, 1),\n",
    "    'maxIter': hp.quniform('maxIter', 10, 100, 10),\n",
    "    'tol': hp.loguniform('tol', -7, 0),\n",
    "    # Другие параметры...\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e2a6453c-21ab-4946-bf6c-0f5a7d63ab62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    lr = LogisticRegression(labelCol=\"payment_event_flag\", featuresCol=\"features\",  ** params)\n",
    "    model = lr.fit(features_train)\n",
    "    predictions = model.transform(features_test)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"payment_event_flag\")\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    # Log metrics and params to W&B\n",
    "    wandb.log({\"roc_auc_hyperopt\": auc, \"params_hyperopt\": params})\n",
    "    return {'loss': -auc, 'status': STATUS_OK} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f11089b2-8a6e-49d1-a4fa-be54176146e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update W&B run\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"model_type\": \"LogisticRegressionHYPEROPT\",\n",
    "    #\"output_file\": \"predictions_future_week.csv\",\n",
    "    \"hyperopt_search_space\": search_space  # Include search space here\n",
    "}\n",
    "\n",
    "\n",
    "# Update the config in the existing W&B run\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "68d0e394-4a21-4966-ad06-b4a6e4afedd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:20<00:00,  2.67s/trial, best loss: -0.7158076855108569]\n"
     ]
    }
   ],
   "source": [
    "# Run Hyperopt optimization\n",
    "trials = Trials()\n",
    "best_lr_params = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=30,  # Number of trials\n",
    "    trials=trials\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e9b9c243-6c33-4519-a1b1-f896c0e077db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elasticNetParam': 0.2796271657027966,\n",
       " 'maxIter': 40.0,\n",
       " 'regParam': 0.004123055975276005,\n",
       " 'tol': 0.0021750755982573337}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lr_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d5fecdea-decc-4272-838b-ac8818fb84c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log best hyperparameters\n",
    "wandb.log({\"best_lr_hyperparameters\": best_lr_params})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "22ad4ddd-3ba5-490d-aa09-fdf9c7d10539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the best lr model\n",
    "best_lr = LogisticRegression(labelCol=\"payment_event_flag\", featuresCol=\"features\", **best_lr_params)\n",
    "best_lr_model = best_lr.fit(features_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6aeb7784-8ae3-474f-ab0d-3200e31cf545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "best_predictions = best_lr_model.transform(features_test)\n",
    "best_evaluator = BinaryClassificationEvaluator(labelCol=\"payment_event_flag\")\n",
    "best_lr_auc = best_evaluator.evaluate(best_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3592846d-23fe-40a6-8bfd-4e3a7ce15681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate classification report for the best model\n",
    "y_true = best_predictions.select('payment_event_flag').rdd.map(lambda x: x['payment_event_flag']).collect()\n",
    "y_pred = best_predictions.select('prediction').rdd.map(lambda x: x['prediction']).collect()\n",
    "best_report = classification_report(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d8326eaa-9a58-48f0-ad36-8186e4370f36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.53      0.57      3945\n",
      "           1       0.65      0.72      0.68      4669\n",
      "\n",
      "    accuracy                           0.64      8614\n",
      "   macro avg       0.63      0.63      0.63      8614\n",
      "weighted avg       0.63      0.64      0.63      8614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(best_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4218d2-92f1-4242-a425-0ab1ddd1cb78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert report to HTML format\n",
    "hyperopt_report_html = classification_report(y_true, y_pred, output_dict=False)\n",
    "\n",
    "# fix \n",
    "hyperopt_lr_report_html = dict_to_html_table(lr_report)\n",
    "\n",
    "# Log the report as HTML to W&B\n",
    "wandb.log({\"hopt_best_lr_classification_report\": wandb.Html(hyperopt_lr_report_html)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a8a02-0cc1-4f6e-ade6-16200b8ac712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdec0b5-3aba-4ee1-9780-1dc5c17d4822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a40d67-6f3f-4ce4-8445-55312d5c531e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677261c-6582-415f-a5e9-72f9c8f30cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619efa83-7ec6-43d4-9aee-b64131161b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb96ce-09aa-408e-88b7-838755d9d816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52960d-4bc7-4efd-803e-75cb0a7404d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1993b-ed85-43c7-9832-76625ceb7fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "14812963-7b7d-4b82-9c7a-9c26862770fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import wandb\n",
    "\n",
    "# Предполагаем, что features_train и features_test уже определены\n",
    "\n",
    "def lr_objective(trial):\n",
    "    params = {\n",
    "        'labelCol': 'payment_event_flag',\n",
    "        'featuresCol': 'features',\n",
    "        # Другие параметры для оптимизации\n",
    "    }\n",
    "    params.update(trial.params)\n",
    "    \n",
    "    lr = LogisticRegression( ** params)\n",
    "    lr_model = lr.fit(features_train)\n",
    "    predictions = lr_model.transform(features_test)\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Log metrics and params to W&B\n",
    "    wandb.log({\"lr_hyperopt_auc\": auc, \"lr_hyperopt_params\": params})\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e146a16f-49bc-4883-a681-bee8710d99fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticNetParam: 0.5040260781935606\n",
      "maxIter: 70.0\n",
      "regParam: 0.0022335019114765094\n",
      "tol: 0.010194121730669711\n"
     ]
    }
   ],
   "source": [
    "'''for key, value in best_params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e3369fb0-b51e-41fb-aeee-85e686821490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"# Update W&B run config\n",
    "wandb.config.update({\n",
    "    \"model_type\": \"LogisticRegressionHyperopt\",\n",
    "    \"hyperopt_search_space\": lr_search_space\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3e6c1c6f-1a51-472f-8ab7-4334235f3143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''best_params = {'elasticNetParam': 0.5040260781935606,\n",
    "               'maxIter': 70.0,\n",
    "               'regParam': 0.0022335019114765094,\n",
    "               'tol': 0.010194121730669711}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbdb2cd-e0c3-4658-b294-7063bffcbbbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log best hyperparameters\n",
    "wandb.log({\"best_lr_hyperparameters\": best_lr_params})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8791057d-aa70-4252-8af8-8852919d9cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675f7c93-f8b9-4d5d-8a5b-8e8b5cdf82c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e26b1-a69f-40a2-954b-85fdfde52f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f3bfe33-14b9-4408-aef5-9685b50bcae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba4d080-07dd-49d2-99fe-18125c02fc6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e9fdbed9-40bb-4223-8954-0ae215ded41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"# Update W&B run\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"model_type\": \"LogisticRegressionHYPEROPT\", # <-- change to your model name \n",
    "    #\"output_file\": \"predictions_future_week.csv\",\n",
    "    \"hyperopt_search_space\": search_space  # Include search space here\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4cf267-f1df-47b4-919a-60ef9555e258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''# Update the config in the existing W&B run\n",
    "wandb.config.update(config)\n",
    "\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "trials = Trials()\n",
    "best_params = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=30,  # Number of trials\n",
    "    trials=trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870ad9b-390d-4c4f-9eb7-a46ea95bb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for key, value in best_params.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "\n",
    "# best_params = convert_feature_subset_strategy(best_params)\n",
    "\n",
    "best_params = convert_hyperopt_params(best_params)\n",
    "\n",
    "\n",
    "for key, value in best_params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f864ad-a5d3-4cf2-9609-9eec34366aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef55396-2d4a-4730-8873-e97507b2fe9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e6fac-82ab-4eef-b9f6-4356c8d10b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2d966-5e58-4c8a-b10a-d7c1e3e63e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from hyperopt import hp\n",
    "from hyperopt.fmin import fmin\n",
    "import time\n",
    "\n",
    "metric = {'auc': 'auc'}\n",
    "\n",
    "def objective(hyperopt_params):\n",
    "    lr = LogisticRegression(\n",
    "        labelCol=\"payment_event_flag\", \n",
    "        featuresCol=\"features\", \n",
    "        maxIter=hyperopt_params['maxIter'], \n",
    "        regParam=hyperopt_params['regParam']\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    model = lr.fit(features_train)\n",
    "    runtime = time.time() - start\n",
    "    \n",
    "    predictions = model.transform(features_test)\n",
    "    auc = roc_auc_score(features_test.select(\"payment_event_flag\").toPandas(), predictions.select('probability').toPandas())\n",
    "    \n",
    "    result = {\n",
    "        'loss': 1 - auc,\n",
    "        'status': STATUS_OK,\n",
    "        'model': lr,\n",
    "        'runtime': runtime,\n",
    "        'params': hyperopt_params\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "params_space = {\n",
    "    'maxIter': hp.qloguniform('maxIter', 5, 15, 1),\n",
    "    'regParam': hp.loguniform('regParam', -5, -2)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=params_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials,\n",
    "    rstate=np.random.RandomState(36)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff41190-ba90-47a6-9145-35a63be44ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from hyperopt import hp\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt.mongoexp import MongoExp\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "186a3c14-4d82-4b00-a2fa-ed647dc20db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "'''# Задаем функцию для оптимизации\n",
    "def objective(params):\n",
    "    lr = LogisticRegression(labelCol=\"payment_event_flag\", featuresCol=\"features\", *params)\n",
    "    model = lr.fit(features_train)\n",
    "    predictions = model.transform(features_test)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"payment_event_flag\")\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    \n",
    "    return {'loss': 1 - auc, 'status': STATUS_OK}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7e8c92c-42bb-4bfe-9bd5-ee5301b869ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Диапазоны гиперпараметров\n",
    "param_space = {\n",
    "    'maxIter': scope.int(hp.quniform('maxIter', 10, 100, 1)),\n",
    "    'regParam': hp.uniform('regParam', 0.0, 1.0),\n",
    "    'elasticNetParam': hp.uniform('elasticNetParam', 0.0, 1.0),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4275ef1-d509-426c-b560-6b787c9ebed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Запускаем оптимизацию\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=param_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10,\n",
    "            trials=trials)\n",
    "\n",
    "print(best)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cc4659-1c97-48b2-83aa-eaad66e70652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b245ad23-b223-4a64-bfd3-8e2bbdba2039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54653837-1520-4a15-86bb-a22a14a527ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c7e44-dab6-446c-82a3-7632197d6727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57b74a-de02-4235-83b4-37bdcbb0f6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486305d-3633-4e30-bac5-4c2b40d22376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62622935-5d40-4419-bef0-ad1001a7fea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12151308-2f25-4a2d-9b69-277f3af5d1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2d5e5-47a9-490c-957b-d5823087ffa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f2a9f4-94f8-4877-bebe-5f8510a153e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f2532-4400-43d5-924f-5f5a74959169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c243dbd9-13ed-4dab-9a60-399e095000fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d282c07-4505-4884-b984-997918ed0d8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color: red;\">Change the model name below </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "75b18ff8-330e-4ac3-b83e-9aa293f448a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update W&B run\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"model_type\": \"LogisticRegressionHYPEROPT\", # <--  model name \n",
    "    #\"output_file\": \"predictions_future_week.csv\",\n",
    "    \"hyperopt_search_space\": search_space  # Include search space here\n",
    "}\n",
    "\n",
    "\n",
    "# Update the config in the existing W&B run\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d6f18-12bf-40b3-a81c-d959a9567419",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Run Hyperopt optimization\n",
    "trials = Trials()\n",
    "best_params = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=30,  # Number of trials\n",
    "    trials=trials\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bcba65-40cb-45e9-a8d0-31d52dd1b57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key, value in best_params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b043de6-c371-4903-9968-7fe7e9fc24e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_params = convert_feature_subset_strategy(best_params)\n",
    "\n",
    "best_params = convert_hyperopt_params(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ec065-b449-47b5-b69f-4fe641998e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in best_params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a567aaa6-d002-4fa3-bc82-70566a070ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log best hyperparameters\n",
    "wandb.log({\"best_hyperparameters_lr\": best_params}) # <\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "097ed7d4-21a2-4d53-aba6-6d446304cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model\n",
    "\n",
    "best_lr = LogisticRegression(labelCol=\"payment_event_flag\", featuresCol=\"features\", **best_params)\n",
    "best_model = best_lr.fit(features_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cfbf2e44-04a9-44ef-be37-aced9a8b55ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "predictions = best_model.transform(features_test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"payment_event_flag\")\n",
    "best_auc = evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "195134de-1246-4197-a258-0a6d716e5a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "y_true = predictions.select('payment_event_flag').rdd.map(lambda x: x['payment_event_flag']).collect()\n",
    "y_pred = predictions.select('prediction').rdd.map(lambda x: x['prediction']).collect()\n",
    "report = classification_report(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "44d3bb51-1117-4b66-82e3-7c25d183e9aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Calculate PR AUC\n",
    "payment_event_flag_preds = predictions.select('probability', 'payment_event_flag')\n",
    "metrics = BinaryClassificationMetrics(payment_event_flag_preds.rdd.map(lambda lp: (float(lp[0][1]), float(lp[1]))))\n",
    "pr_auc = metrics.areaUnderPR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "22a46c9d-4897-4bb0-8fdf-bbaeb57dec1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.7171756055703816\n",
      "PR AUC: 0.7581376191111291\n"
     ]
    }
   ],
   "source": [
    "print(f\"ROC AUC: {best_auc}\")\n",
    "print(f\"PR AUC: {pr_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "930beae7-39d5-409d-8eb5-ac081b33ca9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.54      0.58      3945\n",
      "           1       0.65      0.72      0.69      4669\n",
      "\n",
      "    accuracy                           0.64      8614\n",
      "   macro avg       0.64      0.63      0.63      8614\n",
      "weighted avg       0.64      0.64      0.64      8614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0fd87007-8fd8-4481-b235-489f7c62698a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log additional metrics to W&B\n",
    "wandb.log({\"best_model_classification_report_rf\": wandb.Html(report)})\n",
    "wandb.log({\"best_model_roc_auc_lr\": best_auc})\n",
    "wandb.log({\"best_model_pr_auc_lr\": pr_auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d9a15-249c-4fd4-881b-9f0b95618bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df7819-a5f2-494e-8264-98969d107643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c6202-47cb-46d2-ae66-0c2dde3758d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "438841b4-6885-4ac6-9945-00c29d3a8398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features for Hyperopt lr Model:\n",
      "Feature 982: ['lag_10weeks_to_11weeks_Мои штрафы/Оплата/Ушел с ввода данных'] - Weight: 0.3997\n",
      "Feature 2007: ['lag_21weeks_to_22weeks_Проверка/История платежей', 'lag_21weeks_to_22weeks_Purchase'] - Weight: 0.3982\n",
      "Feature 38: ['lag_1week_to_2weeks_Мои штрафы/Оплата/Завершили оплату', 'lag_1week_to_2weeks_Страхование/Главная/Лендинг/Ошибка загрузки лендинга', 'lag_1week_to_2weeks_Пуш Локальный/Скидочный штраф/Показан'] - Weight: 0.3522\n",
      "Feature 2246: ['lag_23weeks_to_24weeks_Мои штрафы/Оплата/Сберпей/Открыт'] - Weight: 0.3476\n",
      "Feature 1205: ['lag_13weeks_to_14weeks_Мои штрафы/Оплата/Завешили оплату'] - Weight: 0.3400\n",
      "Feature 538: ['lag_6weeks_to_7weeks_Мои штрафы/Оплата/Завершили оплату', 'lag_6weeks_to_7weeks_Пуш Локальный/Скидочный штраф/Показан'] - Weight: -0.3336\n",
      "Feature 682: ['lag_7weeks_to_8weeks_Мои штрафы/Оплата/Ушел с ввода данных'] - Weight: 0.3175\n",
      "Feature 528: ['lag_6weeks_to_7weeks_Мои штрафы/Оплата/Ввод личных данных'] - Weight: 0.3074\n",
      "Feature 1438: ['lag_15weeks_to_16weeks_Пуш Локальный/Скидочный штраф/Показан', 'lag_15weeks_to_16weeks_Мои штрафы/Оплата/Завершили оплату'] - Weight: 0.3072\n",
      "Feature 2182: ['lag_22weeks_to_23weeks_Мои штрафы/Оплата/Ушел с ввода данных'] - Weight: 0.3066\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients (feature weights) from the best SVM model\n",
    "best_lr_coef = best_lr_model.coefficients\n",
    "\n",
    "# Zip coefficients with feature names\n",
    "features_weights = list(zip(features_dict.keys(), best_lr_coef))\n",
    "\n",
    "# Sort features by absolute value of their weights (descending order)\n",
    "features_weights.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print top features and their weights\n",
    "print(\"Top Features for Hyperopt lr Model:\")\n",
    "for feat_num, weight in features_weights[:10]:\n",
    "    print(f\"Feature {feat_num}: {features_dict[feat_num]} - Weight: {weight:.4f}\")\n",
    "\n",
    "# Create a DataFrame for feature importance (without weights)\n",
    "feature_importance_df = pd.DataFrame(features_weights, columns=[\"Feature Number\", \"Weight\"])\n",
    "\n",
    "# Log the DataFrame as a W&B Table\n",
    "wandb.log({\"best_lr_feature_importance\": wandb.Table(dataframe=feature_importance_df)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0812619a-d602-4d41-8e7d-41d7d634c68a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate total absolute weight\n",
    "total_weight = sum(abs(weight) for _, weight in features_weights)\n",
    "\n",
    "# Create a list to store feature information with weight percentages\n",
    "feature_info = []\n",
    "\n",
    "# Extract information for top 20 features with weight percentages\n",
    "for feat_num, weight in features_weights[:20]:\n",
    "    event_names = features_dict[feat_num]\n",
    "    for event_name in event_names:\n",
    "        percentage_contribution = (abs(weight) / total_weight) * 100\n",
    "        feature_info.append({\n",
    "            \"Feature Number\": feat_num,\n",
    "            \"Event Name\": event_name,\n",
    "            \"Weight\": weight,\n",
    "            \"Percentage Contribution\": f\"{percentage_contribution:.2f}%\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame for weighted feature importance\n",
    "feature_weight_df = pd.DataFrame(feature_info)\n",
    "\n",
    "# Log the DataFrame as a W&B Table\n",
    "wandb.log({\"best_lr_feature_weight\": wandb.Table(dataframe=feature_weight_df)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d09fe466-daf8-4353-aaa9-3d0894ce1b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_dict = {\n",
    "    \"PROC_DS\": PROC_DS,\n",
    "    \"PROC_LAGS\": PROC_LAGS,\n",
    "    \"PROC_VECS\": PROC_VECS,\n",
    "    \"flag_min_datetime\": flag_min_datetime,\n",
    "    \"flag_max_datetime\": flag_max_datetime,\n",
    "    \"UPSAMPLE\": UPSAMPLE,\n",
    "    \"Number of Features after TF-IDF\": len(features_dict.items()),\n",
    "    \"Model Type\": type(model).__name__,\n",
    "    \"maxIter\": model.getMaxIter(),\n",
    "    \"regParam\": model.getRegParam(),\n",
    "    \"tol\": model.getTol(),\n",
    "#    \"Predictions saved to\": file_path_pred, \n",
    "    \"user_id\": current_user, \n",
    "    \"unique_identifier\": unique_identifier\n",
    "}\n",
    "\n",
    "summary_dict[\"Number of Features after TF-IDF\"] = str(len(features_dict.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b83b0d77-8d4f-4c33-9582-bb9915ee9c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dict_to_html_table(data):\n",
    "    html = \"<table>\"\n",
    "    for key, value in data.items():\n",
    "        html += f\"<tr><th>{key}</th><td>{value}</td></tr>\"\n",
    "    html += \"</table>\"\n",
    "    return html\n",
    "\n",
    "summary_html = dict_to_html_table(summary_dict)\n",
    "\n",
    "wandb.log({\"pipeline_summary_html\": wandb.Html(summary_html)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f928a32-edff-4d5c-a9be-423ea6ec8f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''# Convert report to HTML format\n",
    "hyperopt_report_html = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "# fix \n",
    "hyperopt_lr_report_html = dict_to_html_table(lr_report)\n",
    "\n",
    "# Log the report as HTML to W&B\n",
    "wandb.log({\"hopt_best_lr_classification_report\": wandb.Html(hyperopt_lr_report_html)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "55c096fb-a80b-4e57-89d6-a6efdcb6f0f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert report to HTML format\n",
    "lr_report = classification_report(y_true, y_pred, output_dict=True)\n",
    "# fix \n",
    "hyperopt_lr_report_html = dict_to_html_table(lr_report)\n",
    "\n",
    "# Log the report as HTML to W&B\n",
    "wandb.log({\"hopt_best_lr_classification_report\": wandb.Html(hyperopt_lr_report_html)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897950f-4e26-48aa-9a69-b915344957d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa22a3c-c018-4ac4-aba6-ce7a6b43afdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373380f8-95ab-4164-abb9-d4acdc696ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76898c24-ba92-4be3-97ec-770e0b456b1d",
   "metadata": {},
   "source": [
    "## <span style=\"color: red;\">Below code will finish my W&B run</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "afbbbd2e-751e-46fd-a868-15a6003d2e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.129 MB of 0.129 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_model_pr_auc_lr</td><td>▁▁</td></tr><tr><td>best_model_roc_auc_lr</td><td>▁▁</td></tr><tr><td>pr_auc</td><td>▁</td></tr><tr><td>roc_auc</td><td>▁</td></tr><tr><td>roc_auc_hyperopt</td><td>▆▁▁▁██▃▇▁██▇██▁▆▁▁▇█▁▆█▅███▁▇▁▅▁▆▇▁▁████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_model_pr_auc_lr</td><td>0.75814</td></tr><tr><td>best_model_roc_auc_lr</td><td>0.71718</td></tr><tr><td>pr_auc</td><td>0.66955</td></tr><tr><td>roc_auc</td><td>0.65632</td></tr><tr><td>roc_auc_hyperopt</td><td>0.7142</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-glade-17</strong> at: <a href='https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/e7f99lil' target=\"_blank\">https://wandb.ai/gsom-diploma-jap/ray-diploma/runs/e7f99lil</a><br/> View project at: <a href='https://wandb.ai/gsom-diploma-jap/ray-diploma' target=\"_blank\">https://wandb.ai/gsom-diploma-jap/ray-diploma</a><br/>Synced 5 W&B file(s), 11 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/home/jovyan/wandb/run-20240423_125806-e7f99lil/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()  # Finalize W&B run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac642d74-d965-4536-b371-e1a2c03f31a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
